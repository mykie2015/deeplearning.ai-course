{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47065bdb",
   "metadata": {},
   "source": [
    "# Lesson 5: Reward functions with LLM as a judge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179c6c18",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "<p> ðŸ’» &nbsp; <b>Access <code>requirements.txt</code>  file:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Open\"</em>.\n",
    "\n",
    "<p> â¬‡ &nbsp; <b>Download Notebooks:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Download as\"</em> and select <em>\"Notebook (.ipynb)\"</em>.</p>\n",
    "\n",
    "<p> ðŸ“’ &nbsp; For more help, please see the <em>\"Appendix â€“ Tips, Help, and Download\"</em> Lesson.</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd4bc22",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\"> ðŸš¨\n",
    "&nbsp; <b>Different Run Results:</b> The output generated by AI chat models can vary with each execution due to their dynamic, probabilistic nature. Don't be surprised if your results differ from those shown in the video.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac9fb57",
   "metadata": {},
   "source": [
    "Start by loading dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d1fb4c5-1951-4293-b643-0fca57f56a03",
   "metadata": {
    "height": 285
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "from utils import *\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "pb_client = OpenAI(\n",
    "    base_url=os.environ[\"PREDIBASE_MODEL_LLAMA_URL\"],\n",
    "    api_key=os.environ[\"PREDIBASE_API_KEY\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dee0416",
   "metadata": {},
   "source": [
    "## The task: creating summaries of earnings call transcripts\n",
    "\n",
    "Start by loading the earnings call dataset from Hugging Face:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3b6a7f2-1bd7-4a3f-a136-ff17b4db8e2d",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ada95676c9b64dff91c145646e84b28d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.json:   0%|          | 0.00/30.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ea8b2afb39640b0afdb445b75b9fa36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.json:   0%|          | 0.00/8.93M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d34c87bde6d422c8e78851e5350866c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1681 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b57c75ecde074ddfa12ab5c10c9f2a74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/495 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm joined by Tom Greco, our President and Chief Executive Officer; and Jeff Shepherd, our Executive Vice President and Chief Financial Officer.\n",
      "We also hope that you and your families are healthy and safe.\n",
      "The health and safety of our team members and customers has been a top priority over the past year.\n",
      "With strength across all channels, we delivered comparable store sales growth of 24.7%, and margin expansion of 478 basis points versus the prior year.\n",
      "On a two-year stack, our comp sales growth was 15.4%.\n",
      "Adjusted diluted earnings per share of $3.34 represented an all-time quarterly high for AAP, and improved more than 230% compared to Q1 2020.\n",
      "Free cash flow of $259 million was up significantly versus the prior year, and we returned over $203 million to our shareholders through a combination of share repurchases and our quarterly cash dividend.\n",
      "In addition, we recently announced an updated capital allocation framework targeting top quartile total shareholder return, highlighted by operating income growth, share repurchases and an increase in our dividend.\n",
      "This further reinforces our confidence in future cash generation and our commitment to returning excess cash to shareholders.\n",
      "As outlined in April, we are building an ownership culture, as well as a differentiated operating model at Advance.\n",
      "Over the past few years, we've made substantial investments in our brands, our digital and physical assets, and our team.\n",
      "These investments, along with external factors, enabled us to post a strong start to 2021.\n",
      "Clearly, the federal stimulus package, along with our first real winter weather in three years, was a benefit to our industry.\n",
      "From a category perspective, net sales growth was led by batteries, appearance chemicals and wipers.\n",
      "Geographically, all eight regions posted over 20% growth.\n",
      "Importantly, over the past year, the Northeast, our largest region, had been below our overall reported growth rate and well below that of our top-performing regions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"mrSoul7766/ECTSum\")\n",
    "transcript = ds[\"train\"][1][\"text\"]\n",
    "print(transcript[:1983])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1b6b47",
   "metadata": {},
   "source": [
    "Define a summarize prompt and helper function, then create and print a summary \n",
    "\n",
    "(Note: the MODEL_NAME is specified in the utils.py file: here you are using Llama-3.1-8B-Instruct-dequantized to generate the summaries.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e64055e9-7219-445e-bfa1-d00c119fe30e",
   "metadata": {
    "height": 370
   },
   "outputs": [],
   "source": [
    "SUMMARIZE_PROMPT = \"\"\"Generate a concise summary of the information in the following earnings call transcript.\n",
    "\n",
    "Only respond with the summary, do not include any extraneous text.\n",
    "\n",
    "Transcript:\n",
    "\n",
    "{transcript}\n",
    "\"\"\"\n",
    "\n",
    "def summarize(transcript, n=1):\n",
    "    prompt = SUMMARIZE_PROMPT.format(transcript=transcript)\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    return pb_client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=messages,\n",
    "        n=n,\n",
    "        temperature=0.9,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d29d9d2-cdc5-47b2-b6a3-ece3f7bdfeba",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advance Auto Parts (AAP) reported Q1 2021 earnings, with highlights including:\n",
      "\n",
      "* Comparable store sales growth of 24.7% and margin expansion of 478 basis points year-over-year\n",
      "* Adjusted diluted earnings per share of $3.34, an all-time quarterly high and up 230% from Q1 2020\n",
      "* Free cash flow of $259 million, up significantly from the prior year\n",
      "* Share repurchases and quarterly cash dividend returning over $200 million to shareholders\n",
      "* Announced an updated capital allocation framework targeting top-quartile total shareholder return\n",
      "* Expecting comp sales growth to be up 4-6% from the prior year, with Professional business expected to outperform DIY for the balance of the year\n",
      "* Adjusted OI margin range updated to be between 9-9.2%\n",
      "* Confident in ability to execute long-term strategic plans to deliver strong and sustainable total shareholder return.\n"
     ]
    }
   ],
   "source": [
    "resp = summarize(transcript)\n",
    "summary = resp.choices[0].message.content\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97645179-fee1-4abf-aaa9-864044c969f5",
   "metadata": {},
   "source": [
    "## Use an LLM as a judge of summary quality\n",
    "\n",
    "Define a prompt that will tell the OpenAI GPT-4o-mini model to assign a reward score to a summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e51d32b-389b-4036-ab7e-93598d02213a",
   "metadata": {
    "height": 1016
   },
   "outputs": [],
   "source": [
    "JUDGE_PROMPT_V1 = \"\"\"\n",
    "Rate the following summary of an earnings call transcript on a \n",
    "scale from 1 to 10. \n",
    "\n",
    "1 means the summary is very poor, 10 means the summary is very good.\n",
    "\n",
    "Provide reasoning followed by the final score at the end \n",
    "surrounded by <score> tags.\n",
    "\n",
    "For example:\n",
    "\n",
    "<score>1</score>\n",
    "\n",
    "Transcript:\n",
    "\n",
    "{transcript}\n",
    "\n",
    "Summary:\n",
    "\n",
    "{summary}\n",
    "\"\"\"\n",
    "\n",
    "def judge_reward_v1(\n",
    "    transcript: str,\n",
    "    summary: str, \n",
    "    model: str = \"gpt-4o-mini\", \n",
    "    verbose: bool = False,\n",
    ") -> float:\n",
    "    prompt = JUDGE_PROMPT_V1.format(\n",
    "        transcript=transcript, \n",
    "        summary=summary,\n",
    "    )\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        n=1,\n",
    "        temperature=0,\n",
    "        )\n",
    "    completion = resp.choices[0].message.content\n",
    "\n",
    "    if verbose:\n",
    "        print(completion)\n",
    "\n",
    "    try:\n",
    "        match = re.search(r\"<score>(\\d+)<\\/score>\", completion)\n",
    "        if match is None:\n",
    "            return 0\n",
    "    \n",
    "        # Extract the \"score\" part from the completion\n",
    "        score = match.group(1).strip()\n",
    "        score = int(score)\n",
    "    except:\n",
    "        score = 0\n",
    "    \n",
    "    return score / 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e525ad",
   "metadata": {},
   "source": [
    "Now score the summary you generated above using the new reward function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a0dc082-edfe-4f19-be8c-535595914f50",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The summary provided captures the key financial highlights and strategic initiatives discussed in the earnings call transcript. It effectively summarizes the most important metrics, such as comparable store sales growth, earnings per share, free cash flow, and shareholder returns. Additionally, it mentions the updated guidance for future sales growth and operating income margins, which are critical for investors and stakeholders.\n",
      "\n",
      "However, the summary could be improved by including more context about the factors driving these results, such as the impact of federal stimulus, changes in consumer behavior, and the company's strategic initiatives in supply chain and product offerings. It also lacks mention of the company's investments in digital and physical assets, which were emphasized in the transcript.\n",
      "\n",
      "Overall, while the summary is concise and covers the essential financial metrics, it misses some of the broader context and strategic insights that would provide a more comprehensive understanding of the company's performance and future outlook.\n",
      "\n",
      "Given these considerations, I would rate the summary as follows:\n",
      "\n",
      "<score>7</score>\n",
      "0.7\n"
     ]
    }
   ],
   "source": [
    "score = judge_reward_v1(transcript, summary, verbose=True)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fec318",
   "metadata": {},
   "source": [
    "Now generate 8 new summaries and score each one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfb20ac7-6a0c-4bd2-a325-35362a00bf60",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "resp = summarize(transcript, n=8)\n",
    "summaries = [choice.message.content for choice in resp.choices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b8ad39e-6806-48d8-9636-5dd572e401f0",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8, 0.8, 0.8, 0.8, 0.7, 0.8, 0.8, 0.8]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = [judge_reward_v1(transcript, summary) for summary in summaries]\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b30c08",
   "metadata": {},
   "source": [
    "## Taking a quiz to assign a reward score\n",
    "\n",
    "In this section, you'll create a multiple choice quiz that tests key facts from the earnings call transcript. You'll then ask another LLM to take the quiz using different call summaries, and use the quiz score as the reward score.\n",
    "\n",
    "Start by creating the quiz prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2fedb41-a3ca-436f-b495-937ded53b05e",
   "metadata": {
    "height": 591
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from random import shuffle\n",
    "\n",
    "\n",
    "QUIZ_PROMPT = \"\"\"\n",
    "Generate a multiple-choice quiz based on the information \n",
    "in the following earnings call transcript.\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "1. What was the q1 adjusted earnings per share?\n",
    "a) $3.34\n",
    "b) $5.32\n",
    "c) $2.49\n",
    "d) $7.78\n",
    "\n",
    "2. By what percent did same store sales rise in q1?\n",
    "a) 29.4%\n",
    "b) 32.1%\n",
    "c) 24.7%\n",
    "d) 21.2%\n",
    "\n",
    "===== ANSWERS =====\n",
    "1. a\n",
    "2. c\n",
    "```\n",
    "\n",
    "Limit the length of the quiz to the top 10 most relevant questions for financial analysts.\n",
    "\n",
    "Transcript:\n",
    "\n",
    "{transcript}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc6fac5",
   "metadata": {},
   "source": [
    "Next, define pydantic classes that define the structure of an individual question, and a quiz comprised of multiple questions. Then define a helper function to create a quiz using structured response from GPT-4o-mini:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31c0769d-ad40-4159-8c3a-3d57acd8cd6f",
   "metadata": {
    "height": 1016
   },
   "outputs": [],
   "source": [
    "class Question(BaseModel):\n",
    "    text: str\n",
    "    options: list[str]\n",
    "    answer: int\n",
    "\n",
    "    def shuffle_options(self) -> None:\n",
    "        \"\"\"Shuffle the options while preserving the correct answer\"\"\"\n",
    "        # Get the correct answer text\n",
    "        correct = self.options[self.answer]\n",
    "        \n",
    "        # Shuffle the options\n",
    "        shuffled = self.options.copy()\n",
    "        shuffle(shuffled)\n",
    "        \n",
    "        # Update the answer index to match new position\n",
    "        self.options = shuffled\n",
    "        self.answer = shuffled.index(correct)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"Pretty print a single question\"\"\"\n",
    "        output = [self.text]\n",
    "        for i, option in enumerate(self.options):\n",
    "            output.append(f\"{chr(65+i)}. {option}\")\n",
    "        return \"\\n\".join(output)\n",
    "\n",
    "\n",
    "class Quiz(BaseModel):\n",
    "    questions: list[Question]\n",
    "\n",
    "    def shuffle_all_questions(self) -> None:\n",
    "        \"\"\"Shuffle the options for all questions in the quiz\"\"\"\n",
    "        for question in self.questions:\n",
    "            question.shuffle_options()\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"Pretty print the entire quiz\"\"\"\n",
    "        output = []\n",
    "        for i, question in enumerate(self.questions, 1):\n",
    "            output.append(f\"\\nQuestion {i}:\")\n",
    "            output.append(str(question))\n",
    "        return \"\\n\".join(output)\n",
    "\n",
    "\n",
    "def create_quiz(transcript: str):\n",
    "    prompt = QUIZ_PROMPT.format(transcript=transcript)\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    resp = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        temperature=0.7,\n",
    "        response_format=Quiz,\n",
    "    )\n",
    "\n",
    "    quiz = resp.choices[0].message.parsed\n",
    "    quiz.shuffle_all_questions()\n",
    "\n",
    "    return quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebaff5c7-7995-46fb-9083-08e0df679f90",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question 1:\n",
      "What was the adjusted diluted earnings per share in Q1?\n",
      "A. $2.49\n",
      "B. $5.32\n",
      "C. $1.00\n",
      "D. $3.34\n",
      "\n",
      "Question 2:\n",
      "By what percent did comparable store sales grow in Q1?\n",
      "A. 29.4%\n",
      "B. 21.2%\n",
      "C. 32.1%\n",
      "D. 24.7%\n",
      "\n",
      "Question 3:\n",
      "What was the total free cash flow for the quarter?\n",
      "A. $259 million\n",
      "B. $500 million\n",
      "C. $203 million\n",
      "D. $330 million\n",
      "\n",
      "Question 4:\n",
      "What is the adjusted operating income margin for Q1?\n",
      "A. 6%\n",
      "B. 4.5%\n",
      "C. 8%\n",
      "D. 9%\n",
      "\n",
      "Question 5:\n",
      "How much was returned to shareholders through share repurchases and dividends?\n",
      "A. $300 million\n",
      "B. $203 million\n",
      "C. $150 million\n",
      "D. $250 million\n",
      "\n",
      "Question 6:\n",
      "What is the expected increase in comp sales guidance for the year?\n",
      "A. 3% to 5%\n",
      "B. 2% to 4%\n",
      "C. 5% to 7%\n",
      "D. 4% to 6%\n",
      "\n",
      "Question 7:\n",
      "What was the year-over-year increase in adjusted SG&A expense?\n",
      "A. 450 basis points\n",
      "B. 200 basis points\n",
      "C. 300 basis points\n",
      "D. 387 basis points\n",
      "\n",
      "Question 8:\n",
      "What is the expected adjusted operating income margin range for the year?\n",
      "A. 9.5% to 10%\n",
      "B. 9% to 9.2%\n",
      "C. 8% to 8.5%\n",
      "D. 10% to 10.5%\n",
      "\n",
      "Question 9:\n",
      "How many new independent locations were added to the Carquest family?\n",
      "A. 29\n",
      "B. 50\n",
      "C. 15\n",
      "D. 40\n",
      "\n",
      "Question 10:\n",
      "What was the increase in VIP members in Q1?\n",
      "A. 14%\n",
      "B. 30%\n",
      "C. 10%\n",
      "D. 20%\n"
     ]
    }
   ],
   "source": [
    "quiz = create_quiz(transcript)\n",
    "print(quiz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5a36a6",
   "metadata": {},
   "source": [
    "Now, define a function that asks an LLM to take a quiz, using a transcript summary as the source material:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de5b7501-1557-40b7-acf4-8dbff0bcf6a0",
   "metadata": {
    "height": 846
   },
   "outputs": [],
   "source": [
    "letter_to_index = {\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3}\n",
    "index_to_letter = [\"A\", \"B\", \"C\", \"D\"]\n",
    "\n",
    "\n",
    "TAKE_QUIZ_PROMPT = \"\"\"Use the provided summary of a transcript \n",
    "to answer the following quiz.\n",
    "\n",
    "Quiz:\n",
    "\n",
    "{quiz}\n",
    "\n",
    "Summary:\n",
    "\n",
    "{summary}\n",
    "\n",
    "Respond with just a list of answers and no additional text, \n",
    "for example:\n",
    "\n",
    "[A, D, C, B, B, C, D, A, A, B]\n",
    "\n",
    "You must provide an answer for all 10 questions. \n",
    "If you don't know the answer, answer with \"0\" for that question. \n",
    "Example:\n",
    "\n",
    "[A, D, 0, B, B, C, D, A, A, B]\n",
    "\"\"\"\n",
    "\n",
    "def take_quiz(summary, quiz):\n",
    "    question_strs = []\n",
    "    for question in quiz.questions:\n",
    "        question_str = question.text\n",
    "        for i, option in enumerate(question.options):\n",
    "            letter = index_to_letter[i]\n",
    "            question_str += f\"\\n{letter}. {option}\"\n",
    "        question_strs.append(question_str)\n",
    "    quiz_str = \"\\n\\n\".join(question_strs)\n",
    "\n",
    "    prompt = TAKE_QUIZ_PROMPT.format(quiz=quiz_str, summary=summary)\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0,\n",
    "    )\n",
    "    resp_str = resp.choices[0].message.content\n",
    "    \n",
    "    # Convert string representation of list to actual list of strings\n",
    "    answers = resp_str.strip('[]').split(', ')\n",
    "\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d284eb9-c0a4-4ae4-bf7d-e99ff16f6dfa",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B', 'D', 'C', 'D', 'B', 'A', 'B', '0', 'A', '0']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers = take_quiz(summaries[0], quiz)\n",
    "answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7b338d",
   "metadata": {},
   "source": [
    "Finally, score the LLM's answers to the quiz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3724319d-4078-4842-aab7-1274f32765cd",
   "metadata": {
    "height": 183
   },
   "outputs": [],
   "source": [
    "def score_quiz_answers(answers, quiz):\n",
    "    assert len(answers) == len(quiz.questions)\n",
    "\n",
    "    total = len(answers)\n",
    "    correct = 0\n",
    "    for answer, question in zip(answers, quiz.questions):\n",
    "        expected_answer = index_to_letter[question.answer]\n",
    "        if answer == expected_answer:\n",
    "            correct += 1\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00bfebf7-3d36-4df2-9db2-fb4afee39328",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_quiz_answers(answers, quiz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c792e93",
   "metadata": {},
   "source": [
    "Finally, generate rewards and advantages for all 8 summaries you created earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1ddac2a-33ec-4a58-8831-170aee7e2a38",
   "metadata": {
    "height": 166
   },
   "outputs": [],
   "source": [
    "def print_quiz_table(all_answers, rewards):\n",
    "    advantages = compute_advantages(rewards)\n",
    "    length = len(all_answers)\n",
    "    elems = list(zip(range(length), rewards, advantages))\n",
    "\n",
    "    headers = [\"Index\", \"Reward\", \"Advantage\"]\n",
    "    table = tabulate(elems, headers=headers, tablefmt=\"grid\").split(\"\\n\")\n",
    "    for row in table:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9fbd259d-bb52-48ac-84c3-a2313e30fddc",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "all_answers = []\n",
    "quiz_rewards = []\n",
    "for summary in summaries:\n",
    "    answers = take_quiz(summary, quiz)\n",
    "    all_answers.append(answers)\n",
    "    quiz_rewards.append(score_quiz_answers(answers, quiz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fc11146-6d19-4324-aea4-28ae2db28541",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-------------+\n",
      "|   Index |   Reward |   Advantage |\n",
      "+=========+==========+=============+\n",
      "|       0 |      0.4 |    1.1209   |\n",
      "+---------+----------+-------------+\n",
      "|       1 |      0.3 |   -0.160128 |\n",
      "+---------+----------+-------------+\n",
      "|       2 |      0.4 |    1.1209   |\n",
      "+---------+----------+-------------+\n",
      "|       3 |      0.2 |   -1.44115  |\n",
      "+---------+----------+-------------+\n",
      "|       4 |      0.3 |   -0.160128 |\n",
      "+---------+----------+-------------+\n",
      "|       5 |      0.3 |   -0.160128 |\n",
      "+---------+----------+-------------+\n",
      "|       6 |      0.2 |   -1.44115  |\n",
      "+---------+----------+-------------+\n",
      "|       7 |      0.4 |    1.1209   |\n",
      "+---------+----------+-------------+\n"
     ]
    }
   ],
   "source": [
    "print_quiz_table(all_answers, quiz_rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
