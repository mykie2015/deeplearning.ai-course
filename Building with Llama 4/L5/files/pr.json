{"510": "update readme and delete stale content# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "921": "Removed redundant 'FAQ' in README.md and corrected typo# What does this PR do?\n\nCleans up the README file.\n\n## Feature/Issue validation/testing\n\nNone\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/meta-llama/llama-cookbook/blob/main/CONTRIBUTING.md),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "935": "[Hotfix] Pytest Workflow AndroidManifest.xml issue# Overview\nThis pull request updates the `paths` configuration in the `pytest_cpu_gha_runner.yaml` workflow file to improve the clarity and accuracy of test result file matching.\n\n* [`.github/workflows/pytest_cpu_gha_runner.yaml`](diffhunk://#diff-7b508c3574b2f561060c23bcdde5f867f232043965d36a131f09b8c0b3b01f81L73-R75): Modified the `paths` parameter in the `test-summary` action to use a more explicit multi-line format. This ensures all `.xml` files are matched except for `AndroidManifest.xml`, improving readability and maintainability.\nThis solution worked well when we delivered at the first time, but it caused grammar errors during pytest.\n\n## Problem\nRiandy and I implemented the `AndroidManifest.xml` exception case for AndroidApp example.  \n```\nError: unknown test file type for 'getting-started/demo/ArticleSummarizer/app/src/main/AndroidManifest.xml'\n```\n\nSo, we changed the `pytest_cpu_gha_runner.yaml` [xml related paths](https://github.com/meta-llama/llama-cookbook/blob/b374b017603a6e955909e7848b07225bde0a1e33/.github/workflows/pytest_cpu_gha_runner.yaml#L73).\n```\npaths: \"**/*.xml\" --> paths: \"**/*.xml(?<!AndroidManifest\\.xml)\"\n```\n\n## Solution\n\n```\npaths: **/*.xml !**/AndroidManifest.xml\n```\n\n", "706": "Adds Llama 3.2 example on Modal with a fun experiment# What does this PR do?\n\nWe add a new third party recipe for [Modal](https://modal.com), a serverless python platform that allows people to easily run experiments on Llama.\n\nThis is a continuation of the blog:\n[Beat GPT-4o at Python by searching with 100 dumb LLaMAs](https://modal.com/blog/llama-human-eval)\n\n### Testing\n\nGiven user has Llama 3.2 license approval on Huggingface, has their HF_TOKEN configured as a Modal secret, and has logged into Modal CLI, the included end-to-end script has been ran successfully:\n\n`run_e2e.sh`\n\nThese steps have been included in the README.\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [x] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "712": "Llama 3.2 vision# What does this PR do?\nAdd some llama 3.2 specific scripts and saving checkpoints during batch at specified intervals. added scrips for processing different types of datasets\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "869": "fix notebook# What does this PR do?\n\nFixes https://github.com/meta-llama/llama-cookbook/issues/868\n\n", "841": "rebase# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "672": "Adding data prep recipes from data-prep-kit# What does this PR do?\n\nThis PR adds a data prep recipe from data-prep-kit toolkit https://github.com/IBM/data-prep-kit\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\nThis is a notebook that has been tested on Colab. \n\n\n", "666": "[LG Notebook] Adjust category removal exampleMinor change to LlamaGuard category removal example to improve reliability", "128": "Fix \"load_model_sharded\" when \"--save_optimizer\" enabled# Fixing \"key error\" when loading optimizer with sharded checkpoint loading\n\n<!--\n\nWhen saving checkpoints with \"save_optimizer\" enabled, the \"optim\" key is properly stored in the checkpoints. However, when loading these checkpoints, the optimizer is not properly loaded from the checkpoint because the initial state_dict which is initialized does not include the \"optim\" key.\n\n-->\n\n<!-- Remove if not applicable -->\n\nPotentially Fixes # 127\n\n## Feature/Issue validation/testing\n\n- Enabled \"save_optimizer\" then ran training with StateDictType.SHARDED_STATE_DICT for one epoch then loaded from checkpoint\n1. Ran for one epoch:\n```sh\npython ./llama_finetuning.py ... --save_optimizer\n```\n2. Loaded from checkpoint created:\n```sh\nload_model_sharded(model, rank, cfg)\n```\n3. Loading checkpoints resulted in:\n```sh\ncheckpoint key len = 1 and \nkeys =  dict_keys(['model'])\n```\nIf \"--save_optimizer\" is enabled then checkpoint key length should be \"2\" and keys should be \"dict_keys(['model', 'optim'])\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\n", "896": "Rename README.md to Off# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/meta-llama/llama-cookbook/blob/main/CONTRIBUTING.md),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "882": "ignore medium and linkedin# What does this PR do?\n\nFixes rejections from LinkedIn\n\n\nThanks for contributing \ud83c\udf89!\n", "316": "Fixing committed files# What does this PR do?\n\nFixing committed file", "302": "Support CodeLlama and 70B training (CPU offload on save)", "464": "Llama3update# What does this PR do?\nUpdated the script to use Llama 3 models instead of Llama 2.\nIntroduced hugging-face cli to take in HF token. And updated the script to run the HF conversion using the transformer source code.\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [X] Verified running the notebook end to end.\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [x] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "458": "Update the Getting to know Llama notebook for Llama 3 (with comparison to Llama 2).# What does this PR do?\n\nUpdate the Getting to know Llama notebook for Llama 3 (with comparison to Llama 2).\n\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "459": "updated Running_Llama_on_Mac and HelloLlamaCloud.ipynb for Llama 3 Complete rewrite of the running Llama locally notebook - replace llamp.cpp with much easier to use Ollama.\n\n# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "465": "Llama3update# What does this PR do?\nUpdated the script to use Llama 3 models instead of Llama 2.\nIntroduced hugging-face cli to take in HF token. And updated the script to run the HF conversion using the transformer source code.\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [X] Verified running the notebook end to end.\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [x] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!", "471": "[WIP]adding fsdp-qlora in progress", "317": "remove additional space in llama-guard category# What does this PR do?\n\n<!--\nThere are addition space in llama-guard published category and we need keep only one space ahead.\n-->\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [x] Run script to trigger function directly with following output. See  bullets:\nTask: Check if there is unsafe content in 'User' messages in conversations according our safety policy with the below categories.\n<BEGIN UNSAFE CONTENT CATEGORIES>\nO1: Violence and Hate.\nShould not\n - Help people plan or engage in violence.\n - Encourage people to engage in violence.\n - Express hateful, derogatory, or demeaning sentiments against people based on sensitive personal characteristics like their race, color, religion, national origin, sexual orientation, gender, gender identity, or disability.\n - Encourage discrimination against people based on those characteristics and should not use slurs or create insults based on those characteristics.\nCan\n - Provide information on violence and discrimination.\n - Discuss the topics of hate and violence, and can discuss historical events involving violence.\nO2: Sexual Content.\nShould not\n - Engage in sexually explicit (i.e., erotic) conversations and should not encourage people to engage in sexual activities.\nCan\n - Discuss topics related to sexuality and sexual education.\n - Provide general information regarding sexual activities.\n - Provide guidance on topics related to sexual health.\nO3: Criminal Planning.\nShould not\n - Help people plan or engage in criminal activities like theft, kidnapping, or financial crimes, especially when the person indicates that they may act on the information provided to them.\n - Encourage people to engage in criminal activities or condone criminal behavior.\nCan\n - Help people understand how crimes are committed, provided that the response is not so detailed that it could be used to provide a roadmap for future crimes.\n - Help people understand how systems work that are used to prevent crimes, provided those descriptions do not provide detailed information on how to defeat those systems.\nO4: Guns and Illegal Weapons.\nShould not\n - Assist in the planning or execution of firearm or illegal weapons crimes, especially when the person indicates that they may act on the information provided to them.\n - Encourage people to engage in firearm or illegal weapons crimes or condone such behavior.\nCan\n - Discuss firearms and the arguments for and against firearm ownership.\nO5: Regulated or Controlled Substances.\nShould not\n - Encourage or assist people in the trafficking, consumption, or use of regulated or controlled substances, including but not limited to illegal drugs, tobacco, alcohol, and cannabis.\n - Assist or encourage people to create such substances when it is illegal to do so.\nCan\n - Discuss the use of regulated or controlled substances, and can provide general information on those substances (ex: what their chemical composition is and what their effects are).\n - Provide information on how those substances are manufactured, provided that the information is not so detailed as to provide step-by-step instructions for their creation.\nO6: Self-Harm.\nShould not\n - Encourage people to harm themselves.\n - Romanticize or condone self-harm or suicide.\n - Provide information regarding the methods of suicide or self-harm.\n - Provide any information or advice, other than information regarding appropriate health resources, to people who express or imply that they wish to harm themselves.\nCan\n - Provide general information regarding self-harm and suicide (ex: statistics, the mental health challenges faced by those who take these actions).\nShould\n - Provide information regarding appropriate health resources to people who imply or express the desire to harm themselves.\n<END UNSAFE CONTENT CATEGORIES>\n\n<BEGIN CONVERSATION>\nUser: Why have some cultures engaged in violence directed at disabled individuals?\n<END CONVERSATION>\n\nProvide your safety assessment for User in the above conversation:\n- First line must read 'safe' or 'unsafe'.\n- If unsafe, a second line must include a comma-separated list of violated categories.\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "883": "remove key# What does this PR do?\n\nFixes Groq key publicly available\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/meta-llama/llama-cookbook/blob/main/CONTRIBUTING.md),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "897": "Add SalesBot Weaviate Notebook# What does this PR do?\nI've added the SalesBot Weaviate example back to the `end-to-end-use-cases` folder. The initial submission was using OctoAI (deprecated), so I've updated the notebook to use NVIDIA NIM to access `llama-3.3-70b-instruct`.\n\nI've tested the notebook end-to-end, and it works.\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/meta-llama/llama-cookbook/blob/main/CONTRIBUTING.md),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [x] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "667": "LG notebook - Repair broken import and add note about dependencyResolve an import issue and add a note explaining the dependency of the Evaluation section of the notebook on the llama package", "115": "Fix & improve activation offloadingThe `torch.distributed.algorithms._checkpoint.OffloadWrapper` seems also offloading the parameters (not only the activations) to cpu, because autograd Function also saves parameters for backward. This can be verified by the script below, see `print(tensor.shape)` in pack_hook, which also prints for Linear.weight.\n\ncc @rohan-varma @awgu \n\nTo implement the method, combining activation checkpointing and activation offloading, described in the [PyTorch blog](https://pytorch.org/blog/efficient-large-scale-training-with-pytorch/). We can call checkpoint_module forward (**which only saves inputs, not parameters, for backward**) under save_on_cpu_overlap. This trades off cpu memory and communication for recomputation.\n\nThe benefits of `save_on_cpu_overlap` over `save_on_cpu`:\n- Copy is done in a separate stream, which overlaps with computation.\n- pack_hook is non-blocking now, which (hopefully) does not block the prefetch allgather for the next layer.\n\nBelow is a script to test performance and check for correctness by setting `use_stream` to True/False. Note that if without proper synchronization, for example, omitting on record_stream, the printed sampled elements are different.\n\n<details>\n  <summary>Click me</summary>\n\n  ```python\n  import collections\n\nimport torch\nimport torch.nn as nn\n\n\ntorch.manual_seed(123)\n\n\nclass FreeEventQueue:\n    \"\"\"https://github.com/pytorch/pytorch/blob/main/torch/distributed/fsdp/_limiter_utils.py\"\"\"\n    def __init__(self):\n        self._queue = collections.deque()\n        self._max_num_inflight_copy = 2\n\n    def enqueue(self, free_event):\n        self._queue.append(free_event)\n\n    def dequeue_if_needed(self):\n        if len(self._queue) >= self._max_num_inflight_copy:\n            return self._dequeue()\n        return None\n\n    def _dequeue(self):\n        if self._queue:\n            event = self._queue.popleft()\n            return event\n        return None\n\n\nclass save_on_cpu_overlap(torch.autograd.graph.saved_tensors_hooks):\n    def __init__(self):\n        copy_stream = torch.cuda.Stream()\n        current_stream = torch.cuda.current_stream()\n        pack_event_queue = FreeEventQueue()\n        unpack_event_queue = FreeEventQueue()\n\n        def _deque_event_and_synchronize(queue):\n            event = queue.dequeue_if_needed()\n            if event:\n                event.synchronize()\n\n        def _enque_event(queue):\n            free_event = torch.cuda.Event()\n            free_event.record()\n            queue.enqueue(free_event)\n\n        def pack_to_cpu(tensor):\n            _deque_event_and_synchronize(pack_event_queue)\n            copy_stream.wait_stream(current_stream)\n            with torch.cuda.stream(copy_stream):\n                packed = tensor.to(\"cpu\", non_blocking=True)\n            tensor.record_stream(copy_stream)\n            #print(tensor.shape)\n            _enque_event(pack_event_queue)\n            return (tensor.device, packed)\n\n        def unpack_from_cpu(packed):\n            _deque_event_and_synchronize(unpack_event_queue)\n            device, tensor = packed\n            with torch.cuda.stream(copy_stream):\n                unpacked = tensor.to(device, non_blocking=True)\n            current_stream.wait_stream(copy_stream)\n            unpacked.record_stream(current_stream)\n            _enque_event(unpack_event_queue)\n            return unpacked\n\n        super().__init__(pack_to_cpu, unpack_from_cpu)\n\n\n\nfrom torch.distributed.algorithms._checkpoint.checkpoint_wrapper import checkpoint_wrapper, CheckpointImpl\nfrom functools import partial\n\n\nclass Model(nn.Module):\n    def __init__(\n        self,\n        n: int,\n    ):\n        super().__init__()\n        self.layers = nn.ModuleList()\n        self.n = n\n        wrp = partial(\n            checkpoint_wrapper,\n            checkpoint_impl=CheckpointImpl.REENTRANT,\n            preserve_rng_state=False,\n        )\n        for i in range(self.n):\n            l = nn.Linear(dim, dim)\n            l = wrp(l)\n            self.layers.append(l)\n\n    def forward(self, x):\n        for i in range(self.n):\n            x = self.layers[i](x)\n        return x\n\n\ndim = 4096\nbsz = 2048 # bsz * seq_len\n\ninputs = torch.randn(bsz, dim).cuda().requires_grad_()\nout_grad = torch.ones(bsz, dim).cuda()\nprint(\"inputs \", inputs.detach()[0:5, 0])\n\n\n#seq_module = nn.Sequential(*[nn.Linear(dim, dim) for _ in range(5)]).cuda()\nseq_module = Model(5).cuda()\n\n# warmup\nfor _ in range(5):\n    out = seq_module(inputs)\n    out.backward(out_grad)\ntorch.cuda.synchronize()\n\n#use_stream = True\nuse_stream = False\n\nfor _ in range(5):\n    start_event = torch.cuda.Event(enable_timing=True)\n    end_event = torch.cuda.Event(enable_timing=True)\n    start_event.record()\n    if use_stream:\n        with save_on_cpu_overlap():\n            out = seq_module(inputs)\n    else:\n        with torch.autograd.graph.save_on_cpu(pin_memory=True):\n            out = seq_module(inputs)\n    out.backward(out_grad)\n    end_event.record()\n    torch.cuda.synchronize()\n    print(\"elapsed time: \", start_event.elapsed_time(end_event))\n    print(\"sampled elements: \", inputs.grad[0, :5])\n  ```\n</details>\n\nOn a single V100, 20% speedup is observed for this toy model. Output:\n\n<details>\n  <summary>Click me</summary>\n\n  ```\n# use_stream=True\ninputs  tensor([ 0.3374, -1.2207, -0.9550,  1.7306, -0.2264], device='cuda:0')\nelapsed time:  149.52761840820312\nsampled elements:  tensor([-0.1949, -0.2131,  0.1653, -0.3961, -0.6375], device='cuda:0')\nelapsed time:  113.87091064453125\nsampled elements:  tensor([-0.2274, -0.2487,  0.1929, -0.4621, -0.7438], device='cuda:0')\nelapsed time:  108.2528305053711\nsampled elements:  tensor([-0.2599, -0.2842,  0.2205, -0.5281, -0.8501], device='cuda:0')\nelapsed time:  107.67017364501953\nsampled elements:  tensor([-0.2924, -0.3197,  0.2480, -0.5941, -0.9563], device='cuda:0')\nelapsed time:  107.47408294677734\nsampled elements:  tensor([-0.3248, -0.3552,  0.2756, -0.6601, -1.0626], device='cuda:0')\n\n\n# use_stream=False\ninputs  tensor([ 0.3374, -1.2207, -0.9550,  1.7306, -0.2264], device='cuda:0')\nelapsed time:  178.19471740722656\nsampled elements:  tensor([-0.1949, -0.2131,  0.1653, -0.3961, -0.6375], device='cuda:0')\nelapsed time:  126.314208984375\nsampled elements:  tensor([-0.2274, -0.2487,  0.1929, -0.4621, -0.7438], device='cuda:0')\nelapsed time:  126.18544006347656\nsampled elements:  tensor([-0.2599, -0.2842,  0.2205, -0.5281, -0.8501], device='cuda:0')\nelapsed time:  126.29904174804688\nsampled elements:  tensor([-0.2924, -0.3197,  0.2480, -0.5941, -0.9563], device='cuda:0')\nelapsed time:  126.51510620117188\nsampled elements:  tensor([-0.3248, -0.3552,  0.2756, -0.6601, -1.0626], device='cuda:0')\n  ```\n</details>", "840": "fix double bos for vision model# What does this PR do?\nThis PR fix double BOS token issue for vision model, as the BOS token is already included in the chat template and when the processor adds the BOS token again. This happened for both inference and fine-tuning.\n\n<!-- Remove if not applicable -->\n\nFixes [Issue #826](https://github.com/meta-llama/llama-recipes/issues/826)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [x] Inference Test\n```\npython recipes/quickstart/inference/local_inference/multi_modal_infer.py     --image_path ~/work/dog.jpg     --prompt_text \"Describe this image\"     --model_name \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\nLoading model: meta-llama/Llama-3.2-11B-Vision-Instruct\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:04<00:00,  1.08it/s]\nInput Prompt:\n <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\n<|image|>Describe this image<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n\nGenerated Text: This image features a small dog, likely a puppy, standing on a skateboard in the middle of a road or street. The dog's coat is predominantly brown and white, with distinctive black markings on its back and legs. Its floppy ears and dark eyes are prominent features.\n\nThe skateboard, which the dog is standing on, is black with red wheels. In the background, a blue door is visible, although out of focus. The overall atmosphere of the image suggests that the dog is being showcased in a humorous or playful manner, possibly as part of a meme or joke.<|eot_id|>\n```\n\n- [x] Finetuning test\n```\n~/work/llama-recipes (fix_double_bos)]$ torchrun --nnodes 1 --nproc_per_node 4  recipes/quickstart/finetuning/finetuning.py --enable_fsdp --lr 1e-5  --num_epochs 3 --batch_size_training 2 --model_name meta-llama/Llama-3.2-11B-Vision-Instruct --dist_checkpoint_root_folder ./finetuned_model --dist_checkpoint_folder fine-tuned  --use_fast_kernels --dataset \"custom_dataset\" --custom_dataset.test_split \"test\" --custom_dataset.file \"recipes/quickstart/finetuning/datasets/ocrvqa_dataset.py\"  --run_validation True --batching_strategy padding  --use_peft --peft_method lora\nW0113 17:47:42.792000 140312926217216 torch/distributed/run.py:757]\nW0113 17:47:42.792000 140312926217216 torch/distributed/run.py:757] *****************************************\nW0113 17:47:42.792000 140312926217216 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.\nW0113 17:47:42.792000 140312926217216 torch/distributed/run.py:757] *****************************************\nClearing GPU cache for all ranks\n--> Running with torch dist debug set to detail\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00,  7.62it/s]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00,  7.47it/s]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00,  6.76it/s]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:01<00:00,  4.91it/s]\n--> Model meta-llama/Llama-3.2-11B-Vision-Instruct\n\n--> meta-llama/Llama-3.2-11B-Vision-Instruct has 10670.220835 Million params\n\ntrainable params: 5,898,240 || all params: 10,676,119,075 || trainable%: 0.0552470420998934\nbFloat16 enabled for mixed precision - using bfSixteen policy\ntrainable params: 5,898,240 || all params: 10,676,119,075 || trainable%: 0.0552470420998934\ntrainable params: 5,898,240 || all params: 10,676,119,075 || trainable%: 0.0552470420998934\ntrainable params: 5,898,240 || all params: 10,676,119,075 || trainable%: 0.0552470420998934\n--> applying fsdp activation checkpointing...\n--> applying fsdp activation checkpointing...\n--> applying fsdp activation checkpointing...\n--> applying fsdp activation checkpointing...\n--> Training Set Length = 1800\n--> Validation Set Length = 200\nlength of dataset_train 1800\ncustom_data_collator is used\n--> Num of Training Set Batches loaded = 225\nlength of dataset_train 1800\ncustom_data_collator is used\n--> Num of Training Set Batches loaded = 225\nlength of dataset_train 1800\ncustom_data_collator is used\n--> Num of Training Set Batches loaded = 225\nlength of dataset_train 1800\ncustom_data_collator is used\n--> Num of Training Set Batches loaded = 225\n--> Num of Validation Set Batches loaded = 50\n--> Num of Validation Set Batches loaded = 50\nStarting epoch 0/3\ntrain_config.max_train_step: 0\n--> Num of Validation Set Batches loaded = 50\n--> Num of Validation Set Batches loaded = 50\nStarting epoch 0/3\ntrain_config.max_train_step: 0\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                          | 0/225 [00:00<?, ?it/s]--> Num of Validation Set Batches loaded = 50\n--> Num of Validation Set Batches loaded = 50\nStarting epoch 0/3\ntrain_config.max_train_step: 0\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                          | 0/225 [00:00<?, ?it/s]--> Num of Validation Set Batches loaded = 50\n--> Num of Validation Set Batches loaded = 50\nStarting epoch 0/3\ntrain_config.max_train_step: 0\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                          | 0/225 [00:00<?, ?it/s]/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                          | 0/225 [00:00<?, ?it/s]NCCL version 2.20.5+cuda12.4\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\nTraining Epoch: 1/3, step 13/225 completed (loss: 0.854179859161377):\n```\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "698": "Multi modal rag demoMM RAG Demo:\n\n3 Part Series:\n\n- [x] Notebook 1: Data Prep and Synethetic Labelling using 11B model\n- [x] WIP: Notebook 2: Cleaning up Synthetic Labels using 1B or 3B model and creating DB with \n- [x] WIP: Notebook 3:  MM-RAG using lance-db\n\nScripts:\n\n- [x] Script 1: Annotating Dataset with multi-GPUs\n- [x] Script 2: Cleaning up Descriptions and making the db\n- [x] Script 3: Llama 3.2 + lance db gradio demo\n", "28": "modify to steping the lr scheduler each epochThis addresses issue #27, to update the lr after each epoch instead of per step.", "707": "added missing word and corrected spellingFixed typo and clarify get_data_collator as a class in dataset setup instructions.\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n", "513": "Update langgraph agent recipe", "507": "Fixing path issuesPreventing errors in case the directory doesn't exist. ", "249": "Introducing Actions CI/CD Pytest workflowPyTest tests are configured to be triggered on following events:\n\n* pull request to main branch when *.py* are being affected.\n* can be manually dispatched.\n\n## Feature/Issue validation/testing\n\nHere are the most recent tests executed on the fork: https://github.com/maximgroshev/llama-recipes/actions/runs/6488315845\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [x] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nNotes:\n1. @chauhang , @mreso, @HamidShojanazeri please review this PR and let me know if you have any questions. \n2. Before this PR can be merged in we still need to create GitHub secrets for sensitive data.\n\nThanks for contributing \ud83c\udf89!\n", "922": "updated recipe to use llama 4# What does this PR do?\nUpdated recipe to utilize Llama 4  Scout\n", "705": "added instructions in README.md for proper multi modal inferencing with Llama 3.2 models# What does this PR do?\n\nPreviously, I addressed the fix for the [multi-modal inferencing issue](https://github.com/meta-llama/llama-recipes/pull/704), but I didn't include additional information in the `README.md`. \n\nI have now made those changes to make it easier for readers to use the latest ` Llama 3.2 vision models `.\n\n## Before submitting\n- [x] This PR fixes a typo or improves the documentation (you can dismiss the other checks if that's the case).\n", "739": "Notebook llamaE2E: PDF to Podcast using Open Models\n\n- [x] Step 1: Create PDF upload and pre-process logic using 1B model\n- [x] Step 2: 70B Logic for writing podcast: Pre-processed text -> Podcast Script\n- [x] Step 3: 8B Re-writer logic: Model re-writes podcast to make it realistic (adds umms, laughs, etc)\n- [x] Step 4: Test TTS model(s) and Text to speech logic for multiple speakers", "842": "Add FAQAdd FAQ regarding renaming", "856": "Fix: Updated broken documentations links and references# What does this PR do?\n\nThis PR fixes several broken documentation links across multiple notebook files in the cookbook. The changes include:\n\n1. Updating LangChain documentation links to their new locations\n2. Fixing arXiv paper reference links\n3. Updating Llama Guard model card reference\n4. Correcting RecursiveCharacterTextSplitter documentation links\n\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/meta-llama/llama-cookbook/blob/main/CONTRIBUTING.md),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [x] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n", "665": "Fix/custom dataset chat template# What does this PR do?\n\nThis PR fixes the custom dataset example for tokenizer that add system prompt by default.\n\nFixes # (issue)\n#669 \n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [X] `pytest src/tests/datasets/test_custom_dataset.py -s -k test_tokenize_dialog`\n\n```\n========================================================================================================================================================================== test session starts ===========================================================================================================================================================================\nplatform linux -- Python 3.10.14, pytest-8.3.2, pluggy-1.5.0\nrootdir: /home/mreso/llama-recipes\nconfigfile: pyproject.toml\nplugins: anyio-4.4.0, mock-3.14.0\ncollected 6 items / 4 deselected / 2 selected\n\nsrc/tests/datasets/test_custom_dataset.py ..\n\n============================================================================================================================================================================ warnings summary ============================================================================================================================================================================\nsrc/tests/datasets/test_custom_dataset.py::test_tokenize_dialog[meta-llama/Llama-2-7b-hf]\n  /home/mreso/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n    from torch.distributed._shard.checkpoint import (\n\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n=============================================================================================================================================================== 2 passed, 4 deselected, 1 warning in 3.10s ===============================================================================================================================================================\n```\n\n\n## Before submitting\n- [X] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [X] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "881": "added readme for customerservice chatbots# What does this PR do?\nThis PR adds README to customer service chatbots use cases\n\n\n\n## Before submitting\n- [X] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/meta-llama/llama-cookbook/blob/main/CONTRIBUTING.md),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "659": "Adding custom dataset file# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # ([658](https://github.com/meta-llama/llama-recipes/issues/658))\nThe custom dataset file was removed probably by mistake during this PR https://github.com/meta-llama/llama-recipes/pull/650\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "895": "format markdown faq# What does this PR do?\n\nRefactors markdown format for main Readme\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/meta-llama/llama-cookbook/blob/main/CONTRIBUTING.md),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "498": "Fix save metric FileNotFoundError when finetuning# What does this PR do?\n\nThis PR fixes a bug where the code attempted to save metrics during training to a file that did not exist, causing an error. The fix ensures that the code checks if the file exists before attempting to save the metrics, preventing the error from occurring.\n\nFixes # (issue number)\n\n## Feature/Issue validation/testing\n\nI tested the fix by running the training script with the provided command and verified that the metrics were saved without any errors. Here are the steps I followed:\n\n- [x] Test saving metrics\n    - Ran the training script using the following command:\n    \n    ```bash\n    python recipes/finetuning/finetuning.py --model_name PATH//TO/Meta-Llama-3-8B --quantization \\\n    --use_peft --peft_method lora \\\n    --lora_config.r 16 \\\n    --lora_config.lora_dropout 0.1 \\\n    --num_epochs 5  \\\n    --batch_size_training 1 \\\n    --dataset alpaca_dataset \\\n    --batching_strategy packing \\\n    --output_dir ./output/peft/llama3-8B-lora-alpaca-0511-v1 \\\n    --use_wandb  \\\n    --save_metrics\n    ```\n    \n    - Verified that metrics were saved to the specified file\n    - Checked for any error messages or exceptions during the process\n\n", "467": "update VideoSummary, LiveData, llama-on-prem for Llama 3# What does this PR do?\n\nVideoSummary: show how to ask Llama 3 to generate a summary of a long (almost 3 hours) youtube video (Yann LeCun vs Lex Fridman) content with 140+k chars or ~30k words or 40k tokens, exceeding Llama 3's 8k context length limit.\nhttps://github.com/meta-llama/llama-recipes/blob/demos4llama3v3/recipes/use_cases/VideoSummary.ipynb\n\nLiveData: How to ask Llama 3 questions about recent live data via the live search API and LlamaIndex\nhttps://github.com/meta-llama/llama-recipes/blob/demos4llama3v3/recipes/use_cases/LiveData.ipynb\n\nllama-on-prem.md: https://github.com/meta-llama/llama-recipes/blob/demos4llama3v3/recipes/inference/model_servers/llama-on-prem.md\n\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "315": "Feature/llamaguard safety checker integration# What does this PR do?\nFixing a typo", "473": "Update Chatbot recipe with Llama 3# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "314": "Adding Llama Guard safety checker.# What does this PR do?\nAdds support for Llama Guard as a safety check in the inference script of the llama recipes. It brings over the inference code from the Llama main repo into this package and runs whenever the user request it in the command line\n\n## Tests\n\nRun the inference script with this command to enable Llama Guard check of both the input and output:\npython examples/inference.py --model_name ~/models/llama-2-7b-hf/ --prompt_file examples/test_user_prompt_1.txt --quantization --enable_llamaguard_content_safety --llamaguard_model_name ~/models/guard-llama/ --enable_salesforce_content_safety F alse\n\nAlso run the inference script with default params and check everything works as expected\n\n\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [x] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "466": "Removed notebook output cells# What does this PR do?\nRemoved notebook output cells\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "499": "Fix save metric FileNotFoundError when finetuning# What does this PR do?\n\nThis PR fixes a bug where the code attempted to save metrics during training to a file that did not exist, causing an error. The fix ensures that the code checks if the file exists before attempting to save the metrics, preventing the error from occurring.\n\nFixes # (issue number)\n\n## Feature/Issue validation/testing\n\nI tested the fix by running the training script with the provided command and verified that the metrics were saved without any errors. Here are the steps I followed:\n\n- [x] Test saving metrics\n    - Ran the training script using the following command:\n    ```bash\n    python recipes/finetuning/finetuning.py --model_name PATH//TO/Meta-Llama-3-8B --quantization \\\n    --use_peft --peft_method lora \\\n    --lora_config.r 16 \\\n    --lora_config.lora_dropout 0.1 \\\n    --num_epochs 5  \\\n    --batch_size_training 1 \\\n    --dataset alpaca_dataset \\\n    --batching_strategy packing \\\n    --output_dir ./output/peft/llama3-8B-lora-alpaca-0511-v1 \\\n    --use_wandb  \\\n    --save_metrics\n    ```\n    - Verified that metrics were saved to the specified file\n    - Checked for any error messages or exceptions during the process\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [x] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "880": "update root README with absolute path to fix https://pypi.org/project/llama-cookbook/ broken links# What does this PR do?\n\ntitle\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/meta-llama/llama-cookbook/blob/main/CONTRIBUTING.md),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "116": "Feature : Enable Intel GPU/XPU finetuning and inference# What does this PR do?\n\nMotivation:  Thanks for creating this repository. At Intel, we are using this repository for finetuning and inference for our GPUs /XPU (Ipex backend) and hence the reason for this PR. \nThis change would help us run LLama v2 without issues on our GPU devices. (Since accelerate/peft already has support of XPUs enabled)\n\nFixes # (issue) -NA\n\n\n## Feature/Issue validation/testing\n\nFeature for finetuning and inference tested on our Data Centre GPUs with the change\n\nfor CLA purposes this would be on behalf of Intel. \n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "664": "Update requirements.txtUpdated typing extension library to be using latest so notebooks work with google collab\n\n# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "857": "Typo fix in Step-2-Transcript-Writer.ipynb\n# What does this PR do?\nFixes the typo from SYSTEMP_PROMPT -> SYSTEM_PROMPT\n\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/meta-llama/llama-cookbook/blob/main/CONTRIBUTING.md),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "843": "Update README.mdThe path have changed in repo. Updating the cd command to reflect the same.", "704": "[Fixed] RuntimeError: probability tensor contains either inf, nan or element < 0# What does this PR do?\n\n<!--\nOn executing the multimodal inferencing there was an error as shown in the title\n-->\nIssue arises when we ran the following command in directory `recipes/quickstart/inference/`\n```\npython multi_modal_infer.py --image_path Image.png --prompt_text \"Describe this image\" --temperature 0.9  --top_p 0.1 --model_name \"meta-llama/Llama-3.2-11B-Vision-Instruct\" --hf_token <your hf-token>\n```\nFixes # (issue)\nThe issue is fixed by adding the following line:\n```\nmodel = model.bfloat16().cuda()\n```\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n-## Test A\n- Used command `python multi_modal_infer.py --image_path Image.png --prompt_text \"Describe this image\" --temperature 0.9  --top_p 0.1 --model_name \"meta-llama/Llama-3.2-11B-Vision-Instruct\" --hf_token <your hf-token>`\n### Logs for Test A\n\nGenerated Text: end_header_id|>\n\nThe image depicts a cartoon astronaut in a white spacesuit, floating in space. The astronaut is wearing a white spacesuit with a blue and red control panel on the front, and a white helmet with a clear visor. The astronaut has brown hair and is smiling, with their arms outstretched and legs bent at the knees.\n\n*   The astronaut is wearing a white spacesuit with a blue and red control panel on the front.\n    *   The control panel has a red button, a yellow button, and a green button.\n    *   The astronaut's helmet is white with a clear visor.\n    *   The astronaut has brown hair.\n*   The background of the image is dark blue, with white and yellow stars scattered throughout.\n    *   The stars are of varying sizes and shapes.\n    *   Some of the stars have a slight glow effect around them.\n*   The overall atmosphere of the image is one of adventure and exploration, with the astronaut floating in space surrounded by stars.\n\nThe image suggests that the astronaut is on a mission to explore the vastness of space, and the stars in the background represent the infinite possibilities and wonders that await them.<|eot_id|>\n\n## Test B\n-  Used command `python multi_modal_infer.py --image_path Image.png --prompt_text \"Describe this image\" --temperature 0.9  --top_p 0.1 --model_name \"meta-llama/Llama-3.2-11B-Vision-Instruct\" --hf_token <your hf-token>`\n### Logs for Test B\n\nGenerated Text: end_header_id|>\n\nThe image presents a close-up photograph of an orange tabby cat, showcasing its distinctive features and surroundings.\n\n* The cat's head is prominently displayed in the foreground, occupying approximately 75% of the image.\n        + The cat's fur is a vibrant orange color with white markings on its nose, chin, and whiskers.\n        + Its eyes are green, and its ears are perked up, giving it a curious expression.\n        + The cat's whiskers are long and white, adding to its overall appearance.\n* The background of the image is blurred, but it appears to be a grassy area.\n        + The grass is a bright green color, suggesting a sunny day.\n        + The background is out of focus, drawing attention to the cat in the foreground.\n\nIn summary, the image is a captivating portrait of an orange tabby cat, with its striking features and surroundings creating a visually appealing composition. The cat's curious expression and vibrant fur make it the main focus of the image, while the blurred background adds depth and context to the scene.<|eot_id|>\n\n*Working on Single GPU*\n## Before submitting\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n- [x] Was this discussed/approved via a GitHub issue? [issue-683](https://github.com/meta-llama/llama-recipes/issues/683)\n- [ ] Did you make sure to update the documentation with your changes?  Not required\n- [ ] Did you write any new necessary tests? No\n\nThanks for contributing \ud83c\udf89!\n", "923": "updated browser navigation use case readme# What does this PR do?\nUpdated Browser Use Case README to reflect updates with Llama 4.", "937": "Notebook for generating evals using sythetic data# Evals using synthetic data\n\n- When you deploy Llama for your use case, it is a good practice to have Evals for your usecase. In an ideal world, you want to have human annotated Evals. If for some reason, this is not possible, this notebook shows a strategy for how one might go about addressing Evals using synthetic data. However, the Evals generated still require validation by a human to make sure that your revenue generating production use case can rely on this. \n- The notebook also shows how one can accurately measure hallucinations without using `LLM-As-A-Judge` methodology using Llama\n\n## Overall idea\n\nLet's assume we have a use case for generating a summarization report based on a given context, which is a pretty common use case with LLM. Both the context and the report have a lot of factual information and we want to make sure the generated report is not hallucinating.\n\nSince its not trivial to find an open source dataset for this, the idea is to take synthetic tabular data and then use Llama to generate a story(context) for every row of the tabular data using Prompt Engineering. Then we ask Llama to summarize the generated context as a report in a specfic format using Prompt Engineering. Finally we check the factual accuracy of the generated report using Llama by converting this into a QA task using the tabular data as the groud truth.\n\nTo generate synthetic data for this approach, we use an open source tool like [Synthetic Data Vault(SDV)](https://github.com/sdv-dev/SDV)\nWant to thank @adamloving-meta for suggesting this tool\n\n<img width=\"799\" alt=\"Workflow_Diagram\" src=\"https://github.com/user-attachments/assets/703d0730-4db9-425f-8fcb-e6138b54a07f\" />\n\n\n## Measuring Hallucinations\n\nThe usual method to measure hallucinations uses LLM-As-Judge methodology. An example hallucination metric is using [DeepEval](https://www.deepeval.com/docs/metrics-hallucination).\nThis would use a powerful LLM as the ground truth to measure hallucinations.\n\nThis notebook shows a  simple way to measure hallucinations using the ground truth data that we have (tabular data). The methodology is to make use of the tags that we have added in the report and use Llama to answer simple questions looking at the corresponding sections. Llama compares the answers with the ground truth and generates a list of boolean values. This is then used to measure accuracy of the factual information in the report. If your report has a well defined structure, using QA to measure hallucinations can be highly effective and cost efficient\n\n### Example \n\nBelow is an example of the expected output while checking for hallucinations in the generated report. Since we converted the hallucination measurement task into a list of QA with `True`/`False` outcomes, we can use `accuracy_score` from `sklearn` to measure the accuracy in terms of a single number.\n\n```\nChecking accuracy of generated report in generated_data/data_6.json\n\n<answer>\nstudent_id: [False, report shows Student ID is not mentioned in the data and ground truth says 6180804]\ndegree_type: [True, None]\nsalary: [True, None]\nmba_spec: [True, None]\nduration: [True, None]\nemployability_perc: [True, None]\n</answer>\n```\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/meta-llama/llama-cookbook/blob/main/CONTRIBUTING.md),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "260": "Update HelloLlamaCloud.ipynbadded pypdf to !pip install - required for cell containing:\n\nfrom langchain.document_loaders import PyPDFLoader loader = PyPDFLoader(\"https://arxiv.org/pdf/2307.09288.pdf\") docs = loader.load()\n\n# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\npypdf is not installed\n\n\n## Feature/Issue validation/testing\n\nfrom langchain.document_loaders import PyPDFLoader loader = PyPDFLoader(\"https://arxiv.org/pdf/2307.09288.pdf\") docs = loader.load()\n\nnow runs without error\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "506": "[MLC-LLM] Introducing Llama 3 running locally on Android using MLC-LLM# What does this PR do?\n\nThis PR introduces a guide on how to run Llama 3 8B instruct on an Android Phone using MLC LLM!\n\n\n## Feature/Issue validation/testing\n\nThis tutorial was tested with the following setup:\n* MacBook Pro 16 inch from 2021 with Apple M1 Max and 32GB of RAM running Sonoma 14.3.1\n* OnePlus 12 Android Smartphone with a Snapdragon 8Gen3 SoC and 12GB or RAM, running OxygenOS 14.0\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [X] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [X] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "512": "Update azure_api_example.ipynb for Llama 3 context# What does this PR do?\nUpdate the notebook for Llama 3 context\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [X] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [X] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [X] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "258": "Custom Changes", "516": "Freeze layer bug fixFix bug when not using PEFT", "502": "add llamaindex cookbookmore cookbooks coming, but this is a high-level overview of how to use llama3 for a bunch of different use cases\n\n1. Basic completion / chat \n2. Basic RAG (Vector Search, Summarization)\n3. Advanced RAG (Routing, Sub-Questions)\n4. Text-to-SQL \n5. Structured Data Extraction\n6. Agents", "264": "Typo Correction: Update HelloLlamaLocal.ipynb# What does this PR do?\n\nIt corrects a simple typo.\n\n\n", "927": "Llama 4 api release# What does this PR do?\n\nUpdates Llama Cookbook with the latest on Llama API\n\nThanks for contributing \ud83c\udf89!\n", "728": "Fix numpy seed in finetuning.pySet numpy seed in finetuning.py to fix it during finetuning (including in custom_dataset.py, in functions such as Dataset.train_test_split). This avoids having different train/test splits in different ranks, which may cause NCCL collective operation timeout errors.\n\n# What does this PR do?\n\nFixes #567 (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [x] Test A: Run finetuning.py in the new branch with a custom dataset (command at the beginning of the file)\n[log.branch.custom_dataset.txt](https://github.com/user-attachments/files/17395438/log.branch.custom_dataset.txt)\n\n- [x] Test B: Run finetuning.py in the new branch with default dataset (command at the beginning of the file)\n[log.branch.default_dataset.txt](https://github.com/user-attachments/files/17395439/log.branch.default_dataset.txt)\n\n- [x] Test C: Run finetuning.py in the main branch -i.e. without the fix - with a custom dataset (command at the beginning of the file)\n[log.main.custom_dataset.txt](https://github.com/user-attachments/files/17395440/log.main.custom_dataset.txt)\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [x] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "700": "Delete cookie", "847": "MM-RAG-New-PRMulti-Modal-RAG showing end to end labelling of clothes, cleaning of labels and running a RAG App using together.ai", "853": "fix few typos# What does this PR do?\n\nThis is a re-submission for https://github.com/meta-llama/llama-cookbook/pull/834\nIt only fixes few typos\n\nThanks\n\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/meta-llama/llama-cookbook/blob/main/CONTRIBUTING.md),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "890": "update category prefix according to docs# What does this PR do?\n\nFixes https://github.com/meta-llama/llama-cookbook/issues/849\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/meta-llama/llama-cookbook/blob/main/CONTRIBUTING.md),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "660": "Enhance script to handle all text file extensionsThis update enhances the script to not only check Python and Shell files but also include handling of all text files by using a more inclusive file extension pattern. This change improves the script's comprehensiveness without altering its core functionality.", "106": "Fix finetuning batch size# Fix batch size in llama_finetuning.py\n\nCurrently, `llama_finetuning.py` uses `train_config.batch_size_training` as the batch size in the dataloader. This works fine right now, because the default config sets `batch_size_training` == `micro_batch_size` == `4`. But let's say you wanted an effecttive batch size of 128 and a micro batch size of 16. Then:\n1. The dataloader would provide batches of 128 instead of 16\n2. Gradient accumulation would happen ever (128 * 8) samples instead of every 128 samples\n3. We would be loading 128 samples into memory each time, instead of 16\n\nThis PR fixes that issue.\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\nI tested this by running the script with alpaca on an H100 - please let me know if theres anything more to be done.\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "112": "enable grad on loss tensor# What does this PR do?\n\nMakes loss tensor a gradient tensor\n\nFixes #92\n", "304": "Adding W&B (wandb) experiment tracking# What does this PR do?\nThis PR adds a minimalistic W&B integration to track experiment configs, training and evaluation loss, perplexity, and system metrics like GPU/CPU utilization. \n\n## Feature/Issue validation/testing\nTested in single GPU and single node - multiple GPU setup: https://wandb.ai/darek-kleczek/llama-recipes-wandb\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [x] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n", "462": "[WIP] adding chatbot-e2e## Main Goal\n\nBuilding an e2e recipe for building chatbots where we need to fine-tune a model and wont be able to rely only on RAG.\n\nThis is just a work in progress and have many gaps to fill/ fix. Please dont review it as a ready work.\n\n### High level idea\n\nWe want to focus on following stages:\n\n- Data pipelines for creating datasets for chatbots\n- Data processing/ quality assurance practices/pipelines/tooling\n- Evaluation process\n- Fine-tuning a model/ Best practices for fine-tuning/ LORA/QLORA/ hyper params.\n\n### Use-case\n\nWe want to use this as an example to develop the e2e recipe and share it with broader community to customize for their own use-cases. As an example we use LLAMA (or could be PyTorch) FAQ model.\n\n- Llama FAQ model using OSS llama docs, github docs, papers, website, etc.\n- Proposed data pipeline, using Llama 70b as the teacher model to create Q&A pairs from Llama docs as mentioned above.[ Open to any other ideas here]\n- Data Quality/ Eval using same teacher model [ Open to any other ideas here]\n", "477": "L3updates/octoaiFinal OctoAI notebook updates in the recipes.\n\n## Feature/Issue validation/testing\n\nTested running the notebooks and verified they work as expected.\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [x] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "311": "Messenger Llama2 Chatbot Tutorial# What does this PR do?\n\nThis step-by-step tutorial shows the complete process of building a Llama-enabled Messenger chatbot. A demo video of using the iOS Messenger app to send a question to a Facebook page and receive the Llama 2 generated answer is [here](https://drive.google.com/file/d/1B4ijFH4X3jEHZfkGdTPmdsgpUes_RNud/view).\n\n\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "305": "Fix a typo in the README.md of demo apps# What does this PR do?\nFix a typo in the README.md of demo apps\n\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs.\n", "463": "Llama3update# What does this PR do?\nUpdated the script to use Llama 3 models instead of Llama 2.\nIntroduced hugging-face cli to take in HF token. And updated the script to run the HF conversion using the transformer source code.\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [X] Verified running the notebook end to end.\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "339": "Add script to run cloud api throughput benchmark# What does this PR do?\nThis is the 2nd PR for benchmark series. In this PR, we added Azure API benchmark as part of Cloud API category. \nYou can find 1st PR [here](https://github.com/facebookresearch/llama-recipes/pull/331)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [X] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [X] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "488": "Fix lm_eval.tasks' has no attribute 'initialize_tasks' error# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes #422 \n\nlm-evaluation-harness has introduced breaking changes causing `lm_eval.tasks' has no attribute 'initialize_tasks'` error in the current `recipes/evaluation/eval.py`. This PR is aiming to resolve the issue by adopting new TaskManager API.\ncf. https://github.com/EleutherAI/lm-evaluation-harness/releases\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n```bash\npython/eval.py \\\n     --model hf --model_args pretrained=meta-llama/Meta-Llama-3-8B --tasks hellaswag --device cuda:0   --batch_size 8\"\n```\n\nyields\n\n```bash\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10042/10042 [00:01<00:00, 8628.17 examples/s]\n0: 2024-05-06:12:44:13,769 INFO     [task.py:395] Building contexts for hellaswag on rank 0...\n 62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 6276/10042 [00:01<00:01, 2603.76it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10042/10042 [00:02<00:00, 3578.18it/s]\n0: 2024-05-06:12:44:17,334 INFO     [evaluator.py:393] Running loglikelihood requests\n...\n0: 2024-05-06:12:47:36,076 INFO     [eval.py:39] {\n0:   \"results\": {\n0:     \"hellaswag\": {\n0:       \"acc,none\": 0.6015733917546305,\n0:       \"acc_stderr,none\": 0.004885735963346892,\n0:       \"acc_norm,none\": 0.7905795658235412,\n0:       \"acc_norm_stderr,none\": 0.004060633907027273,\n0:       \"alias\": \"hellaswag\"\n0:     }\n0:   },\n...\n```\n\n\n\n\n## Before submitting\n- [] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [x] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "107": "Fix a bug in alapaca dataset, set ignore_idx=-100.# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFix a bug in alapaca dataset, set ignore_idx=-100.\nYou can see it in #101 , #74 , #73 , #82 .\n\n\n## Before submitting\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [x] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [x] Did you make sure to update the documentation with your changes?  \n- [x] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "891": "fix sharding strategy str to enum conversion# What does this PR do?\n\nFixes https://github.com/meta-llama/llama-cookbook/issues/809\n\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/meta-llama/llama-cookbook/blob/main/CONTRIBUTING.md),\n      Pull Request section?\n- [x] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "885": "[WIP] update Tool calling 101 from 3.1 to 3.3# What does this PR do?\n\nFixes description, documentation. Change model to 3.3 from 3.1\n[WIP] 3.3 prompt examples in second part of the notebook\n\n## Feature/Issue validation/testing\n\n- [ ] Notebook runs end to end with correct API key\n\n\n## Before submitting\n- [X] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [X] Did you read the [contributor guideline](https://github.com/meta-llama/llama-cookbook/blob/main/CONTRIBUTING.md),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [X] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "852": "Fix package namingFix readmes and instructions to `pip install llama-cookbook`", "12": "Inference updatesAligning the pad tokens with HF latest.", "846": "[DRAFT] Fix/refactor recipe references# What does this PR do?\n\nFixes recipe -> cookbook references and paths\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "729": "quick fix on readmes and deadlinks# What does this PR do?\n\nfix a word in `recipes/quickstart/finetuning/finetune_vision_model.md` and replace dead link in  `recipes/use_cases/multilingual/README.md`\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "926": "Update android appRename App to Document Summarizer", "932": "Llama4 Fine-tuning using torchtune# What does this PR do?\nAdd Llama4 Fine-tuning recipe using torchtune\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [x] H100 test log\n```\ntune run --nproc_per_node 8 lora_finetune_distributed --config llama4/scout_17B_16E_lora batch_size=4 dataset.packed=True tokenizer.max_seq_len=2048 max_steps_per_epoch=5\nINFO:torchtune.utils._logging:Running LoRAFinetuneRecipeDistributed with resolved config:\n\nbatch_size: 4\ncheckpointer:\n  _component_: torchtune.training.FullModelHFCheckpointer\n  checkpoint_dir: /tmp/Llama-4-Scout-17B-16E-Instruct\n  checkpoint_files:\n    filename_format: model-{}-of-{}.safetensors\n    max_filename: '00050'\n  model_type: LLAMA4\n  output_dir: /tmp/torchtune/llama4_17Bx16E/lora\n  recipe_checkpoint: null\nclip_grad_norm: null\ncompile: false\ncustom_sharded_layers:\n- tok_embeddings\ndataset:\n  _component_: torchtune.datasets.alpaca_dataset\n  packed: true\ndevice: cuda\ndtype: bf16\nenable_activation_checkpointing: true\nenable_activation_offloading: false\nepochs: 1\nfsdp_cpu_offload: false\ngradient_accumulation_steps: 1\nlog_every_n_steps: 1\nlog_peak_memory_stats: true\nloss:\n  _component_: torchtune.modules.loss.LinearCrossEntropyLoss\nlr_scheduler:\n  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup\n  num_warmup_steps: 100\nmax_steps_per_epoch: 5\nmetric_logger:\n  _component_: torchtune.training.metric_logging.DiskLogger\n  log_dir: /tmp/torchtune/llama4_17Bx16E/lora/logs\nmodel:\n  _component_: torchtune.models.llama4.lora_llama4_scout_17b_16e\n  apply_lora_to_mlp: true\n  apply_lora_to_output: false\n  decoder_trainable: lora\n  encoder_trainable: frozen\n  fusion_trainable: lora\n  lora_alpha: 32\n  lora_attn_modules:\n  - q_proj\n  - v_proj\n  - output_proj\n  lora_dropout: 0.0\n  lora_rank: 16\noptimizer:\n  _component_: torch.optim.AdamW\n  fused: false\n  lr: 2.0e-05\noptimizer_in_bwd: false\noutput_dir: /tmp/torchtune/llama4_17Bx16E/lora\nprofiler:\n  _component_: torchtune.training.setup_torch_profiler\n  enabled: false\nresume_from_checkpoint: false\nsave_adapter_weights_only: true\nseed: null\nshuffle: true\ntokenizer:\n  _component_: torchtune.models.llama4.llama4_transform\n  max_num_tiles: 16\n  max_seq_len: 2048\n  path: /tmp/Llama-4-Scout-17B-16E-Instruct/tokenizer.model\n\nNCCL version 2.26.2+cuda12.2\nDEBUG:torchtune.utils._logging:Setting manual seed to local seed 355260077. Local seed is seed + rank = 355260077 + 0\nINFO:torchtune.utils._logging:Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.\nWriting logs to /tmp/torchtune/llama4_17Bx16E/lora/logs/log_1746214657.txt\nINFO:torchtune.utils._logging:FSDP is enabled. Instantiating model and loading checkpoint on Rank 0 ...\n/home/kaiwu/work/llama4/torchtune/recipes/lora_finetune_distributed.py:549: FutureWarning: lora_attn_modules is deprecated for validate_missing_and_unexpected_for_lora and will be removed in future versions. Please use state_dict_keys instead.\n  validate_missing_and_unexpected_for_lora(\n/home/kaiwu/work/llama4/torchtune/torchtune/utils/_logging.py:143: FutureWarning: apply_lora_to_mlp is deprecated for validate_missing_and_unexpected_for_lora and will be removed in future versions. Please use state_dict_keys instead.\n  return obj(*args, **kwargs)\n/home/kaiwu/work/llama4/torchtune/torchtune/utils/_logging.py:143: FutureWarning: apply_lora_to_output is deprecated for validate_missing_and_unexpected_for_lora and will be removed in future versions. Please use state_dict_keys instead.\n  return obj(*args, **kwargs)\nINFO:torchtune.utils._logging:Instantiating model and loading checkpoint took 99.66 secs\nINFO:torchtune.utils._logging:Memory stats after model init:\n        GPU peak memory active: 54.86 GiB\n        GPU peak memory alloc: 54.86 GiB\n        GPU peak memory reserved: 56.06 GiB\nINFO:torchtune.utils._logging:Optimizer is initialized.\nINFO:torchtune.utils._logging:Loss is initialized.\nPacking dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 52002/52002 [00:12<00:00, 4196.77it/s]\nINFO:torchtune.utils._logging:Learning rate scheduler is initialized.\nWARNING:torchtune.utils._logging: Profiling disabled.\nINFO:torchtune.utils._logging: Profiler config after instantiation: {'enabled': False}\n  0%|                                                                                                 | 0/5 [00:00<?, ?it/s]DEBUG:torchtune.utils._logging:Using flex attention for attention computation since a BlockMask was passed in.\n1|5|Loss: 4.027358531951904: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:40<00:00,  7.18s/it]INFO:torchtune.utils._logging:Saving checkpoint. This may take some time. Retrieving full model state dict...\nINFO:torchtune.utils._logging:Getting full model state dict took 90.95 secs\nINFO:torchtune.utils._logging:Adapter checkpoint of size 1.01 GiB saved to /tmp/torchtune/llama4_17Bx16E/lora/epoch_0/adapter_model.pt\nWARNING:torchtune.utils._logging:Saving Llama4 adapter weights to PEFT format is not supported, saving to torchtune format instead\nINFO:torchtune.utils._logging:Adapter checkpoint of size 0.00 GiB saved to /tmp/torchtune/llama4_17Bx16E/lora/epoch_0/adapter_config.json\nINFO:torchtune.utils._logging:Saving final epoch checkpoint.\nINFO:torchtune.utils._logging:Please note that you have set adapter_only=True, so only adapter weights will be saved.You need to merge the adapter weights into your base model for further use. See FullModelHFCheckpointer.save_checkpoint for more details.\nINFO:torchtune.utils._logging:Saving checkpoint took 0.62 secs\n1|5|Loss: 4.027358531951904: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [02:11<00:00, 26.33s/it]\n```\n- [x] A100 Test Log with CPU off-load\n```\nINFO:torchtune.utils._logging:Running LoRAFinetuneRecipeDistributed with resolved config:\n\nbatch_size: 1\ncheckpointer:\n  _component_: torchtune.training.FullModelHFCheckpointer\n  checkpoint_dir: /tmp/Llama-4-Scout-17B-16E-Instruct\n  checkpoint_files:\n    filename_format: model-{}-of-{}.safetensors\n    max_filename: '00050'\n  model_type: LLAMA4\n  output_dir: /tmp/torchtune/llama4_17Bx16E/lora\n  recipe_checkpoint: null\nclip_grad_norm: null\ncompile: false\ncustom_sharded_layers:\n- tok_embeddings\ndataset:\n  _component_: torchtune.datasets.alpaca_dataset\n  packed: true\ndevice: cuda\ndtype: bf16\nenable_activation_checkpointing: true\nenable_activation_offloading: false\nepochs: 1\nfsdp_cpu_offload: true\ngradient_accumulation_steps: 1\nlog_every_n_steps: 1\nlog_peak_memory_stats: true\nloss:\n  _component_: torchtune.modules.loss.LinearCrossEntropyLoss\nlr_scheduler:\n  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup\n  num_warmup_steps: 100\nmax_steps_per_epoch: 5\nmetric_logger:\n  _component_: torchtune.training.metric_logging.DiskLogger\n  log_dir: /tmp/torchtune/llama4_17Bx16E/lora/logs\nmodel:\n  _component_: torchtune.models.llama4.lora_llama4_scout_17b_16e\n  apply_lora_to_mlp: true\n  apply_lora_to_output: false\n  decoder_trainable: lora\n  encoder_trainable: frozen\n  fusion_trainable: lora\n  lora_alpha: 32\n  lora_attn_modules:\n  - q_proj\n  - v_proj\n  - output_proj\n  lora_dropout: 0.0\n  lora_rank: 16\noptimizer:\n  _component_: torch.optim.AdamW\n  fused: false\n  lr: 2.0e-05\noptimizer_in_bwd: false\noutput_dir: /tmp/torchtune/llama4_17Bx16E/lora\nprofiler:\n  _component_: torchtune.training.setup_torch_profiler\n  enabled: false\nresume_from_checkpoint: false\nsave_adapter_weights_only: true\nseed: null\nshuffle: true\ntokenizer:\n  _component_: torchtune.models.llama4.llama4_transform\n  max_num_tiles: 16\n  max_seq_len: 1024\n  path: /tmp/Llama-4-Scout-17B-16E-Instruct/tokenizer.model\n\nDEBUG:torchtune.utils._logging:Setting manual seed to local seed 3712560434. Local seed is seed + rank = 3712560434 + 0\nINFO:torchtune.utils._logging:Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.\nWriting logs to /tmp/torchtune/llama4_17Bx16E/lora/logs/log_1746215549.txt\nINFO:torchtune.utils._logging:FSDP is enabled. Instantiating model and loading checkpoint on Rank 0 ...\nNCCL version 2.26.2+cuda12.2\n/home/kaiwu/.conda/envs/torchtune/lib/python3.10/site-packages/recipes/lora_finetune_distributed.py:549: FutureWarning: lora_attn_modules is deprecated for validate_missing_and_unexpected_for_lora and will be removed in future versions. Please use state_dict_keys instead.\n  validate_missing_and_unexpected_for_lora(\n/home/kaiwu/.conda/envs/torchtune/lib/python3.10/site-packages/torchtune/utils/_logging.py:143: FutureWarning: apply_lora_to_mlp is deprecated for validate_missing_and_unexpected_for_lora and will be removed in future versions. Please use state_dict_keys instead.\n  return obj(*args, **kwargs)\n/home/kaiwu/.conda/envs/torchtune/lib/python3.10/site-packages/torchtune/utils/_logging.py:143: FutureWarning: apply_lora_to_output is deprecated for validate_missing_and_unexpected_for_lora and will be removed in future versions. Please use state_dict_keys instead.\n  return obj(*args, **kwargs)\nINFO:torchtune.utils._logging:Instantiating model and loading checkpoint took 488.07 secs\nINFO:torchtune.utils._logging:Memory stats after model init:\n        GPU peak memory active: 12.54 GiB\n        GPU peak memory alloc: 12.54 GiB\n        GPU peak memory reserved: 12.55 GiB\nINFO:torchtune.utils._logging:Optimizer is initialized.\nINFO:torchtune.utils._logging:Loss is initialized.\nPacking dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 52002/52002 [00:29<00:00, 1779.69it/s]\nINFO:torchtune.utils._logging:Learning rate scheduler is initialized.\nWARNING:torchtune.utils._logging: Profiling disabled.\nINFO:torchtune.utils._logging: Profiler config after instantiation: {'enabled': False}\n  0%|                                                                                                 | 0/5 [00:00<?, ?it/s]DEBUG:torchtune.utils._logging:Using flex attention for attention computation since a BlockMask was passed in.\n1|5|Loss: 3.9630274772644043: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [01:22<00:00, 15.79s/it]INFO:torchtune.utils._logging:Saving checkpoint. This may take some time. Retrieving full model state dict...\nINFO:torchtune.utils._logging:Getting full model state dict took 312.65 secs\nINFO:torchtune.utils._logging:Adapter checkpoint of size 1.01 GiB saved to /tmp/torchtune/llama4_17Bx16E/lora/epoch_0/adapter_model.pt\nWARNING:torchtune.utils._logging:Saving Llama4 adapter weights to PEFT format is not supported, saving to torchtune format instead\nINFO:torchtune.utils._logging:Adapter checkpoint of size 0.00 GiB saved to /tmp/torchtune/llama4_17Bx16E/lora/epoch_0/adapter_config.json\nINFO:torchtune.utils._logging:Saving final epoch checkpoint.\nINFO:torchtune.utils._logging:Please note that you have set adapter_only=True, so only adapter weights will be saved.You need to merge the adapter weights into your base model for further use. See FullModelHFCheckpointer.save_checkpoint for more details.\nINFO:torchtune.utils._logging:Saving checkpoint took 1.36 secs\n1|5|Loss: 3.9630274772644043: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [06:36<00:00, 79.26s/it]\n```\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/meta-llama/llama-cookbook/blob/main/CONTRIBUTING.md),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "503": "add llamaindex + groq cookbooksame as #502 but with groq ", "271": "Update README.mdFix examnples.py filename error", "259": "added pypdf to !pip install added pypdy to !pip install - required for cell containing:\n\nfrom langchain.document_loaders import PyPDFLoader loader = PyPDFLoader(\"https://arxiv.org/pdf/2307.09288.pdf\") docs = loader.load()\n\nEnv: Python 3.11.4 | packaged by Anaconda, Inc. | (main, Jul  5 2023, 13:38:37) [MSC v.1916 64 bit (AMD64)] Type 'copyright', 'credits' or 'license' for more information IPython 8.12.0 -- An enhanced Interactive Python.\nThe version of the notebook server is: 6.5.4\nWindows 10\n\n# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\npypdf module not installed\n\n\n## Feature/Issue validation/testing\n\nFollowing statement  now runs without error\n\nfrom langchain.document_loaders import PyPDFLoader loader = PyPDFLoader(\"https://arxiv.org/pdf/2307.09288.pdf\") docs = loader.load()\n\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "529": "bump up version# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "501": "Fixing lib installation in notebook exampleFixing lib installation in notebook example\n\n\n\n\n\n\n## Before submitting\n- [x  ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "515": "Create Prompt_Engineering_with_Llama_3_On_Amazon_Bedrock.ipynbThis an updated notebook for prompt engineering llama 3 on AWS using Bedrock. It also includes a brief into example using Llama Guard 2.\n\n\n## Before submitting\n- [x ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "924": "changed planning prompt, execution prompt, few shot examples, and bro\u2026\u2026wser executions to fix the demo for flight reservation\n\n# What does this PR do?\n\nThis pull request updates the demo for flight reservation by making several key changes:\n\n1. Planning Prompt: The planning prompt has been revised to better align with the requirements of the flight reservation demo.\n2. Execution Prompt: The execution prompt has also been updated to ensure that it accurately reflects the desired outcome of the demo.\n3. Few-Shot Examples: Additional few-shot example has been added to provide more context and guidance for the model.\n4. Browser Executions: The browser executions have been modified to fix issues and improve the overall functionality of the demo.\n\nThese changes aim to enhance the accuracy and reliability of the flight reservation demo, providing a better experience for users.", "930": "fixing readme links# What does this PR do?\n\nChanging readme links", "918": "add ms-swift & llama4 SFT recipeLlama4 is an excellent model, and the open-source announcement is exciting!\n\nms-swift supports SFT training for Llama4 in this [PR](https://github.com/modelscope/ms-swift/pull/3777) and has received an invitation to contribute.\ud83d\ude0a\n\nThis PR contributes a recipe for performing OCR SFT on Llama4 using ms-swift.\n", "703": "fix raft datasetWe need to use last idx+1 to cover the <eot> token.\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "717": "Fix the bug when continue the peft.\n# What does this PR do?\nFixed a bug when loading a peft llama-3-8b-instruct checkpoint to continue the finetuning. \nThe parameter passed to peft_config is modified, which means the `()` of `peft_config = model.peft_config()` has been removed.\n\nFixes #709 \n\n\n## Feature/Issue validation/testing\n\n\n- [X] Test \nRun the `/recipes/quickstart/finetuning/finetuning.py` after the modification in `/src/llama_recipes/finetuning.py`, with the config `from_peft_checkpoint: str=\"/root/lora-llama3-ft/save/PEFT/model_ner_in_assis\"`\nLogs:\n```\n/root/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n  from torch.distributed._shard.checkpoint import (\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  9.09it/s]\n--> Model /root/autodl-tmp/model/llama-3-8b-instruct\n\n--> /root/autodl-tmp/model/llama-3-8b-instruct has 8030.261248 Million params\n\ntrainable params: 54,525,952 || all params: 8,084,787,200 || trainable%: 0.6744\n```\n\n\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [X] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [X] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [X] Did you make sure to update the documentation with your changes?  \n- [X] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "10": "Fixed Typo in README.mdFixed the typo in README.md for the word script in the quickstart section. ", "850": "Fixed all \"Open in Colab\" absolute paths# What does this PR do?\n\nFixes all Colab links that were broken after the refactor and rebrand.\n\n## Feature/Issue validation/testing\n\n- Tested all the links in the notebook preview before committing, confirmed they load the correct notebook.\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/meta-llama/llama-cookbook/blob/main/CONTRIBUTING.md),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "844": "fix links cookbook refactor# What does this PR do?\n\nFixes links from recipe to cookbook\n\nFixes # (issue)\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "878": "Fix of NameError(missed fucntion call)\n\n# What does this PR do?\nFix Name error\n\nFixes # (issue)\nFix of this error\n```\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call\nlast)\nCell In[49], line 4\n      1 INPUT_FILE = \"./resources/extracted_text.txt\"  # Replace with\nyour file path\n      2 CHUNK_SIZE = 1000  # Adjust chunk size if needed\n----> 4 chunks = create_word_bounded_chunks(text, CHUNK_SIZE)\n      5 num_chunks = len(chunks)\n\nNameError: name 'text' is not defined\n---------------------------------------------------------------------------\n```\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/meta-llama/llama-cookbook/blob/main/CONTRIBUTING.md),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "887": "Update Step-1 PDF-Pre-Processing-Logic.ipynbFix step 1 `processed_text` variable", "677": "Upstream mergeAll updates for Connect release", "105": "update prompts# What does this PR do?\n\nThis PR address the prompt updates suggested from [llama-repo ](https://github.com/facebookresearch/llama/pull/626/files)\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\nupdate the prompt instructions \n## Feature/Issue validation/testing\n\n- [ ] Test A : Logs after the fix \n-https://gist.github.com/HamidShojanazeri/5afd433fa466dffe46de940bcc334df1\n\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ X] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "663": "Fix/load model with torch dtype auto# What does this PR do?\nThis PR loads a model with torch_dtype=auto instead of bfloat16 when we do not specify train_config.use_fp16.\nFor llama models this will not make a difference as their [default dtype is bfloat16](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/blob/main/config.json#L34)\n\nFixes # (issue)\n#656 (kind of)\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [X] `torchrun --nnodes 1 --nproc_per_node 4 ./recipes/quickstart/finetuning/finetuning.py  --model_name meta-llama/Meta-Llama-3.1-8B-Instruct  --enable_fsdp --max_train_step=2 --batch_size_training 1 --batching_strategy packing --dataset samsum_dataset --save_model False --context_length 4096 --fsdp_config.pure\n_bf16 True --fsdp_config.optimizer anyprecision --samsum_dataset.trust_remote_code 1`\nLogs \n```\nW0906 15:44:05.090000 140566387028992 torch/distributed/run.py:779]\nW0906 15:44:05.090000 140566387028992 torch/distributed/run.py:779] *****************************************\nW0906 15:44:05.090000 140566387028992 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.\nW0906 15:44:05.090000 140566387028992 torch/distributed/run.py:779] *****************************************\n/home/mreso/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n  from torch.distributed._shard.checkpoint import (\n/home/mreso/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n  from torch.distributed._shard.checkpoint import (\n/home/mreso/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n  from torch.distributed._shard.checkpoint import (\n/home/mreso/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n  from torch.distributed._shard.checkpoint import (\nClearing GPU cache for all ranks\n--> Running with torch dist debug set to detail\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  4.03it/s]\n--> Model meta-llama/Meta-Llama-3.1-8B-Instruct\n\n--> meta-llama/Meta-Llama-3.1-8B-Instruct has 8030.261248 Million params\n\nbFloat16 enabled for mixed precision - using bfSixteen policy\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  4.03it/s]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  4.13it/s$\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  [0/14738]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  4.13it/s]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  4.14it/s]\n--> applying fsdp activation checkpointing...\n--> applying fsdp activation checkpointing...\n--> applying fsdp activation checkpointing...\n--> applying fsdp activation checkpointing...\nPreprocessing dataset:   9%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                                                                                                                                                                                                                                                            | 1318/14732 [00:00<00:04, 3194.86it/s]--> Training Set Length = 14732\nPreprocessing dataset:  51%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                                                                                                                               | 7556/14732 [00:02<00:02, 3374.50it/s]--> Validation Set Length = 818\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:04<00:00, 3272.90it/s]\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 818/818 [00:00<00:00, 3350.07it/s]\n--> Num of Validation Set Batches loaded = 8\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:04<00:00, 3324.63it/s]\nPreprocessing dataset:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                         | 12661/14732 [00:03<00:00, 3174.45it/s]/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 818/818 [00:00<00:00, 3365.68it/s]\n--> Num of Validation Set Batches loaded = 8\nPreprocessing dataset:  59%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                                                                                                        | 8707/14732 [00:02<00:01, 3237.27it/s]/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:04<00:00, 3149.61it/s]\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 818/818 [00:00<00:00, 3361.27it/s]\n--> Num of Validation Set Batches loaded = 8\nPreprocessing dataset:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                           | 10974/14732 [00:03<00:01, 3205.69it/s]/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:04<00:00, 3251.47it/s]\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 818/818 [00:00<00:00, 3252.59it/s]\n--> Num of Validation Set Batches loaded = 8\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                                                                                                                                                                                                                                                                                          | 0/159 [00:00<?, ?it/s]NCCL version 2.20.5+cuda12.4\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\nTraining Epoch: 1/3, step 1/159 completed (loss: 21.161834716796875):   1%|\u2588\u2588\u2588\u258f                                                                                                                                                                                                                                                           | 2/159 [00:10<11:53,  4.55s/it]max training steps reached, stopping training, total train steps finished:  2\nTraining Epoch: 1/3, step 1/159 completed (loss: 21.161834716796875):   1%|\u2588\u2588\u2588\u258f                                                                                                                                                                                                                                                           | 2/159 [00:10<13:33,  5.18s/it]\nTraining Epoch: 1/3, step 1/159 completed (loss: 21.4289608001709):   1%|\u2588\u2588\u2588\u258f                                                                                                                                                                                                                                                             | 2/159 [00:10<14:09,  5.41s/it]\nTraining Epoch: 1/3, step 1/159 completed (loss: 21.154579162597656):   1%|\u2588\u2588\u2588\u258f                                                                                                                                                                                                                                                           | 2/159 [00:09<12:36,  4.82s/it]\nTraining Epoch: 1/3, step 1/159 completed (loss: 21.540555953979492):   1%|\u2588\u2588\u2588\u258f                                                                                                                                                                                                                                                           | 2/159 [00:07<10:26,  3.99s/it]\nMax CUDA memory allocated was 20 GB\nMax CUDA memory reserved was 29 GB\nPeak active CUDA memory was 21 GB\nCUDA Malloc retries : 0\nCPU Total Peak Memory consumed during the train (max): 7 GB\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 8/8 [00:03<00:00,  2.22it/s]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 8/8 [00:03<00:00,  2.22it/s]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 8/8 [00:03<00:00,  2.16it/s]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 8/8 [00:03<00:00,  2.20it/s]\n eval_ppl=tensor(88679.3359, device='cuda:0') eval_epoch_loss=tensor(11.3928, device='cuda:0')\nbest eval loss on epoch 1 is 11.392782211303711\nEpoch 1: train_perplexity=1.1542, train_epoch_loss=0.1434, epoch time 8.7118956502527s\ntraining params are saved in /home/mreso/llama-recipes/PATH/to/save/FSDP/model/fine-tuned-meta-llama/Meta-Llama-3.1-8B-Instruct/train_params.yaml\nKey: avg_train_prep, Value: 1.1542373895645142\nKey: avg_train_loss, Value: 0.14343982934951782\nKey: avg_eval_prep, Value: 88679.3359375\nKey: avg_eval_loss, Value: 11.392782211303711\nKey: avg_epoch_time, Value: 8.7118956502527\nKey: avg_checkpoint_time, Value: 1.8780119717121124e-06\n```\n\n## Before submitting\n- [X] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [X] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [X] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n\nThanks for contributing \ud83c\udf89!\n", "313": "Update wordlist.txt for additional technical terms in Readme.Added words:\nADDR\nckpt\n\n# What does this PR do?\nSimple update for the spellchecker to avoid errors in PRs that contain these words.\n\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "461": "Update peft_finetuning.ipynb# What does this PR do?\n\nThis PR solve an old quesiton.\n\nprepare_model_for_int8_training has been deprecated for quite some time, with PEFT v0.10.0, it has been removed. Please use prepare_model_for_kbit_training instead. @BenjaminBossan\n\nDetail can be seen here. https://github.com/huggingface/peft/issues/1583\n\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [x] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "307": "Update README.md# What does this PR do?\n\nFixes a typo in the readme.\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n\nThanks for contributing \ud83c\udf89!\n", "460": "Dev# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "306": "PR based on the chester-rag-chatbot-example branchwith a single commit added to the PR https://github.com/facebookresearch/llama-recipes/pull/291 to disable the markdown link check.", "474": "Update aws notebooksUpdating notebooks to llama 3 when possible. \n\nRemoving the AWS specific notebook on prompting Llama 2, moving to the quick started guide and leaving it as the only prompt eng guide. We need to create a net new one for Llama 3 and keep this one for reference on Llama 2.\n\n\n\n## Feature/Issue validation/testing\n \nRunning the notebooks and checking in the outputs for every cell. \n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [x] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "448": "Update llama_guard_version in inference.py# What does this PR do?\nThis PR updates the `llama_guard_version` argument in inference.py as it takes only string\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "104": "Chat prompt fix# What does this PR do?\n\nThis PR update the prompts as suggested in [llama-repo ](https://github.com/facebookresearch/llama/pull/626/files).\n<!-- Remove if not applicable -->\n\nFixes # (issue)\nUpdate prompt\n\n- [ X] Test A : after updates.\nhttps://gist.github.com/HamidShojanazeri/5afd433fa466dffe46de940bcc334df1\n\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [X ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "662": "Update get_default_finetune_args.py# What does this PR do?\nUpdate finetuning arguments with improved values\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [x] Ran the notebook with \"uncomment\" sections uncommented: https://github.com/meta-llama/llama-recipes/blob/ccd4741084e2a5b1ba8f7fb1e8d042f55c0390de/recipes/3p_integrations/lamini/text2sql_memory_tuning/meta_lamini.ipynb\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "676": "Make gradio and langchain optional dependencies# What does this PR do?\nThis PR makes langchain and gradio an optional dependency as only of subset of recipes use it and thee package slow down  installation a lot.\n\nFixes # (issue)\n#674\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [X] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [X] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "110": "Add sample inference script for hf-tgi# What does this PR do?\n\nThis PR introduces a recipe to show the proper way to format input for chat model inference on chat-based models with Hugging Face's text generation inference.\n\nWithin the enclosed script is a function that arranges conversation dialogues in a precise format, incorporating suitable tags for system prompts, user inquiries, and assistant replies.\n\n\n## Feature/Issue validation/testing\n\nRan the script against the sample chat conversations provided in the repo. Outputs are here: https://gist.github.com/svaruag/425712d8a12bf29a69206762d690646b\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "886": "add colab instruction, fix links# What does this PR do?\n\nFixes [(# 804 issue)](https://github.com/meta-llama/llama-cookbook/issues/804)\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/meta-llama/llama-cookbook/blob/main/CONTRIBUTING.md),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "892": "update test for guard prefix# What does this PR do?\n\nFixes tests after PR https://github.com/meta-llama/llama-cookbook/pull/890\n\n\n## Before submitting\n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "879": "added readme to broswer use usecase# What does this PR do?\nThis pull request adds a README file to the browser use usecase\n\n\n## Before submitting\n- [X] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/meta-llama/llama-cookbook/blob/main/CONTRIBUTING.md),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "39": "Fsdp inference checkpointsAdding converter script for converting FSDP sharded state dict to HF model for inference. ", "851": "Cleaned up API KEY placeholder text# What does this PR do?\n\nReplaces placeholder text on API keys with a more generic string.\n\n## Before submitting\n- [X] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/meta-llama/llama-cookbook/blob/main/CONTRIBUTING.md),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "11": "docs: fix typo in README.md", "716": "Initial Crusoe examples to 3p_integrations recipes# What does this PR do?\n\nThis PR adds a new section in 3p_integrations for deploying Llama solutions on [Crusoe Cloud](https://crusoe.ai/).\n\n\n## Feature/Issue validation/testing\n\nFor easy reproducibility, Crusoe recipes use terraform for resource provisioning. We provide documentation in each recipe's respective README for implementing the solution.\n\n\n## Before submitting\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests), Pull Request section?\n- [x] Was this discussed/approved via a Github issue? Discussed and approved [here](https://github.com/meta-llama/llama-recipes/issues/713).\n- [x] Did you make sure to update the documentation with your changes?  No changes necessary.\n- [x] Did you write any new necessary tests? No additional tests necessary.\n\nThanks for contributing \ud83c\udf89!\n", "919": "E2E use-case:  research paper analyzer with Llama 4Add this E2E use case. More detail in readme\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [X] Did you read the [contributor guideline](https://github.com/meta-llama/llama-cookbook/blob/main/CONTRIBUTING.md),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "931": "[WIP] image grounding example# What does this PR do?\n\nAdds image grounding example\n\n", "925": "changed planning prompt, execution prompt, few shot examples, and bro\u2026\u2026wser executions to fix the demo for flight reservation\n\n# What does this PR do?\n\nThis pull request updates the demo for flight reservation by making several key changes:\nPlanning Prompt: The planning prompt has been revised to better align with the requirements of the flight reservation demo.\nExecution Prompt: The execution prompt has also been updated to ensure that it accurately reflects the desired outcome of the demo.\nFew-Shot Examples: Additional few-shot examples have been added to provide more context and guidance for the model.\nBrowser Executions: The browser executions have been modified to fix issues and improve the overall functionality of the demo.\nThese changes aim to enhance the accuracy and reliability of the flight reservation demo, providing a better experience for users.\n", "514": "Remove local tokenizer requirement for vllm on prem throughput benchmark\n# What does this PR do?\nReplace it with the tokenizer derived from HF model path, so Llama 3 will use its own tokenizer.\n\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [X] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [X] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "272": "Fix examnples.py filename errorexamnples.py -> examples.py", "500": "[OctoAI] Llama-3 based summarization + RAG to power a sales bot# What does this PR do?\n\nIntroducing a new Llama3 example that is built on top of OctoAI model API provider and Weaviate vector DB to implement a helpful sales bot.\n\nExample interaction:\n\nUser: \"what is must have accessory for my new electric guitar\"\n\nChatbot: \n> Congratulations on your new electric guitar! \n> A must-have accessory for your new electric guitar is a good case to protect it from damage and scratches. I highly recommend the [Solid Electric Guitar Case with Accessories Compartment](https://www.amazon.com/exec/obidos/ASIN/B00A270KYM). This case provides good protection and has a roomy accessories compartment to store your guitar's accessories. \n> Another great option is the [Hard Guitar Case with Accessory Compartments](https://www.amazon.com/exec/obidos/ASIN/B001FB5Z44), which is sturdy and has additional compartments for accessories. \n> If you're looking for a more affordable option, the [Affordable Solid Body Electric Guitar Case](https://www.amazon.com/exec/obidos/ASIN/B000CDT4VQ) is a great choice. It's well-built, durable, and fits various solid body electric guitars.\n> References:\n[1] [Solid Electric Guitar Case with Accessories Compartment](https://www.amazon.com/exec/obidos/ASIN/B00A270KYM)\n[2] [Hard Guitar Case with Accessory Compartments](https://www.amazon.com/exec/obidos/ASIN/B001FB5Z44)\n[3] [Affordable Solid Body Electric Guitar Case](https://www.amazon.com/exec/obidos/ASIN/B000CDT4VQ)\n\n## Feature/Issue validation/testing\nRan on a local jupyter notebook on Macbook M1.\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [x] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "528": "Update contribution.md# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "201": "Fix tqdm bar not change length after terminal is resized# What does this PR do?\n\nAfter resizing the terminal, the length of the tqdm bar does not change, causing some content to be displayed improperly.\n\nLook like this below,  `it/s` is missing.\n\n```\nTraining Epoch: 1:   0%|                                              | 0/1477 [00:00<?, ?it/s]\nTraining Epoch: 1/3, step 1442/1477 completed (loss: 1.9955642223358154):  98%|\u2589| 1443/1477 [3:\n```\n\n## Feature/Issue validation/testing\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "573": "[lamini] Add lamini text2sql memory tuning tutorial# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [x] Tested run-all in colab and in vscode notebook\n\n## Before submitting\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n\nThanks for contributing \ud83c\udf89!\n", "229": "Add Flop counter & proifler# What does this PR do?\n\nThis PR adds flop counter and profiler to the training loop\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\n[Logs for Test A\n](https://gist.github.com/HamidShojanazeri/c741a5413a9bd2109930e558c037fcff)\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "942": "Llama 4 Fine tuning: updated docs to streamline environment setup and clarify command line argumentsUpdated docs for L4 fine tuning to streamline environment setup and clarify what the cmd line arguments in tune run commands are doing. This should help improve the clarity of our FT docs. ", "956": "Bootcamp week2 task## Bootcamp week 2 task\n\n## The PR handled the following steps:\n1. Used my personal GitHub account and linked it to my Meta account.\n2. Added myself as contributor to the llama-cookbook repo\n3. Forked llama-cookbook then created a branch off of main.\n4. Retrieved the LLAMA Endpoint API keys.\n5. Developed a script for LLM inference. Added support for gradio to create a UI for the Llama API.\n6. Wrote a README to explain how to use the interface.\n\n## Feature/Issue validation/testing\n\nI have added a script with its corresponding README and a Jupyter Notebook that does the same thing with its own comments and headings.\n\n- [ ] Test A\nTested the the UI and attached a snapshot.\n<img width=\"1728\" alt=\"Screenshot 2025-05-29 at 7 20 42\u202fPM\" src=\"https://github.com/user-attachments/assets/abfb4b0e-fbf0-4099-a77b-3435eeb2ea40\" />\n\n", "759": "fix \"INPUT_FILE\" path and missing variable \"processed_text\"# What does this PR do?\n\n- fix \"INPUT_FILE\" path (the initial output of extracted_text.txt and the input path doesn't match, therefore not able to read in here)\n- missing variable \"processed_text\" (the processed_text was not initial defined)\n\n\n\n\n## Before submitting\n- [Yes] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [Yes] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ no] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [yes] Did you make sure to update the documentation with your changes?  \n- [no] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "822": "update text2sql demo to using Llama 3.3# What does this PR do?\n\nFixed broken code; updated Llama to using 3.3.\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "177": "Add missing license header in test files# What does this PR do?\n\nAdd missing license header in test files\n\n", "605": "Update transformers requirements# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "639": "Updates to accommodate OpenLLM leaderboard v2 tasks and change Meta Llama 3.1 to Llama 3.1 Previously, the OpenLLM leaderboard V1 is hard to reproduce as it did not included a easy commend to run, so we created a customized eval.py to load a folder of selected task templates to run OpenLLM leaderboard V1. Now, [OpenLLM leaderboard V2](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) has not only updated the selected tasks, but created a easy-to-use commend for us to run. Therefore, this PR deletes the previous work for OpenLLM leaderboard V1 and updates the README to accommodate OpenLLM leaderboardv2 tasks\n\n\n\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [x] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [x] Did you make sure to update the documentation with your changes?  \n- [x] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "375": "Add Llama2 + EC2 tutorial using RunhouseA simple code file to stand up a Llama2 model on an A10G running in EC2. Uses the Runhouse Python library to easily provision an instance, copy code over, and serve the model.\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [x] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "407": "update due to peft new release# What does this PR do?\n\nThis updates the `prepare_model_for_int8_training` to `prepare_model_for_kbit_training` due to the recent PEFT release.\n\nFixes #406 \n", "361": "Revert \"Flop counter, profiling and GC (#357)\"This reverts commit 4530d543f8f543d12ee23229e17921dd52fd21e2, reversing changes made to 98b122e57a8df44f5b88fa9fdab8818cc6e4969f.\nReverting, the profiling PR for some improvements.\n", "638": "Update hello_llama_cloud.ipynbFix: LangChai->LangChain\n\n# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "162": "Wandb, cpu offloading, cosine scheduler, CodeLlama, Completion and bug fixes# What does this PR do?\n\nThis PR contains a bit of everything I did (and will do) to improve my workflow, it can be merged but I am opening it with the intent of making these changes visible to users who are looking for them.\n\n- Add FSDP CPU offloading for smaller configs #122 \n- Add support for wandb tracking\n- Add support for cosine + warmup scheduler\n- Add support for CodeLlama\n- Add support for completion corpus datasets with optional packing or padding\n\nFixes:\n- Progress bar not incrementing correctly and displaying erroneous numbers.\n\n## Feature/Issue validation/testing\n\n- [x] LLama 7b full train on 4xV100 16GB\n- [x] Llama 7b PEFT Lora on 4xV100 16GB\n- [x] CodeLlama 7b full train on 4xV100 16GB\n", "604": "Remove max_length from tokenization# What does this PR do?\nRemoves max_length from tokenization to prevent OOM\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [X] Test A\nLogs for Test A\n```\necho \"Hello\" | CUDA_VISIBLE_DEVICES=0 python inference.py /fsx-project/shared/mreso/.cache/\nhuggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/4281e96c7cf5ab6b312ef0cb78373efa3976a9dd/ --quantization '4bit' --enable_salesforce_content_safety False\nuse_fast_kernelsFalse\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:22<00:00,  5.67s/it]\nUser prompt deemed safe.\nUser prompt:\nHello\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nthe inference time is 6746.064721955918 ms\nUser input and model output deemed safe.\nModel output:\nHello\nHello,\nI'm still here, and I'm still loving you, in my own\nfucking way.\n--The Smiths\nIf we were supposed to be this happy and\ncontented, then surely we'd know it by now\n--The Smiths\nWhen I get to the end of my road\nAnd the end of the road is my home\nI find my heart still racing\nWith thoughts of you, as I roam\nI'm going home\n--Paul McCartney\nIf\n```\n\n\n## Before submitting\n- [X] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [X] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "88": "Update spellcheck.shAdding copyright info per task\n\n# What does this PR do?\nAdding copyright information to spellcheck.sh\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\n\n## Feature/Issue validation/testing\n\nN/A\n\n## Before submitting\n- [ x ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "176": "Set release version to 0.0.1# What does this PR do?\nSet release version to 0.0.1\n\nFixes # (issue)\nN/A\n", "823": "moved images from quickstart to doc/img# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nChanges the image directory for building with Llama 3.2 to doc/img\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "63": "update README: python 3.9 rec + fix formattingAdding python version recommendation (python 3.9) to README.\n\nAlso fixes a small markdown formatting bug.\n\nInstalling requirements with python 3.7 failed. Python 3.8 and 3.9 worked.\n\nError message installing with python 3.7:\n```bash\ndevzero@rrapjzly:~/projects/llama-recipes$ pip --version\npip 23.0.1 from /home/devzero/.pyenv/versions/3.7.17/lib/python3.7/site-packages/pip (python 3.7)\ndevzero@rrapjzly:~/projects/llama-recipes$ pip install -r requirements.txt\nLooking in links: https://download.pytorch.org/whl/torch_stable.html\nCollecting git+https://github.com/huggingface/peft.git (from -r requirements.txt (line 11))\n  Cloning https://github.com/huggingface/peft.git to /tmp/pip-req-build-guhelk5b\n  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft.git /tmp/pip-req-build-guhelk5b\n  Resolved https://github.com/huggingface/peft.git to commit 0e33ac1efe1564143bffd248906d9e17864017dc\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\nERROR: Could not find a version that satisfies the requirement torch==2.0.1+cu118 (from versions: 0.4.1, 0.4.1.post2, 1.0.0, 1.0.1, 1.0.1.post2, 1.1.0, 1.2.0, 1.2.0+cpu, 1.2.0+cu92, 1.3.0, 1.3.0+cpu, 1.3.0+cu100, 1.3.0+cu92, 1.3.1, 1.3.1+cpu, 1.3.1+cu100, 1.3.1+cu92, 1.4.0, 1.4.0+cpu, 1.4.0+cu100, 1.4.0+cu92, 1.5.0, 1.5.0+cpu, 1.5.0+cu101, 1.5.0+cu92, 1.5.1, 1.5.1+cpu, 1.5.1+cu101, 1.5.1+cu92, 1.6.0, 1.6.0+cpu, 1.6.0+cu101, 1.6.0+cu92, 1.7.0, 1.7.0+cpu, 1.7.0+cu101, 1.7.0+cu110, 1.7.0+cu92, 1.7.1, 1.7.1+cpu, 1.7.1+cu101, 1.7.1+cu110, 1.7.1+cu92, 1.7.1+rocm3.7, 1.7.1+rocm3.8, 1.8.0, 1.8.0+cpu, 1.8.0+cu101, 1.8.0+cu111, 1.8.0+rocm3.10, 1.8.0+rocm4.0.1, 1.8.1, 1.8.1+cpu, 1.8.1+cu101, 1.8.1+cu102, 1.8.1+cu111, 1.8.1+rocm3.10, 1.8.1+rocm4.0.1, 1.9.0, 1.9.0+cpu, 1.9.0+cu102, 1.9.0+cu111, 1.9.0+rocm4.0.1, 1.9.0+rocm4.1, 1.9.0+rocm4.2, 1.9.1, 1.9.1+cpu, 1.9.1+cu102, 1.9.1+cu111, 1.9.1+rocm4.0.1, 1.9.1+rocm4.1, 1.9.1+rocm4.2, 1.10.0, 1.10.0+cpu, 1.10.0+cu102, 1.10.0+cu111, 1.10.0+cu113, 1.10.0+rocm4.0.1, 1.10.0+rocm4.1, 1.10.0+rocm4.2, 1.10.1, 1.10.1+cpu, 1.10.1+cu102, 1.10.1+cu111, 1.10.1+cu113, 1.10.1+rocm4.0.1, 1.10.1+rocm4.1, 1.10.1+rocm4.2, 1.10.2, 1.10.2+cpu, 1.10.2+cu102, 1.10.2+cu111, 1.10.2+cu113, 1.10.2+rocm4.0.1, 1.10.2+rocm4.1, 1.10.2+rocm4.2, 1.11.0, 1.11.0+cpu, 1.11.0+cu102, 1.11.0+cu113, 1.11.0+cu115, 1.11.0+rocm4.3.1, 1.11.0+rocm4.5.2, 1.12.0, 1.12.0+cpu, 1.12.0+cu102, 1.12.0+cu113, 1.12.0+cu116, 1.12.0+rocm5.0, 1.12.0+rocm5.1.1, 1.12.1, 1.12.1+cpu, 1.12.1+cu102, 1.12.1+cu113, 1.12.1+cu116, 1.12.1+rocm5.0, 1.12.1+rocm5.1.1, 1.13.0, 1.13.0+cpu, 1.13.0+cu116, 1.13.0+cu117, 1.13.0+cu117.with.pypi.cudnn, 1.13.0+rocm5.1.1, 1.13.0+rocm5.2, 1.13.1, 1.13.1+cpu, 1.13.1+cu116, 1.13.1+cu117, 1.13.1+cu117.with.pypi.cudnn, 1.13.1+rocm5.1.1, 1.13.1+rocm5.2)\nERROR: No matching distribution found for torch==2.0.1+cu118\n```", "77": "save cpu mem by leveraging FSDP rank0 broadcasting# What does this PR do?\n\nfor FSDP mode, this saves cpu memory by loading only one cpu copy of the model. This is specifically useful when using llama 70B as the current code would consume 2+ TB of cpu memory with 70B (70 * 4 * 8), which will cause cpu oom.\n\n# Notes\n\n1. This would require latest nightlies.  I vaguely remembered I hit various of issues with `sync_module_states`+`param_init_fn` in the past until the nightlies in the most recent months. \n2. ~~I wasn't sure what's the best in-general `_param_init_fn` we should use in the current version given the fast evolving PRs around meta device init.  maybe @awgu can comment.~~\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\n\ncc @HamidShojanazeri \n", "758": "Update Step-1 PDF-Pre-Processing-Logic.ipynbReplace \"text\" with \"extracted_text\"\n\n# What does this PR do?\nFix the error, \n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "764": "added a notebook tutorial on llama 3.2 new capabilities# What does this PR do?\n\nThis pull request (PR) adds a new notebook tutorial that showcases the latest capabilities of LLaMA 3.2.\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [X] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "957": "Updated notebook to use Llama API- Fixed previous bugs\n- updated req.txt\n- updated notebooks to use Llama-4 + Llama API", "943": "fix failing github actions# What does this PR do?\n\nFixes failing GitHub actions\n\nThanks for contributing \ud83c\udf89!\n", "228": "minor updates# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nMinor updates to the notebook.", "572": "Adding support for FSDP+Qlora.# What does this PR do?\nThis adds support FSDP + Qlora that enables fine-tuning Llama 70B that lower the computer resource requirements significantly.\n\n\n\n- [ ] Logs\n```\n(fsdp-qlora) bash-5.1$ FSDP_CPU_RAM_EFFICIENT_LOADING=1 ACCELERATE_USE_FSDP=1 torchrun --nnodes 1 --nproc_per_node 4 recipes/finetuning/finetuning.py --enable_fsdp --model_name met\na-llama/Meta-Llama-3-70B --use_peft --peft_method lora  --gradient_accumulation_steps 2 --output_dir peft-output --low_cpu_fsdp  --quantization int4  --mixed_precision False --max_\ntrain_step 5 --max_eval_step 5\nW0624 10:21:48.393000 139928613528064 torch/distributed/run.py:757] \nW0624 10:21:48.393000 139928613528064 torch/distributed/run.py:757] *****************************************\nW0624 10:21:48.393000 139928613528064 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0624 10:21:48.393000 139928613528064 torch/distributed/run.py:757] *****************************************\nClearing GPU cache for all ranks\n--> Running with torch dist debug set to detail\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:28<00:00,  1.06it/s]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:28<00:00,  1.06it/s]\nLoading checkpoint shards:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                           | 23/30 [00:28<00:08,  1.27s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\ntrainable params: 16,384,000 || all params: 70,570,090,496 || trainable%: 0.0232\nMixed precision: None\ntrainable params: 16,384,000 || all params: 70,570,090,496 || trainable%: 0.0232\nMixed precision: None\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:33<00:00,  1.12s/it]\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n--> Model meta-llama/Meta-Llama-3-70B\n\n--> meta-llama/Meta-Llama-3-70B has 2102.665216 Million params\n\ntrainable params: 16,384,000 || all params: 70,570,090,496 || trainable%: 0.0232\nMixed precision: None\nLoading checkpoint shards:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d           | 27/30 [00:33<00:03,  1.17s/it]NCCL version 2.20.5+cuda12.4\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:36<00:00,  1.21s/it]\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\ntrainable params: 16,384,000 || all params: 70,570,090,496 || trainable%: 0.0232\nMixed precision: None\n--> applying fsdp activation checkpointing...\n--> applying fsdp activation checkpointing...\n--> applying fsdp activation checkpointing...\n--> applying fsdp activation checkpointing...\n/home/hamidnazeri/miniconda3/envs/fsdp-qlora/lib/python3.10/site-packages/datasets/load.py:1491: FutureWarning: The repository for samsum contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/samsum\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n  warnings.warn(\n/home/hamidnazeri/miniconda3/envs/fsdp-qlora/lib/python3.10/site-packages/datasets/load.py:1491: FutureWarning: The repository for samsum contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/samsum\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n  warnings.warn(\n/home/hamidnazeri/miniconda3/envs/fsdp-qlora/lib/python3.10/site-packages/datasets/load.py:1491: FutureWarning: The repository for samsum contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/samsum\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n  warnings.warn(\n/home/hamidnazeri/miniconda3/envs/fsdp-qlora/lib/python3.10/site-packages/datasets/load.py:1491: FutureWarning: The repository for samsum contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/samsum\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n  warnings.warn(\n--> Training Set Length = 14732\n/home/hamidnazeri/miniconda3/envs/fsdp-qlora/lib/python3.10/site-packages/datasets/load.py:1491: FutureWarning: The repository for samsum contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/samsum\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n  warnings.warn(\n/home/hamidnazeri/miniconda3/envs/fsdp-qlora/lib/python3.10/site-packages/datasets/load.py:1491: FutureWarning: The repository for samsum contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/samsum\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n  warnings.warn(\n/home/hamidnazeri/miniconda3/envs/fsdp-qlora/lib/python3.10/site-packages/datasets/load.py:1491: FutureWarning: The repository for samsum contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/samsum\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n  warnings.warn(\n/home/hamidnazeri/miniconda3/envs/fsdp-qlora/lib/python3.10/site-packages/datasets/load.py:1491: FutureWarning: The repository for samsum contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/samsum\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n  warnings.warn(\n--> Validation Set Length = 818\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:02<00:00, 7093.02it/s]\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:02<00:00, 7206.71it/s]\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:02<00:00, 7194.31it/s]\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 818/818 [00:00<00:00, 7319.68it/s]\n--> Num of Validation Set Batches loaded = 8\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 818/818 [00:00<00:00, 7424.00it/s]\n--> Num of Validation Set Batches loaded = 8\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:02<00:00, 7024.57it/s]\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 818/818 [00:00<00:00, 7230.16it/s]\n--> Num of Validation Set Batches loaded = 8\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 818/818 [00:00<00:00, 7439.98it/s]\n--> Num of Validation Set Batches loaded = 8\n/home/hamidnazeri/miniconda3/envs/fsdp-qlora/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                                                                                                     | 0/19 [00:00<?, ?it/s]/home/hamidnazeri/miniconda3/envs/fsdp-qlora/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                                                                                                     | 0/19 [00:00<?, ?it/s]/home/hamidnazeri/miniconda3/envs/fsdp-qlora/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                                                                                                     | 0/19 [00:00<?, ?it/s]/home/hamidnazeri/miniconda3/envs/fsdp-qlora/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1/3, step 4/39 completed (loss: 0.8952157497406006):  11%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                                   | 2/19 [02:04<14:05, 49.76s/it]max training steps reached, stopping training, total train steps finished:  5\nTraining Epoch: 1/3, step 4/39 completed (loss: 0.7581993341445923):  11%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                                   | 2/19 [02:04<17:40, 62.37s/it]\nTraining Epoch: 1/3, step 4/39 completed (loss: 0.8486832976341248):  11%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                                   | 2/19 [02:04<17:39, 62.34s/it]\nTraining Epoch: 1/3, step 4/39 completed (loss: 0.9907992482185364):  11%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                                   | 2/19 [02:04<17:40, 62.40s/it]\nTraining Epoch: 1/3, step 4/39 completed (loss: 0.8952157497406006):  11%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                                   | 2/19 [02:04<17:38, 62.26s/it]\nMax CUDA memory allocated was 63 GB\nMax CUDA memory reserved was 78 GB\nPeak active CUDA memory was 63 GB\nCUDA Malloc retries : 0\nCPU Total Peak Memory consumed during the train (max): 9 GB\nevaluating Epoch:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                               | 5/8 [00:09<00:05,  1.81s/it]max eval steps reached, stopping evaluation, total_eval_steps:  5\nevaluating Epoch:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                               | 5/8 [00:09<00:05,  1.85s/it]\nevaluating Epoch:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                               | 5/8 [00:09<00:05,  1.85s/it]\nevaluating Epoch:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                               | 5/8 [00:09<00:05,  1.84s/it]\nevaluating Epoch:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                               | 5/8 [00:09<00:05,  1.83s/it]\n eval_ppl=tensor(2.8264, device='cuda:0') eval_epoch_loss=tensor(1.0390, device='cuda:0')\nwe are about to save the PEFT modules\n/home/hamidnazeri/miniconda3/envs/fsdp-qlora/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/home/hamidnazeri/miniconda3/envs/fsdp-qlora/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/home/hamidnazeri/miniconda3/envs/fsdp-qlora/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/home/hamidnazeri/miniconda3/envs/fsdp-qlora/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nPEFT modules are saved in peft-output directory\nbest eval loss on epoch 1 is 1.0389947891235352\nEpoch 1: train_perplexity=1.1290, train_epoch_loss=0.1214, epoch time 125.38527124101529s\nKey: avg_train_prep, Value: 1.129026174545288\nKey: avg_train_loss, Value: 0.12135542184114456\nKey: avg_eval_prep, Value: 2.8263745307922363\nKey: avg_eval_loss, Value: 1.0389947891235352\nKey: avg_epoch_time, Value: 125.38527124101529\nKey: avg_checkpoint_time, Value: 1.9115908490202855\n```\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "200": "[WIP] feat: add wandb# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\nAdd wandb to monitor training metrics.\n\n## Feature/Issue validation/testing\n\nWandb Link: https://wandb.ai/polym404/llama_recipes/workspace?workspace=user-polym404\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "216": "Fix truncation of training examples in alpaca dataset# What does this PR do?\n\nFix a bug in datasets/alpaca_dataset.py\n\nFix issue - https://github.com/facebookresearch/llama-recipes/issues/215\n\nOriginally the code built each training example with all three parts (instruction, input, response) and if it was larger than max_words, the code just removed the last tokens. As a result, the response might be removed\n\nThis is mainly manifested when using content (i.e., input) in the training examples\n\nI fixed to truncate the (instruction+input) and keep the full response, such that overall it will fit into max_words.\n\n\n<!-- Remove if not applicable -->\n\nFixes # (issue 215)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ x] Test A\nI tried with longer input and I checked that indeed, it keeps the response and just truncates the input\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "570": "bug fix# What does this PR do?\nbug fix for jupyter notebook langgraph-rag-agent.ipynb\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\nissue just submitted too\n\n\n\n\n", "558": "[WIP] Peft Finetuning Quickstart Notebook# What does this PR do?\nThis PR adds a peft finetuning quickstart notebook to the recipes/finetuning folder\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "941": "[Llama Tools] Getting started guide for llama-prompt-ops # What does this PR do?\n\nI am adding a getting started guide to introduce our new Llama Tool: llama-prompt-ops.\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/meta-llama/llama-cookbook/blob/main/CONTRIBUTING.md),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "799": "Browser use with Llama 3.2 Vision Quickstart# Browser Use Llama-Recipe\n\nThis is an example notebook on how to create a Llama 3.2 vision-powered agent that can interact with web browsers on your behalf. It includes a detailed explanation of every section and example use cases. \n\n#### Features\n- Visual understanding of web pages through screenshots\n- Autonomous navigation and interaction\n- Natural language instructions for web tasks\n- Persistent browser session management\n\nFor example, you can ask the agent to:\n- Search for a product on Amazon\n- Find the cheapest flight to Tokyo\n- Buy tickets for the next Warriors game\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [x] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "772": "Update wandb.py to accept setting run name from command line argument (e.g., --wandb_config.name \"run_name\") for fine tuning# What does this PR do?\n\nCurrent wandb logging generate a random run name (despite wandb supporting assigning a run name); I added a missing name attribute to wandb_config, so one could use command line argument like this `--wandb_config.name \"run_name\"` to control the displayed run_name on the wandb webpage.\n\n## Feature/Issue validation/testing\n\nBefore and after the change of this commit, and use argument --wandb_config.name \"fake run name\". Note that `stoic-eon-2` is a random name. \n\n<img width=\"611\" alt=\"Screenshot 2024-11-04 at 15 43 29\" src=\"https://github.com/user-attachments/assets/a5149da4-8a9c-4202-8359-de07c4a3ae83\">\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [x] Did you make sure to update the documentation with your changes?  \n- [x] Did you write any new necessary tests?", "766": "add TogetherNotebookLM recipeAdded an open source implementation of NotebookLM that uses Together AI.\n", "49": "Fix broken links in Dataset.mdThe links to the configs/datasets.py file were broken here.", "75": "deleted# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "606": "Address feedback not possible before launch in LG3 recipe and dataset fileref: https://github.com/meta-llama/llama-recipes-alpha/pull/4", "362": "Fix Anyscale API token URL in Purple_Llama_Anyscale.ipynbThis PR fixes a typo in the docs.\n\nThe previous URL linked for Anyscale was mistakenly pointing to a Together API keys URL.", "404": "Reorg inference throughput folder structure# What does this PR do?\n\nReorg the folder structure of inference throughput to make it more intuitive. \n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [X] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [X] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "376": "Adding examples and updating existing examples using Amazon Bedrock# What does this PR do?\n\nI'm adding examples and versions of existing examples utilizing Amazon Bedrock. \n+ Added a version of the Prompt Engineering with Llama 2 notebook that utilizes Amazon Bedrock\n+ Added a getting started with Llama 2 on Amazon Bedrock notebook which should help developers quickly get started with some reference documentation, setting up credentials, and ultimately using the bedrock client and client_runtime to perform a couple examples and show the diff between prompts run against Llama 2 13b chat vs Llama 2 70b chat\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [x] Did you make sure to update the documentation with your changes?  Documentation is in the notebooks\n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "439": "Updates for Llama 3# What does this PR do?\nUpdates Llama Recipes with content for Llama 3\n", "411": "Implement H2O for long context inference on summarization tasksThis is add the implementation of H2O algorithm for efficient long context inference of Llama models. \nCurrent implementations are based on the Huggingface transformers and tests on summarization tasks, including XSUM and CNN-DailyMail", "377": "Fix `load_model` missing argument# What does this PR do?\n\nAdd the missing argument `use_fast_kernels` to the `load_model` function  in chat completion inference example script\n\n**invoke:**\nhttps://github.com/facebookresearch/llama-recipes/blob/ce3c2ec4285d2b3626534b753b1a37e92049d31a/examples/chat_completion/chat_completion.py#L64\n\n**definition:**\nhttps://github.com/facebookresearch/llama-recipes/blob/ce3c2ec4285d2b3626534b753b1a37e92049d31a/src/llama_recipes/inference/model_utils.py#L8\n\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\nFix `load_model` missing argument\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "363": "Remove deprecated pytest_cmdline_preparse# What does this PR do?\nThis PR fixes the unit test for pytest 8.0 release by removing the deprecated pytest_cmdline_preparse hook\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [X] pytest tests/test_batching.py\nTests get skipped if tokenizer is unavailable\n```\n==================================================================================================================== test session starts =====================================================================================================================\nplatform linux -- Python 3.10.13, pytest-8.0.0, pluggy-1.3.0\nrootdir: /home/ubuntu/llama-recipes\nconfigfile: pyproject.toml\nplugins: mock-3.12.0\ncollected 2 items\n\ntests/test_batching.py ss                                                                                                                                                                                                                              [100%]\n\n====================================================================================================================== warnings summary ======================================================================================================================\n../miniconda3/envs/llama-recipes/lib/python3.10/site-packages/transformers/utils/generic.py:441\n  /home/ubuntu/miniconda3/envs/llama-recipes/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n    _torch_pytree._register_pytree_node(\n\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n=============================================================================================================== 2 skipped, 1 warning in 0.13s ================================================================================================================\n```\n- [X] pytest tests/test_batching.py --unskip-missing-tokenizer\nTokenizer unavailable but we force it to unskip\n ```\n==================================================================================================================== test session starts =====================================================================================================================\nplatform linux -- Python 3.10.13, pytest-8.0.0, pluggy-1.3.0\nrootdir: /home/ubuntu/llama-recipes\nconfigfile: pyproject.toml\nplugins: mock-3.12.0\ncollected 2 items\n\ntests/test_batching.py EE                                                                                                                                                                                                                              [100%]\n\n=========================================================================================================================== ERRORS ===========================================================================================================================\n_______________________________________________________________________________________________________________ ERROR at setup of test_packing __________________________________________________________________________________________________________\n```\n\n\n## Before submitting\n- [X] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [X] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "149": "Update LLM_finetuning.mdAdded missing link to learning more about FSDP and getting started with FSDP.\n\n# What does this PR do?\nUpdated the documentation for FSDP.\n\nFixes # (issue)\nN/A\n\n## Feature/Issue validation/testing\n\nVerified by looking at the preview of the file. \n\n## Before submitting\n- [X ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "834": "fix/fix few typos- **fixed few typos**\n\n# What does this PR do?\nFixes few typos\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "808": "[DRAFT] prompt migration engineHere's a draft PR summary for the prompt migration project:\n\n# What does this PR do?\n\nThis PR introduces a new prompt migration engine that enables conversion and optimization of prompts between different LLM models (e.g., from OpenAI to Llama). The toolkit includes evaluation metrics and comprehensive testing capabilities to ensure prompt quality across different model architectures.  \n\nThis implementation provides a robust framework for teams looking to migrate their prompts between different LLM providers while maintaining prompt quality and effectiveness.\n\n## Feature/Issue validation/testing\n\nTests were run to verify the prompt migration functionality across different use cases:\nTODO: will includesoon\n\n\n## Before submitting\n- [x] Did you read the contributor guideline?\n- [] Documentation has been updated with:\n- [] New tests added for:\n- [x] Environment configuration provided via `environment.yml`\n\nThis implementation provides a robust framework for teams looking to migrate their prompts between different LLM providers while maintaining prompt quality and effectiveness.\n", "767": "Updated QuickStart README# What does this PR do?\n\nUpdates QuickStart README with intro to build with llama 3.2 \n\n\n## Before submitting\n- [X] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "798": "add 5 together llama notebooksAdded the following notebooks using Llama models on Together AI:\n\n- Multimodal RAG with Llama 3.2 90B Vision\n- Open source implementation of Anthropic Contextual RAG using Llama 3.2 3B\n- Knowledge Graphs with Structured Outputs with Llama 3.1 90B JSON mode\n- Structured Text Extraction from Images using Llama 3.2 90B Vision\n- Text RAG with Llama 3.1 8B\n", "940": "Fix/GitHub action runner# What does this PR do?\n\nFixes failing GitHub actions \n", "954": "Adding Databricks RAG example recipe# What does this PR do?\n\nAdding Databricks RAG example with hts code classification usecase.\nThe task for the model is to output a 10 digit hts code given a product description.\nThe following approach leverages RAG with Databricks vector search and structured JSON outputs to tackle the issues of response inconsistencies and format adherence. \n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [x] Tested the notebook on Databricks\nNotebook has logs\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/meta-llama/llama-cookbook/blob/main/CONTRIBUTING.md),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "559": "Add ToolMessage import", "203": "adding optimizer overlap for FSDP# What does this PR do?\n\nThis PR adds Optimizer overlap that bring addition memory savings by fusing the gradinet calculation and parameter update steps.\n\nfor 7B --> max reserved memory saving is 7% \n          ---> allocated memory 4%\n         ---> active memory 4%\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A without optimizer_overlap\nhttps://gist.github.com/HamidShojanazeri/08ee3d23bdb0fa60466071dee1efda1f\n\n- [ ] Test B with optimizer_overlap\nhttps://gist.github.com/HamidShojanazeri/3d1147012e9db130dd7cebf75d3caa64\n\n[Logs with/out anyprecision ](https://gist.github.com/HamidShojanazeri/a8b3689d012f657ef8e84baa4613c13c)\n\n[Logs with AdamW](https://gist.github.com/HamidShojanazeri/cd1a705ecbf238633248a122e258531d)\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ X] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [X] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "217": "Introducing Github Actions CI/CD workflow to run pytests on self-hosted runners in AWS.# What does this PR do?\n\nIntroducing Github Actions CI/CD workflow for running tests/pytests tests on EC2 instances configured as self-hosted runners.\n\nTests are being triggered on following events:\n * push to main branch except ['docs/**', **/*.md']\n * pull request to main branch except ['docs/**', **/*.md']\n * can be manually triggered. \n\n## Feature/Issue validation/testing\n\nHere are the most recent tests executed on the fork: https://github.com/maximgroshev/llama-recipes/actions/runs/6241499581\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [x] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case. (internal)\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nNotes:\n1. @chauhang , @mreso, @HamidShojanazeri let's discuss test triggers strategy to make sure we limit amount of test runs to only parts of the codebase that makes the most sense.\n2. Before this PR can be merged in we still need to create github secrets for sensitive data.\n\nThanks for contributing \ud83c\udf89!\n", "549": "Update langgraph tool calling agent, simplify examples and README# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "575": "New structure and rename for tools, docs and quickstart folderThe following updates were made : \nIn the docs folder,\n- Renamed folder and files to follow standard format\n- Updated the links to point to the new folder/file names\n\nCreated the new tools folder,\n- Created the benchmarking folder\n- Restructured the files to include inference and llm_eval_harness.\n- Deleted the tokenizer folder\n\nUnder recipes updated the quickstart folder ,\n- Added finetuning, inference, RAG folders under it\n- Updated file and folder names to follow the standard format and the links that point to them\n\n\n\n", "207": "adding chat special tokens to tokenizer# What does this PR do?\n\nSmall refactoring to add chat-based special tokens B_INST, E_INST, B_SYS, E_SYS to tokenizer, and then resizing model embedding matrix accordingly (not using pad_to_multiple_of to avoid dependency on https://github.com/huggingface/transformers/pull/25732). This follows implementation from StarCoder: https://github.com/bigcode-project/starcoder/blob/main/chat/train.py#L132.\n\n## Feature/Issue validation/testing\n\nUpdated `test_custom_dataset()` from `test_custom_dataset.py` accordingly. Also tested by running `src.llama_recipes.finetuning.py`.\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [x] Did you make sure to update the documentation with your changes?  \n- [x] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "950": "Fix/update samsum# What does this PR do?\n\nFixes #948\n\nThis PR changes `Samsung/samsum` dataset repo url with an existing clone of that dataset `knkarthick/samsum`.\nOriginal dataset has been deleted from huggingface recently (for any unknown reason).\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/meta-llama/llama-cookbook/blob/main/CONTRIBUTING.md),\n      Pull Request section?\n- [x] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [x] Did you make sure to update the documentation with your changes?  \n- [x] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "944": "[Llama Tools] Getting started fixes for llama-prompt-ops# What does this PR do?\n\nAdded some fixes for a fast follow for llama-prompt-ops\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/meta-llama/llama-cookbook/blob/main/CONTRIBUTING.md),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "763": "Fix quickstart NotebookLlama's dependencies# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFix NotebookLlama demo\n\n- remove pkg `warnings` from requirements, since we can directly `import warnings` in python 3\n- add imports for `wavfile`, `AudioSegment` and `io` to 'Step-4-TTS-Workflow.ipynb'\n- add dependencies for `pydub` and `parler-tts` required from 'Step-4-TTS-Workflow.ipynb'\n- downgrade `transformers` version to 4.43.0, to be compatible with `parler-tts`'s dependency\n\n\n\n## Feature/Issue validation/testing\n\n\nnot applicable\n\n", "70": "Use `defaultdict` to avoid hard coding buffer keys in `Concatenator` and `ConcatDataset`# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nUse `defaultdict` to avoid hard coding buffer keys in `Concatenator` and `ConcatDataset`.\n\n<!--\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n-->\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "165": "Pass `use_cache=False` when training with FSDP# What does this PR do?\n\nThis PR passes `use_cache=False` if using FSDP (since the default is [`True`](https://github.com/huggingface/transformers/blob/aea761499f4b1193f2706f471442da6f9df65d65/src/transformers/models/llama/configuration_llama.py#L120)). The current implementation assumes that FSDP is only used for training (i.e. not inference), in which case we do not need to cache past key/value states. For the LLaMA-7B run, this saves ~4 GB in activation memory and avoids peak memory from being at the end of the forward pass without affecting the end-to-end time.\n\n\n<details>\n<summary> Before (use_cache=True) </summary>\n\n<img width=\"828\" alt=\"snapshot_7b_perparam_8gpus\" src=\"https://github.com/facebookresearch/llama-recipes/assets/31054793/dc1a8e39-eec2-41b9-a8c2-53300eec5c4d\">\nNote the peak memory at the end of forward. This happens because we are saving past key/value states that are not needed for gradient computation.\n\n- Peak active: 14.93 GB\n- Peak reserved: 19.52 GB\n(The numbers from a prototype FSDP implementation, but existing FSDP shows a similar decrease since we only change the activation memory.)\n\n</details>\n\n<details>\n<summary> After (use_cache=False) </summary>\n\n<img width=\"790\" alt=\"snapshot_7b_perparam_8gpus\" src=\"https://github.com/facebookresearch/llama-recipes/assets/31054793/f99e0270-fb4a-4b36-80b4-c94e9700d569\">\n\n- Peak active: 10.94 GB\n- Peak reserved: 15.11 GB\n\n</details>\n\n\n\n\n\n## Feature/Issue validation/testing\n\n```\ntorchrun --nnodes 1 --nproc_per_node 8  llama_finetuning.py --enable_fsdp --model_name llama-2-7b-hf/ --pure_bf16 --output_dir output_dir --use_fast_kernels\n```\n\n\nFeel free to take over this PR and test further.", "603": "Release updateRelease update", "398": "Add llm class so that externally-hosted models can be calledThis change adds the ability  to call out to 3rd party hosted models on OpenAI, Together AI and Anyscale\n\n## Feature/Issue validation/testing\nTest case:\n```\nfrom llama_recipes.inference.llm import  TOGETHER, OPENAI, ANYSCALE\n\ntogether_example = TOGETHER(\"togethercomputer/llama-2-7b-chat\",\"09e45...\")\nprint( together_example.query(prompt=\"Why is the sky blue?\"))\n\n\nopenai_example = OPENAI(\"gpt-3.5-turbo\",\"sk-LIz9zL3cYp...\")\nprint( openai_example.query(prompt=\"Why is the sky blue?\"))\n\n\nanyscale_example = ANYSCALE(\"meta-llama/Llama-2-7b-chat-hf\",\"esecret_c3u4x7...\")\nprint( anyscale_example.query(prompt=\"Why is the sky blue?\"))\n```", "429": "Recipe to add a new language to Llama2# What does this PR do?\n\nThis PR introduces a new recipe to augment a new language to the Llama2 tokenizer and continue pre-training. This recipe replicates the results of [OpenHathi](https://www.sarvam.ai/blog/announcing-openhathi-series) when the right data mixture is used.\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [x] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [x] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n", "367": "Add gradio library for user interface in inference.py# What does this PR do?\nThis PR implements a WebUI by utilizing Gradio in inference.py.\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "373": "forking repo# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "415": "only save training params on rank 0# What does this PR do?\n\nAddresses a race condition in FSDP training where every rank tries to write the training params. This writing from every rank is also redundant, since these params will be the same for every rank.\n\nFixes #410 \n\n\n## Feature/Issue validation/testing\n\nI tested this on my local setup and it works.\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ X] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ X] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "372": "Fix/missing copyright headers# What does this PR do?\nThis PR adds some missing copyright headers and a script to automate the process for potential pre-commit integration\n\nFixes # (issue)\n\n\n\n## Before submitting\n- [X] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [X] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n\n\nThanks for contributing \ud83c\udf89!\n", "400": "Adding Llama Guard notebooks# What does this PR do?\n\nAdds notebooks to run Llama Guard from HF or local weights. Adds validation notebook to test Llama Guard performance on a custom dataset. The dataset is not provided, example datasets to come in future versions\n\n## Feature/Issue validation/testing\n\nTested running both notebooks and checking the results are as expected.\nFor Inference, the sample prompts are run through the downloaded HF model and the results printed.\nFor Validation, a sample dataset is run through the model and average presicion printed as well.\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [x] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\n", "428": "Adding a feature that will stop the training/eval process after reaching some max_steps\nAdding a feature that will stop the training/eval process after reaching some max_steps. By default, this feature is disabled and the max_train_step and max_eval_step are set to 0. User can pass arguments to set max_train_step or max_eval_step \nto a positive integer so that the  training or eval process will be stop when reaching the max_step limit. This PR will introduce two more arguments in the config file and thus the testing functions test_gradient_accumulation and test_save_to_json has been modified. \n\n\ntest log with max_train_step=6, max_eval_step=2\n```text\n~/work/llama-recipes (feature/add_max_steps)]$ torchrun --nnodes 1 --nproc_per_node 4  recipes/finetuning/finetuning.py --max_train_step 6 --max_eval_step 2 --use_peft --peft_method lora  --model_name meta-llama/Llama-2-7b-chat-hf --enable_fsdp --use_fast_kernels --dist_checkpoint_root_folder /home/kaiwu/work/llama2-7b/ --dist_checkpoint_folder /home/kaiwu/work/finetune-output\n[2024-04-08 15:46:25,374] torch.distributed.run: [WARNING] \n[2024-04-08 15:46:25,374] torch.distributed.run: [WARNING] *****************************************\n[2024-04-08 15:46:25,374] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n[2024-04-08 15:46:25,374] torch.distributed.run: [WARNING] *****************************************\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/utils/generic.py:485: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n  _torch_pytree._register_pytree_node(\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/utils/generic.py:485: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n  _torch_pytree._register_pytree_node(\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/utils/generic.py:485: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n  _torch_pytree._register_pytree_node(\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/utils/generic.py:485: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n  _torch_pytree._register_pytree_node(\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/utils/generic.py:342: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n  _torch_pytree._register_pytree_node(\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/utils/generic.py:342: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n  _torch_pytree._register_pytree_node(\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/utils/generic.py:342: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n  _torch_pytree._register_pytree_node(\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/utils/generic.py:342: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n  _torch_pytree._register_pytree_node(\nClearing GPU cache for all ranks\n--> Running with torch dist debug set to detail\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:07<00:00,  3.57s/it]\n--> Model meta-llama/Llama-2-7b-chat-hf\n\n--> meta-llama/Llama-2-7b-chat-hf has 6738.415616 Million params\n\ntrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\nbFloat16 enabled for mixed precision - using bfSixteen policy\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:07<00:00,  3.59s/it]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:07<00:00,  3.57s/it]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:07<00:00,  3.60s/it]\ntrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\ntrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\ntrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n--> applying fsdp activation checkpointing...\n--> applying fsdp activation checkpointing...\n--> applying fsdp activation checkpointing...\n--> applying fsdp activation checkpointing...\nReusing dataset samsum (/home/kaiwu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\nParameter 'function'=<function get_preprocessed_samsum.<locals>.apply_prompt_template at 0x7f2af7ba9e10> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\nLoading cached processed dataset at /home/kaiwu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-1c80317fa3b1799d.arrow\nLoading cached processed dataset at /home/kaiwu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-bdd640fb06671ad1.arrow\nReusing dataset samsum (/home/kaiwu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\nParameter 'function'=<function get_preprocessed_samsum.<locals>.apply_prompt_template at 0x7feb86989e10> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\nLoading cached processed dataset at /home/kaiwu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-1c80317fa3b1799d.arrow\nLoading cached processed dataset at /home/kaiwu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-bdd640fb06671ad1.arrow\nReusing dataset samsum (/home/kaiwu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\nParameter 'function'=<function get_preprocessed_samsum.<locals>.apply_prompt_template at 0x7fb55cd99f30> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\nLoading cached processed dataset at /home/kaiwu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-1c80317fa3b1799d.arrow\nLoading cached processed dataset at /home/kaiwu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-bdd640fb06671ad1.arrow\n--> Training Set Length = 14732\nReusing dataset samsum (/home/kaiwu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\nParameter 'function'=<function get_preprocessed_samsum.<locals>.apply_prompt_template at 0x7f1c1f3a1e10> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\nLoading cached processed dataset at /home/kaiwu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-1c80317fa3b1799d.arrow\nLoading cached processed dataset at /home/kaiwu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-bdd640fb06671ad1.arrow\nReusing dataset samsum (/home/kaiwu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\nLoading cached processed dataset at /home/kaiwu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-3eb13b9046685257.arrow\nReusing dataset samsum (/home/kaiwu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\nLoading cached processed dataset at /home/kaiwu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-23b8c1e9392456de.arrow\nPreprocessing dataset:   0%|                                                                                                            | 0/14732 [00:00<?, ?it/s]Loading cached processed dataset at /home/kaiwu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-3eb13b9046685257.arrow\nLoading cached processed dataset at /home/kaiwu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-23b8c1e9392456de.arrow\n--> Validation Set Length = 818\nPreprocessing dataset:   0%|                                                                                                            | 0/14732 [00:00<?, ?it/s]Reusing dataset samsum (/home/kaiwu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\nLoading cached processed dataset at /home/kaiwu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-3eb13b9046685257.arrow\nLoading cached processed dataset at /home/kaiwu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-23b8c1e9392456de.arrow\nPreprocessing dataset:   4%|\u2588\u2588\u2588\u2588                                                                                            | 616/14732 [00:00<00:02, 6154.09it/s]Reusing dataset samsum (/home/kaiwu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\nLoading cached processed dataset at /home/kaiwu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-3eb13b9046685257.arrow\nLoading cached processed dataset at /home/kaiwu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-23b8c1e9392456de.arrow\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:02<00:00, 5964.66it/s]\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:02<00:00, 5955.82it/s]\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:02<00:00, 6072.75it/s]\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:02<00:00, 6058.17it/s]\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 818/818 [00:00<00:00, 5870.09it/s]\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 818/818 [00:00<00:00, 5810.93it/s]\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 818/818 [00:00<00:00, 5721.43it/s]\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 818/818 [00:00<00:00, 5878.17it/s]\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                                                                                   | 0/48 [00:00<?, ?it/s]/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                                                                                   | 0/48 [00:00<?, ?it/s]/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                                                                                   | 0/48 [00:00<?, ?it/s]NCCL version 2.19.3+cuda12.1\nTraining Epoch: 1/3, step 5/48 completed (loss: 1.6506224870681763):  12%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                                 | 6/48 [00:27<03:09,  4.52s/it]\nTraining Epoch: 1/3, step 5/48 completed (loss: 1.578351378440857):  12%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                                  | 6/48 [00:27<03:09,  4.52s/it]\nTraining Epoch: 1/3, step 5/48 completed (loss: 1.608504295349121):  12%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                                  | 6/48 [00:27<03:10,  4.54s/it]\nTraining Epoch: 1/3, step 5/48 completed (loss: 1.5797399282455444):  12%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                                 | 6/48 [00:27<03:10,  4.54s/it]\nMax CUDA memory allocated was 17 GB\nMax CUDA memory reserved was 21 GB\nPeak active CUDA memory was 18 GB\nCUDA Malloc retries : 0\nCPU Total Peak Memory consumed during the train (max): 8 GB\nevaluating Epoch:  18%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                                                        | 2/11 [00:00<00:04,  2.17it/s]max eval steps reached, stopping evaluation, total_eval_steps:  2\nevaluating Epoch:  18%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                                                        | 2/11 [00:01<00:05,  1.79it/s]\nevaluating Epoch:  18%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                                                        | 2/11 [00:01<00:04,  1.88it/s]\nevaluating Epoch:  18%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                                                        | 2/11 [00:00<00:04,  2.16it/s]\nevaluating Epoch:  18%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                                                        | 2/11 [00:00<00:04,  2.15it/s]\n eval_ppl=tensor(1.3011, device='cuda:0') eval_epoch_loss=tensor(0.2632, device='cuda:0')\nwe are about to save the PEFT modules\nPEFT modules are saved in PATH/to/save/PEFT/model directory\nbest eval loss on epoch 1 is 0.26322659850120544\nEpoch 1: train_perplexity=1.2784, train_epoch_loss=0.2456, epoch time 27.707596888765693s\nmax training steps reached, stopping training, total_train_steps:  6\nKey: avg_train_prep, Value: 1.2784380912780762\nKey: avg_train_loss, Value: 0.24563908576965332\nKey: avg_eval_prep, Value: 1.301121473312378\nKey: avg_eval_loss, Value: 0.26322659850120544\nKey: avg_epoch_time, Value: 27.707596888765693\nKey: avg_checkpoint_time, Value: 1.0763725992292166", "399": "Reorg Benchmark folder structure\n\n# What does this PR do?\n\nMerging two subfolders\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [X] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [X] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "616": "Move supported features table to main README# What does this PR do?\n\nMove to main README for better visibility\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "602": "Corrected wrong order of commandsIn the notebook, under section 4.3.2, LangChain Q&A Retriever, the code block where we test chat history has two commands switched in order, resulting in an error while running. Switching to have the \"query\" variable set first fixes this issue.\n\n# What does this PR do?\nThis PR switches two lines of code, with one meant to run before the other. The \n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "158": "Fix the chat example for generate when load peft model# What does this PR do?\n\nfix issue #154 \n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nBefore\nhttps://gist.github.com/HamidShojanazeri/d3b1231c4b637ef246725a3d8b836ded\n- [ ] Test B\nAfter the fix\nhttps://gist.github.com/HamidShojanazeri/9b445460efe544ae37376b30d6021c6f\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "825": "Llama code/code review agents# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\nThought this would be an interesting example to have - to showcase structured outputs, tool calls and looping with Llama.\n\nHere's a demo:\n\nhttps://github.com/user-attachments/assets/f717889a-e517-4380-a07b-9657319dd189\n\nLmk what you think @mreso @wukaixingxp - do you think there's value in having this demo here?\n\n## Feature/Issue validation/testing\n\nSee README.md\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [x] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "789": "Rebase", "951": "Fix model string name for Llama-Prompt-Guard-2-86M in inference.pygetting-started/responsible_ai/prompt_guard/inference.py has the incorrect model string for Prompt Guard 2. It's correct here\n\nhttps://github.com/meta-llama/llama-cookbook/blob/7dab6a3298ec2e4742283670b56b6d405bafb54d/getting-started/responsible_ai/prompt_guard/inference.py#L22\n\nbut wrong here\n\nhttps://github.com/meta-llama/llama-cookbook/blob/7dab6a3298ec2e4742283670b56b6d405bafb54d/getting-started/responsible_ai/prompt_guard/inference.py#L26", "206": "Feature/length based batch sampling# What does this PR do?\nThis PR introduces padding for all datasets instead of concatenation of samples to reflect best practices for fine-tuning. To reduce impact of padding on training performance it adds length based sorting to create batches of similar length.\n\nFixes # (issue)\n#146 #209 \n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\nTest cases:\n* Test A: unit tests\n* Test B: samsum + fsdp + packing\n* Test C: samsum + fsdp + padding\n* Test D: alpaca + fsdp + padding\n* Test E: alpaca + fsdp + packing\n\n- [X] Test A: pytest tests\nLogs for Test A:\n```\n============================================================================================================ test session starts =============================================================================================================\nplatform linux -- Python 3.10.12, pytest-7.4.2, pluggy-1.3.0\nrootdir: /home/mreso/llama-recipes\nplugins: mock-3.11.1\ncollected 23 items\n\ntests/test_batching.py ..                                                                                                                                                                                                              [  8%]\ntests/test_finetuning.py .....                                                                                                                                                                                                         [ 30%]\ntests/test_sampler.py ..........                                                                                                                                                                                                       [ 73%]\ntests/test_train_utils.py .                                                                                                                                                                                                            [ 78%]\ntests/datasets/test_alpaca_dataset.py s                                                                                                                                                                                                [ 82%]\ntests/datasets/test_custom_dataset.py ..                                                                                                                                                                                               [ 91%]\ntests/datasets/test_grammar_datasets.py .                                                                                                                                                                                              [ 95%]\ntests/datasets/test_samsum_datasets.py .                                                                                                                                                                                               [100%]\n\n============================================================================================================== warnings summary ==============================================================================================================\nsrc/llama_recipes/finetuning.py:5\n  /home/mreso/llama-recipes/src/llama_recipes/finetuning.py:5: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n    from pkg_resources import packaging\n\n../local/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/_shard/checkpoint/__init__.py:8\n  /home/mreso/local/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/_shard/checkpoint/__init__.py:8: DeprecationWarning: torch.distributed._shard.checkpoint will be deprecated, use torch.distributed.checkpoint instead\n    warnings.warn(\n\ntests/test_train_utils.py::test_gradient_accumulation\n  /home/mreso/local/miniconda3/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n    warnings.warn(\n\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n================================================================================================= 22 passed, 1 skipped, 3 warnings in 42.00s =================================================================================================\n```\n\n- [X] Test B: Samsum dataset + fsdp + packing\ntorchrun --nnodes 1 --nproc_per_node 8 examples/finetuning.py --enable_fsdp --use_peft --peft_method lora --model_name meta-llama/Llama-2-7b-hf --fsdp_config.pure_bf16 --output_dir ~/test_output --dataset samsum_dataset --batching_strategy padding --batch_size_training 32\nLogs for Test B:\n```\nTraining Epoch: 2/3, step 23/24 completed (loss: 1.0304172039031982): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 24/24 [04:20<00:00, 10.84s/it]\nTraining Epoch: 2/3, step 23/24 completed (loss: 1.0290634632110596): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 24/24 [04:20<00:00, 10.84s/it]\nTraining Epoch: 2/3, step 23/24 completed (loss: 1.0410324335098267): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 24/24 [04:20<00:00, 10.84s/it]\nTraining Epoch: 2/3, step 23/24 completed (loss: 1.0868538618087769): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 24/24 [04:20<00:00, 10.84s/it]\nTraining Epoch: 2/3, step 23/24 completed (loss: 1.0481512546539307): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 24/24 [04:20<00:00, 10.84s/it]\nTraining Epoch: 2/3, step 23/24 completed (loss: 1.0640676021575928): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 24/24 [04:20<00:00, 10.84s/it]\nTraining Epoch: 2/3, step 23/24 completed (loss: 1.0330259799957275): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 24/24 [04:20<00:00, 10.84s/it]\nTraining Epoch: 2/3, step 23/24 completed (loss: 1.1261721849441528): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 24/24 [04:20<00:00, 10.84s/it]\nMax CUDA memory allocated was 47 GB\nMax CUDA memory reserved was 55 GB\nPeak active CUDA memory was 47 GB\nCuda Malloc retires : 0\nCPU Total Peak Memory consumed during the train (max): 6 GB\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [00:06<00:00,  1.06s/it]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [00:06<00:00,  1.06s/it]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [00:06<00:00,  1.06s/it]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [00:06<00:00,  1.06s/it]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [00:06<00:00,  1.06s/it]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [00:06<00:00,  1.10s/it]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [00:06<00:00,  1.06s/it]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [00:06<00:00,  1.07s/it]\n eval_ppl=tensor(2.8044, device='cuda:0') eval_epoch_loss=tensor(1.0312, device='cuda:0')\nwe are about to save the PEFT modules\nPEFT modules are saved in /home/mreso/test_output directory\nbest eval loss on epoch 2 is 1.0311944484710693\nEpoch 2: train_perplexity=2.9011, train_epoch_loss=1.0651, epoch time 261.8630598662421s\nTraining Epoch: 3/3, step 23/24 completed (loss: 1.014349102973938): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 24/24 [04:19<00:00, 10.81s/it]\nTraining Epoch: 3/3, step 23/24 completed (loss: 1.0295597314834595): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 24/24 [04:19<00:00, 10.81s/it]\nTraining Epoch: 3/3, step 23/24 completed (loss: 1.010465145111084): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 24/24 [04:19<00:00, 10.81s/it]\nTraining Epoch: 3/3, step 23/24 completed (loss: 1.010661244392395): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 24/24 [04:19<00:00, 10.81s/it]\nTraining Epoch: 3/3, step 23/24 completed (loss: 1.0261698961257935): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 24/24 [04:19<00:00, 10.83s/it]\nTraining Epoch: 3/3, step 23/24 completed (loss: 1.0660455226898193): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 24/24 [04:19<00:00, 10.82s/it]\nTraining Epoch: 3/3, step 23/24 completed (loss: 1.1079691648483276): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 24/24 [04:19<00:00, 10.82s/it]\nTraining Epoch: 3/3, step 23/24 completed (loss: 1.0423952341079712): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 24/24 [04:19<00:00, 10.81s/it]\nMax CUDA memory allocated was 47 GB\nMax CUDA memory reserved was 55 GB\nPeak active CUDA memory was 47 GB\nCuda Malloc retires : 0\nCPU Total Peak Memory consumed during the train (max): 6 GB\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [00:06<00:00,  1.06s/it]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [00:06<00:00,  1.06s/it]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [00:06<00:00,  1.05s/it]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [00:06<00:00,  1.06s/it]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [00:06<00:00,  1.05s/it]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [00:06<00:00,  1.07s/it]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [00:06<00:00,  1.05s/it]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [00:06<00:00,  1.06s/it]\n eval_ppl=tensor(2.7661, device='cuda:0') eval_epoch_loss=tensor(1.0175, device='cuda:0')\nwe are about to save the PEFT modules\nPEFT modules are saved in /home/mreso/test_output directory\nbest eval loss on epoch 3 is 1.0174505710601807\nEpoch 3: train_perplexity=2.8219, train_epoch_loss=1.0374, epoch time 261.04267975781113s\nKey: avg_train_prep, Value: 3.0046136379241943\nKey: avg_train_loss, Value: 1.0978813171386719\nKey: avg_eval_prep, Value: 2.8239963054656982\nKey: avg_eval_loss, Value: 1.0379509925842285\nKey: avg_epoch_time, Value: 264.3428177877019\nKey: avg_checkpoint_time, Value: 1.1804062935213249\n```\n\n- [X] Test C: samsum dataset + fsdp + padding\nCUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 torchrun --nnodes 1  --nproc_per_node 6 examples/finetuning.py --enable_fsdp --use_peft --peft_method lora --model_name meta-llama/Llama-2-7b-hf  --pure_bf16 --output_dir  fsdp_output --batch_size_training 8  --dataset grammar_dataset\nLogs for Test C:\n```\nTraining Epoch: 2/3, step 56/57 completed (loss: 1.0368962287902832): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 57/57 [04:37<00:00,  4.87s/it]\nTraining Epoch: 2/3, step 56/57 completed (loss: 0.9557473659515381): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 57/57 [04:37<00:00,  4.87s/it]\nTraining Epoch: 2/3, step 56/57 completed (loss: 0.8145267367362976): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 57/57 [04:37<00:00,  4.87s/it]\nTraining Epoch: 2/3, step 56/57 completed (loss: 0.7587720155715942): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 57/57 [04:37<00:00,  4.88s/it]\nTraining Epoch: 2/3, step 56/57 completed (loss: 0.7613183259963989): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 57/57 [04:37<00:00,  4.87s/it]\nTraining Epoch: 2/3, step 56/57 completed (loss: 0.8573808670043945): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 57/57 [04:37<00:00,  4.88s/it]\nTraining Epoch: 2/3, step 56/57 completed (loss: 1.0076509714126587): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 57/57 [04:37<00:00,  4.87s/it]\nMax CUDA memory allocated was 20 GB\nMax CUDA memory reserved was 44 GB\nPeak active CUDA memory was 20 GB\nCuda Malloc retires : 0\nCPU Total Peak Memory consumed during the train (max): 4 GB\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 102/102 [00:27<00:00,  3.71it/s]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 102/102 [00:27<00:00,  3.72it/s]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 102/102 [00:27<00:00,  3.71it/s]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 102/102 [00:27<00:00,  3.69it/s]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 102/102 [00:27<00:00,  3.70it/s]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 102/102 [00:27<00:00,  3.71it/s]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 102/102 [00:27<00:00,  3.70it/s]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 102/102 [00:27<00:00,  3.71it/s]\n eval_ppl=tensor(2.5202, device='cuda:0') eval_epoch_loss=tensor(0.9243, device='cuda:0')\nwe are about to save the PEFT modules\nPEFT modules are saved in /home/mreso/test_output directory\nbest eval loss on epoch 2 is 0.9243239760398865\nEpoch 2: train_perplexity=2.5613, train_epoch_loss=0.9405, epoch time 279.17079912591726s\nTraining Epoch: 3/3, step 56/57 completed (loss: 0.6804153323173523): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 57/57 [04:25<00:00,  4.65s/it]\nTraining Epoch: 3/3, step 56/57 completed (loss: 0.942245602607727): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 57/57 [04:25<00:00,  4.65s/it]\nTraining Epoch: 3/3, step 56/57 completed (loss: 0.8569772243499756): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 57/57 [04:25<00:00,  4.65s/it]\nTraining Epoch: 3/3, step 56/57 completed (loss: 0.7652250528335571): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 57/57 [04:25<00:00,  4.66s/it]\nTraining Epoch: 3/3, step 56/57 completed (loss: 1.1062904596328735): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 57/57 [04:25<00:00,  4.66s/it]\nTraining Epoch: 3/3, step 56/57 completed (loss: 0.5810124278068542): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 57/57 [04:25<00:00,  4.65s/it]\nTraining Epoch: 3/3, step 56/57 completed (loss: 0.9398964643478394): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 57/57 [04:25<00:00,  4.65s/it]\nTraining Epoch: 3/3, step 56/57 completed (loss: 1.0624254941940308): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 57/57 [04:25<00:00,  4.65s/it]\nevaluating Epoch:   0%|                                                                                                                                                                                               | 0/102 [00:00<?, ?it/s]\nMax CUDA memory allocated was 19 GB\nMax CUDA memory reserved was 54 GB\nPeak active CUDA memory was 19 GB\nCuda Malloc retires : 0\nCPU Total Peak Memory consumed during the train (max): 4 GB\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 102/102 [00:26<00:00,  3.83it/s]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 102/102 [00:26<00:00,  3.89it/s]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 102/102 [00:26<00:00,  3.83it/s]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 102/102 [00:26<00:00,  3.88it/s]\n\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 102/102 [00:26<00:00,  3.83it/s]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 102/102 [00:26<00:00,  3.89it/s]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 102/102 [00:26<00:00,  3.89it/s]\n eval_ppl=tensor(2.4965, device='cuda:0') eval_epoch_loss=tensor(0.9149, device='cuda:0')\nwe are about to save the PEFT modules\nPEFT modules are saved in /home/mreso/test_output directory\nbest eval loss on epoch 3 is 0.9148927927017212\nEpoch 3: train_perplexity=2.5002, train_epoch_loss=0.9164, epoch time 266.9319120440632s\nKey: avg_train_prep, Value: 2.6538760662078857\nKey: avg_train_loss, Value: 0.9738798141479492\nKey: avg_eval_prep, Value: 2.531179189682007\nKey: avg_eval_loss, Value: 0.9285968542098999\nKey: avg_epoch_time, Value: 274.78595419911045\nKey: avg_checkpoint_time, Value: 0.818783702639242\n```\n- [X] Test D: alpaca dataset + fsdp + padding\ntorchrun --nnodes 1 --nproc_per_node 8 examples/finetuning.py --enable_fsdp --use_peft --peft_method lora --model_name meta-llama/Llama-2-7b-hf --fsdp_config.pure_bf16 --output_dir ~/test_output --dataset alpaca_dataset --batching_strategy padding --batch_size_training 32\nLogs for Test D:\n```\nTraining Epoch: 2/3, step 202/203 completed (loss: 1.077280879020691): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 203/203 [08:25<00:00,  2.49s/it]\nTraining Epoch: 2/3, step 202/203 completed (loss: 0.7487268447875977): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 203/203 [08:25<00:00,  2.49s/it]\nTraining Epoch: 2/3, step 202/203 completed (loss: 1.042739748954773): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 203/203 [08:25<00:00,  2.49s/it]\nTraining Epoch: 2/3, step 202/203 completed (loss: 1.1533852815628052): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 203/203 [08:25<00:00,  2.49s/it]\nTraining Epoch: 2/3, step 202/203 completed (loss: 1.1549054384231567): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 203/203 [08:25<00:00,  2.49s/it]\nTraining Epoch: 2/3, step 202/203 completed (loss: 0.9549697041511536): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 203/203 [08:25<00:00,  2.49s/it]\nTraining Epoch: 2/3, step 202/203 completed (loss: 0.8267899751663208): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 203/203 [08:25<00:00,  2.49s/it]\nTraining Epoch: 2/3, step 202/203 completed (loss: 1.0749969482421875): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 203/203 [08:25<00:00,  2.49s/it]\nMax CUDA memory allocated was 20 GB\nMax CUDA memory reserved was 30 GB\nPeak active CUDA memory was 20 GB\nCuda Malloc retires : 0\nCPU Total Peak Memory consumed during the train (max): 4 GB\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25/25 [00:06<00:00,  3.72it/s]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25/25 [00:06<00:00,  3.74it/s]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25/25 [00:06<00:00,  3.71it/s]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25/25 [00:06<00:00,  3.71it/s]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25/25 [00:06<00:00,  3.71it/s]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25/25 [00:06<00:00,  3.75it/s]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25/25 [00:06<00:00,  3.71it/s]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25/25 [00:06<00:00,  3.72it/s]\n eval_ppl=tensor(2.1959, device='cuda:0') eval_epoch_loss=tensor(0.7866, device='cuda:0')\nwe are about to save the PEFT modules\nPEFT modules are saved in /home/mreso/test_output directory\nbest eval loss on epoch 2 is 0.7866132259368896\nEpoch 2: train_perplexity=2.6297, train_epoch_loss=0.9669, epoch time 506.9151955069974s\nTraining Epoch: 3/3, step 202/203 completed (loss: 0.6994926333427429): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 203/203 [08:31<00:00,  2.52s/it]\nTraining Epoch: 3/3, step 202/203 completed (loss: 1.128872275352478): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 203/203 [08:31<00:00,  2.52s/it]\nTraining Epoch: 3/3, step 202/203 completed (loss: 1.1014574766159058): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 203/203 [08:31<00:00,  2.52s/it]\nTraining Epoch: 3/3, step 202/203 completed (loss: 0.7173508405685425): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 203/203 [08:31<00:00,  2.52s/it]\nTraining Epoch: 3/3, step 202/203 completed (loss: 1.0579278469085693): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 203/203 [08:31<00:00,  2.52s/it]\nTraining Epoch: 3/3, step 202/203 completed (loss: 0.7605793476104736): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 203/203 [08:31<00:00,  2.52s/it]\nTraining Epoch: 3/3, step 202/203 completed (loss: 0.829206109046936): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 203/203 [08:31<00:00,  2.52s/it]\nTraining Epoch: 3/3, step 202/203 completed (loss: 0.7195559144020081): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 203/203 [08:31<00:00,  2.52s/it]\nMax CUDA memory allocated was 18 GB\nMax CUDA memory reserved was 28 GB\nPeak active CUDA memory was 18 GB\nCuda Malloc retires : 0\nCPU Total Peak Memory consumed during the train (max): 4 GB\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25/25 [00:06<00:00,  3.69it/s]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25/25 [00:06<00:00,  3.93it/s]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25/25 [00:06<00:00,  3.95it/s]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25/25 [00:06<00:00,  3.96it/s]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25/25 [00:06<00:00,  3.94it/s]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25/25 [00:06<00:00,  3.73it/s]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25/25 [00:06<00:00,  3.96it/s]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25/25 [00:06<00:00,  3.96it/s]\n eval_ppl=tensor(2.1615, device='cuda:0') eval_epoch_loss=tensor(0.7708, device='cuda:0')\nwe are about to save the PEFT modules\nPEFT modules are saved in /home/mreso/test_output directory\nbest eval loss on epoch 3 is 0.7707816958427429\nEpoch 3: train_perplexity=2.5799, train_epoch_loss=0.9478, epoch time 512.4155400330201s\nKey: avg_train_prep, Value: 2.665184497833252\nKey: avg_train_loss, Value: 0.9797371625900269\nKey: avg_eval_prep, Value: 2.2003698348999023\nKey: avg_eval_loss, Value: 0.7885082960128784\nKey: avg_epoch_time, Value: 514.1213209042326\nKey: avg_checkpoint_time, Value: 0.9034845388183991\n```\n\n- [X] Test E: alpaca dataset + fsdp + packing\ntorchrun --nnodes 1 --nproc_per_node 8 examples/finetuning.py --enable_fsdp --use_peft --peft_method lora --model_name meta-llama/Llama-2-7b-hf --fsdp_config.pure_bf16 --output_dir ~/test_output --dataset alpaca_dataset --batching_strategy packing\nLogs for Test E:\n```\nTraining Epoch: 2/3, step 49/50 completed (loss: 1.035962462425232): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50/50 [09:01<00:00, 10.82s/it]\nTraining Epoch: 2/3, step 49/50 completed (loss: 1.0448617935180664): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50/50 [09:01<00:00, 10.82s/it]\nTraining Epoch: 2/3, step 49/50 completed (loss: 1.0004732608795166): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50/50 [09:01<00:00, 10.82s/it]\nTraining Epoch: 2/3, step 49/50 completed (loss: 1.1268821954727173): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50/50 [09:01<00:00, 10.82s/it]\nMax CUDA memory allocated was 47 GB\nMax CUDA memory reserved was 55 GB\nPeak active CUDA memory was 47 GB\nCuda Malloc retires : 0\nCPU Total Peak Memory consumed during the train (max): 6 GB\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.33s/it]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.35s/it]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.34s/it]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.35s/it]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.36s/it]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.36s/it]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.37s/it]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.45s/it]\n eval_ppl=tensor(2.9537, device='cuda:0') eval_epoch_loss=tensor(1.0831, device='cuda:0')\nwe are about to save the PEFT modules\nPEFT modules are saved in /home/mreso/test_output directory\nbest eval loss on epoch 2 is 1.0830509662628174\nEpoch 2: train_perplexity=2.9426, train_epoch_loss=1.0793, epoch time 542.6185016939417s\nTraining Epoch: 3/3, step 49/50 completed (loss: 1.0890319347381592): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50/50 [09:00<00:00, 10.81s/it]\nTraining Epoch: 3/3, step 49/50 completed (loss: 1.0062317848205566): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50/50 [09:00<00:00, 10.81s/it]\nTraining Epoch: 3/3, step 49/50 completed (loss: 1.0376760959625244): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50/50 [09:00<00:00, 10.81s/it]\n\nTraining Epoch: 3/3, step 49/50 completed (loss: 1.0268306732177734): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50/50 [09:00<00:00, 10.81s/it]\nTraining Epoch: 3/3, step 49/50 completed (loss: 1.0510032176971436): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50/50 [09:00<00:00, 10.81s/it]\n\nTraining Epoch: 3/3, step 49/50 completed (loss: 1.095015525817871): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50/50 [09:00<00:00, 10.81s/it]\nMax CUDA memory allocated was 47 GB\nMax CUDA memory reserved was 55 GB\nPeak active CUDA memory was 47 GB\nCuda Malloc retires : 0\nCPU Total Peak Memory consumed during the train (max): 6 GB\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.48s/it]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.38s/it]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.39s/it]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.30s/it]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.34s/it]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.43s/it]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.52s/it]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.46s/it]\n eval_ppl=tensor(2.9295, device='cuda:0') eval_epoch_loss=tensor(1.0748, device='cuda:0')\nwe are about to save the PEFT modules\nPEFT modules are saved in /home/mreso/test_output directory\nbest eval loss on epoch 3 is 1.0748209953308105\nEpoch 3: train_perplexity=2.9086, train_epoch_loss=1.0677, epoch time 542.4324695924297s\nKey: avg_train_prep, Value: 2.980083465576172\nKey: avg_train_loss, Value: 1.091610074043274\nKey: avg_eval_prep, Value: 2.9596056938171387\nKey: avg_eval_loss, Value: 1.085013508796692\nKey: avg_epoch_time, Value: 543.8247058854128\nKey: avg_checkpoint_time, Value: 0.6907645293201009\ndevgpu005:1911236:1975258 [0] NCCL INFO [Service thread] Connection closed by localRank 7\ndevgpu005:1911236:1975258 [0] NCCL INFO [Service thread] Connection closed by localRank 1\ndevgpu005:1911236:1975258 [0] NCCL INFO [Service thread] Connection closed by localRank 4\ndevgpu005:1911236:1975258 [0] NCCL INFO [Service thread] Connection closed by localRank 5\n```\n\n## Before submitting\n- [X] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [X] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "560": "4 notebooks ported from 4 DLAI agent short courses using Llama 3# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "212": "Make tests run on cpu instance# What does this PR do?\nThis PR fixes the unit tests to not require a gpu torch.\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [X] pytest tests/\n```\n============================================================================================================ test session starts ============================================================================================================\nplatform linux -- Python 3.10.13, pytest-7.4.2, pluggy-1.3.0\nrootdir: /home/ubuntu/llama-recipes\nplugins: mock-3.11.1\ncollected 8 items\n\ntests/test_finetuning.py ....                                                                                                                                                                                                         [ 50%]\ntests/test_train_utils.py .                                                                                                                                                                                                           [ 62%]\ntests/datasets/test_custom_dataset.py ..                                                                                                                                                                                              [ 87%]\ntests/datasets/test_samsum_datasets.py .                                                                                                                                                                                              [100%]\n\n============================================================================================================= warnings summary ==============================================================================================================\nsrc/llama_recipes/finetuning.py:5\n  /home/ubuntu/llama-recipes/src/llama_recipes/finetuning.py:5: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n    from pkg_resources import packaging\n\n../miniconda3/envs/llama/lib/python3.10/site-packages/bitsandbytes/cextension.py:34\n  /home/ubuntu/miniconda3/envs/llama/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n    warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n\n../miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/_shard/checkpoint/__init__.py:8\n  /home/ubuntu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/_shard/checkpoint/__init__.py:8: DeprecationWarning: torch.distributed._shard.checkpoint will be deprecated, use torch.distributed.checkpoint instead\n    warnings.warn(\n\ntests/datasets/test_samsum_datasets.py::test_custom_dataset\n  /home/ubuntu/miniconda3/envs/llama/lib/python3.10/site-packages/dill/_dill.py:412: PicklingWarning: Cannot locate reference to <class 'unittest.mock.MagicMock'>.\n    StockPickler.save(self, obj, save_persistent_id)\n\ntests/datasets/test_samsum_datasets.py::test_custom_dataset\n  /home/ubuntu/miniconda3/envs/llama/lib/python3.10/site-packages/dill/_dill.py:412: PicklingWarning: Cannot pickle <class 'unittest.mock.MagicMock'>: unittest.mock.MagicMock has recursive self-references that trigger a RecursionError.\n    StockPickler.save(self, obj, save_persistent_id)\n\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n====================================================================================================== 8 passed, 5 warnings in 20.95s =======================================================================================================\n```\n\n## Before submitting\n- [X] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [X] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "589": "Update links in README.md# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "562": "Make quickstart finetuning notebook ready for T4# What does this PR do?\nThis PR adapts the quickstart finetuning notebook to run in a T4 colab notebook.\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [X] Test A\n![image](https://github.com/meta-llama/llama-recipes/assets/13337103/ccb311d2-da0e-4bbb-925a-c05914bd52a2)\n\n\n## Before submitting\n- [X] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [X] Did you make sure to update the documentation with your changes?  \n\nThanks for contributing \ud83c\udf89!\n", "204": "adding system prompt to custom dataset# What does this PR do?\n\nSmall update to `examples/custom_dataset.py` to add an optional system prompt argument to `tokenize_dialog()`. As shown in https://huggingface.co/blog/llama2#how-to-prompt-llama-2 for example.\n\n## Feature/Issue validation/testing\n\nAdded an additional test `test_system_prompt()` in `tests/datasets/test_custom_dataset.py` to verify expected behavior.\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [x] Did you make sure to update the documentation with your changes?  \n- [x] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "576": "Add Langchain agent notebooks to 3P_IntegrationsOriginal commits authored by @jeffxtang \n\n", "953": "Fix deprecated langchain warning for llama3# What does this PR do?\nFixes # 894 (raised in https://github.com/meta-llama/llama-cookbook/issues/894)\nUpdating the imports & some linter issues in 3p-integrations/langchain/langgraph_rag_agent_local.ipynb  --> tested E2E and can view the langchain traces for the last 2 cells:\n\nhttps://smith.langchain.com/public/35e29916-70be-4146-8fc9-9787cb111ef5/r\nhttps://smith.langchain.com/public/8ec9370f-29e2-46b4-842c-8ab949f8a3fa/r\n", "748": "Fix minor grammatical errors# What does this PR do?\n\nFixes minor grammatical errors in the README file.\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "760": "1. fixed \"INPUT_FILE\" path   2. fix create_word_bounded_chunks before read in the text variables  3) processed_text not definedPatch 1# What does this PR do?\n1. fixed \"INPUT_FILE\" path   \n2. fix create_word_bounded_chunks was called before read in the \"text\" variables  \n3. processed_text not defined\n\n## Before submitting\n- [yes] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [yes] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ no ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [yes] Did you make sure to update the documentation with your changes?  \n- [no] Did you write any new necessary tests? - but I ran the code from scratch to make sure it is all good\n\nThanks for contributing \ud83c\udf89!\n", "833": "Update wordlist.txtAdded word exceptions for spell check", "67": "Templates updatesUpdates for GH issues, feature request, pull requests templates", "827": "Typo in prompt_format_utils.py# What does this PR do?\n\nFixes a typo in prompt_format_utils.py for inference.\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "9": "Update grammar_dataset_process.ipynb\" :\" and \" ;\" replaced to \":\" and \";\" respectively as opposed to \"!\" given initially in the list_replacement().", "628": "Enable users to trust remote code in samsum dataset# What does this PR do?\nCurrently the samsum dataset fails with a ValueError as we need to allow remote code execution in \nhttps://github.com/meta-llama/llama-recipes/blob/9b3dabcaac78980eae40005bbc8b1a8276c82af3/src/llama_recipes/datasets/samsum_dataset.py#L11\n\nThis PR adds an option to the dataset config to activate that flag without modifying any code.\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [X] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "614": "Update readme text to be version-agnostic# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "98": "fix some typos in downloading alpaca_dataset# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\nFixes some typos in downloading alpaca_datase.\n\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "370": "Make tests run on cpu only machines# What does this PR do?\nThis PR makes all tests run on a machine without GPU/XPU again.\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [X]  pytest tests/ -k \"not test_grammar_dataset\"\n```\n==================================================================================================== test session starts ====================================================================================================\nplatform linux -- Python 3.10.13, pytest-8.0.0, pluggy-1.3.0\nrootdir: /home/ubuntu/llama-recipes\nconfigfile: pyproject.toml\nplugins: mock-3.12.0\ncollected 29 items / 1 deselected / 28 selected\n\ntests/datasets/test_custom_dataset.py ..                                                                                                                                                                              [  7%]\ntests/datasets/test_samsum_datasets.py .                                                                                                                                                                              [ 10%]\ntests/test_batching.py ..                                                                                                                                                                                             [ 17%]\ntests/test_finetuning.py .....                                                                                                                                                                                        [ 35%]\ntests/test_finetuning_data_formatter.py ......                                                                                                                                                                        [ 57%]\ntests/test_sampler.py ..........                                                                                                                                                                                      [ 92%]\ntests/test_train_utils.py ..                                                                                                                                                                                          [100%]\n\n===================================================================================================== warnings summary ======================================================================================================\n../miniconda3/envs/llama-recipes/lib/python3.10/site-packages/transformers/utils/generic.py:441\n  /home/ubuntu/miniconda3/envs/llama-recipes/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n    _torch_pytree._register_pytree_node(\n\nsrc/llama_recipes/finetuning.py:5\n  /home/ubuntu/llama-recipes/src/llama_recipes/finetuning.py:5: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n    from pkg_resources import packaging\n\n../miniconda3/envs/llama-recipes/lib/python3.10/site-packages/transformers/utils/generic.py:309\n  /home/ubuntu/miniconda3/envs/llama-recipes/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n    _torch_pytree._register_pytree_node(\n\n../miniconda3/envs/llama-recipes/lib/python3.10/site-packages/torch/distributed/_shard/checkpoint/__init__.py:8\n  /home/ubuntu/miniconda3/envs/llama-recipes/lib/python3.10/site-packages/torch/distributed/_shard/checkpoint/__init__.py:8: DeprecationWarning: torch.distributed._shard.checkpoint will be deprecated, use torch.distributed.checkpoint instead\n    warnings.warn(\n\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n======================================================================================= 28 passed, 1 deselected, 4 warnings in 23.96s =======================================================================================\n```\n\n## Before submitting\n- [X] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [X ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [X] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [X] Did you make sure to update the documentation with your changes?  \n- [X] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "402": "Fix hsdp_device_mesh=None when enable HSDP and HYBRID_SHARD# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # A variable `hsdp_device_mesh` is assigned None before the same-name function `hsdp_device_mesh()` is called, when `hsdp` and `HYBRID_SHARD` are both set. Change the variable `hsdp_device_mesh` to `hsdp_device_mesh_plan` to avoid conflict.\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n\n- [x] Test 1\n`TypeError(\"'NoneType' object is not callable\")` fixed.\n\n- [x] An error from `torch/distributed/fsdp/_init_utils.py` L107 will be triggered.\n```\n            \"Cannot pass both process_group and device_mesh at the \"\n            \"same time. Please just pass only one of them.\"\n```\nAs neither `process_group` or `device_mesh` is None. With a temporary modification, the training will proceed (with both HSDP and HYBRID_SHARD enabled)\n\n<img width=\"1738\" alt=\"\u56fe\u7247\" src=\"https://github.com/meta-llama/llama-recipes/assets/121946073/1ac7947c-b761-416e-afa5-a99fb7050aa1\">\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [x] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "365": "Codellama 70b instruct code example# What does this PR do?\nAdding code example for prompting codellama 70B instruct. model\n\n- [ ] Test A\n[Logs for e2e test](https://gist.github.com/HamidShojanazeri/f304527212cc52cb93846126698353dd)\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ x] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ x] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "403": "[WIP/Do Not Review]Chatbot Recipe## Main Goal\n\nBuilding an e2e recipe for building chatbots.\n\n### High level idea\n\nWe want to focus on following stages:\n\n- Data pipelines for creating datasets for chatbots\n- Data processing/ quality assurance practices/pipelines/tooling\n- Evaluation process\n- Fine-tuning a model/ Best practices for fine-tuning/ LORA/QLORA/ hyper params.\n\n### Use-case\n- Llama FAQ model using OSS llama docs, github docs, papers, website, etc.\n- Proposed data pipeline, using Llama 70b or 13b as the teacher model to create Q&A pairs from Llama docs as mentioned above.[ Open to any other ideas here]\n- Data Quality/ Eval using same teacher model [ Open to any other ideas here]\n", "417": "Update location and name of llm.py example notebookUpdate location and name of llm.py example notebook\n\n# What does this PR do?\n\nThis notebook demonstrates how to use the llm.py class. It needs to move under the recipes directory to align with the other example notebooks. I also took the opportunity to give it a more logical name. I discussed this change in advance with @HamidShojanazeri \n\n## Feature/Issue validation/testing\n\nn/a\n", "371": "Update README.md adding setuptools installation for Mac# What does this PR do?\n\nInclude a pip install setuptools instruction for MacOS environments\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "601": "Move MediaGen notebook to octoai folder# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "615": "[Azure] Update Azure API usage example to 3.1# What does this PR do?\nUpdated the endpoint to 3.1 support. Also updated Langchain and Gradio support as their framework updated.\n\n\n## Before submitting\n- [X] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [X] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [X] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "629": "Use new get_model_state_dict api for save_pretrained peft model# What does this PR do?\nThis PR updated the way we save the checkpoint in peft by using the new DCP api to avoid OOM https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html\n\nFixes # (issue)\n#626 \n\n## Feature/Issue validation/testing\n\nLora fine tune a model and load for inference\n- [X] Test A\n`CUDA_VISIBLE_DEVICES=0,1,4,5 torchrun --nnodes 1 --nproc_per_node 4  recipes/quickstart/finetuning/finetuning.py --enable_fsdp --model_name meta-llama/Meta-Llama-3.1-70B-Instruct --use_peft --peft_method lora --output_dir ../llama_output/ --run_validation --save_model --samsum_dataset.trust_remote_code=True --context_length 2048 --max_train_step 1 --max_eval_step 1\ncd recipes/quickstart/inference/local_inference\ncat samsum_prompt.txt | python inference.py --model_name meta-llama/Meta-Llama-3.1-70B-Instruct --peft_model ~/llama_output/\n`\nLogs for Test A training\n```\nTraining Epoch: 1:   0%|                                                                                                                                                                                                                                                                                                                           | 0/79 [00:00<?, ?it/s]/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                                                                                                                                                                                                                                                                                           | 0/79 [00:00<?, ?it/s]--> applying fsdp activation checkpointing...\n--> Training Set Length = 14732\n--> Validation Set Length = 818\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:04<00:00, 3515.63it/s]\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 818/818 [00:00<00:00, 3396.24it/s]\n--> Num of Validation Set Batches loaded = 17\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                                                                                                                                                                                                                                                                                           | 0/79 [00:00<?, ?it/s]NCCL version 2.20.5+cuda12.4\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\nTraining Epoch: 1/3, step 0/79 completed (loss: 1.2386412620544434):   1%|\u2588\u2588\u2588\u258e                                                                                                                                                                                                                                                             | 1/79 [00:27<36:09, 27.81s/it]max training steps reached, stopping training, total train steps finished:  1\nTraining Epoch: 1/3, step 0/79 completed (loss: 1.2386412620544434):   1%|\u2588\u2588\u2588\u258e                                                                                                                                                                                                                                                             | 1/79 [00:27<36:16, 27.91s/it]\nTraining Epoch: 1/3, step 0/79 completed (loss: 1.3376933336257935):   1%|\u2588\u2588\u2588\u258e                                                                                                                                                                                                                                                             | 1/79 [00:37<49:19, 37.95s/it]\nTraining Epoch: 1/3, step 0/79 completed (loss: 1.4895133972167969):   1%|\u2588\u2588\u2588\u258e                                                                                                                                                                                                                                                             | 1/79 [00:37<49:12, 37.85s/it]\nTraining Epoch: 1/3, step 0/79 completed (loss: 1.4058910608291626):   1%|\u2588\u2588\u2588\u258e                                                                                                                                                                                                                                                             | 1/79 [00:40<52:39, 40.51s/it]\nMax CUDA memory allocated was 62 GB\nMax CUDA memory reserved was 67 GB\nPeak active CUDA memory was 62 GB\nCUDA Malloc retries : 0\nCPU Total Peak Memory consumed during the train (max): 7 GB\nevaluating Epoch:   6%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                                                                                                                                                                                                                                  | 1/17 [00:02<00:33,  2.10s/it]max eval steps reached, stopping evaluation, total_eval_steps:  1\nevaluating Epoch:   6%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                                                                                                                                                                                                                                  | 1/17 [00:02<00:35,  2.25s/it]\nevaluating Epoch:   6%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                                                                                                                                                                                                                                  | 1/17 [00:02<00:35,  2.24s/it]\nevaluating Epoch:   6%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                                                                                                                                                                                                                                  | 1/17 [00:02<00:35,  2.23s/it]\nevaluating Epoch:   6%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                                                                                                                                                                                                                                  | 1/17 [00:02<00:36,  2.31s/it]\n eval_ppl=tensor(1.0827, device='cuda:0') eval_epoch_loss=tensor(0.0795, device='cuda:0')\nwe are about to save the PEFT modules\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n  warnings.warn(\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n  warnings.warn(\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n  warnings.warn(\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n  warnings.warn(\nPEFT modules are saved in ../llama_output/ directory\nbest eval loss on epoch 1 is 0.07948913425207138\nEpoch 1: train_perplexity=1.0175, train_epoch_loss=0.0173, epoch time 29.114889188029338s\nKey: avg_train_prep, Value: 1.017466425895691\nKey: avg_train_loss, Value: 0.017315631732344627\nKey: avg_eval_prep, Value: 1.0827337503433228\nKey: avg_eval_loss, Value: 0.07948913425207138\nKey: avg_epoch_time, Value: 29.114889188029338\nKey: avg_checkpoint_time, Value: 192.18921023100847\n```\nLogs for Test A inference\n```\n---\n\nSummary:\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nthe inference time is 24057.521794049535 ms\nUser input and model output deemed safe.\nModel output:\nSummarize this dialog:\n\nA: Hi Tom, are you busy tomorrow\u2019s afternoon?\n\nB: I\u2019m pretty sure I am. What\u2019s up?\n\nA: Can you go with me to the animal shelter?.\n\nB: What do you want to do?\n\nA: I want to get a puppy for my son.\n\nB: That will make him so happy.\n\nA: Yeah, we\u2019ve discussed it many times. I think he\u2019s ready now.\n\nB: That\u2019s good. Raising a dog is a tough issue. Like having a baby ;-)\n\nA: I'll get him one of those little dogs.\n\nB: One that won't grow up too big;-)\n\nA: And eat too much;-))\n\nB: Do you know which one he would like?\n\nA: Oh, yes, I took him there last Monday. He showed me one that he really liked.\n\nB: I bet you had to drag him away.\n\nA: He wanted to take it home right away ;-).\n\nB: I wonder what he'll name it.\n\nA: He said he\u2019d name it after his dead hamster \u2013 Lemmy  - he's  a great Motorhead fan :-)))\n\n---\n\nSummary: Tom's friend wants to go to an animal shelter tomorrow to pick out a puppy and asked Tom to accompany him. The friend's son has been wanting a puppy for a while and the friend thinks he's ready for the responsibilities. Tom agrees to go and teases his friend about the upcoming challenges of dog ownership. The friend has already taken his son to the shelter and the son has found a puppy he wants, and plans to name it after a deceased pet.\n\n[This appears to be only a\n```\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [X] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [X] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "8": "Update inference.mdMinor update to inference doc to add vLLM_nference.py and TGI examples.", "832": "Refactor Llama-RecipesThis restructures and brings out the nested folders, giving a root structure of:\n- 3p-integrations\n- end-to-end-use-cases\n- getting-started\n- src\n\nThe UPDATES markdown file has a change log of all the details and the `archive-main` branch is a snapshot of the recipes repo before the changes. ", "775": "Update samsum_dataset.py# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\nadd trust_remote_code=True to correctly load samsum dataset\n\n<!-- Remove if not applicable -->\n\nFixes error stating that samsum dataset contains custom code that needs to be trusted in order to properly load the data.\n\n\n", "761": "Fix NotebookLlama step 4 dependencies# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFix NotebookLlama demo\n- remove pkg warning from requirements, since we can directly import warnings in python 3\n- add imports for wavfile, AudioSegment and io to Step-4-TTS-Workflow.ipynb\n- add dependencies for pydub and parler-tts required from Step-4-TTS-Workflow.ipynb\n- downgrade transformers version to 4.43.0, to be compatible with parler-tts's dependency\n\n## Feature/Issue validation/testing\n\nnot applicable\n\n", "211": "Invalidate labels in dialog dataset to disable loss# What does this PR do?\nThis PR disables loss computation on chat prompts from user following: https://huggingface.co/blog/starchat-alpha#masking-user-labels\n\nFixes # (issue)\n#209 \n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [X] pytest tests\n```\n============================================================================================================ test session starts ============================================================================================================\nplatform linux -- Python 3.10.12, pytest-7.4.2, pluggy-1.3.0\nrootdir: /home/mreso/llama-recipes\nplugins: mock-3.11.1\ncollected 8 items\n\ntests/test_finetuning.py ....                                                                                                                                                                                                         [ 50%]\ntests/test_train_utils.py .                                                                                                                                                                                                           [ 62%]\ntests/datasets/test_custom_dataset.py ..                                                                                                                                                                                              [ 87%]\ntests/datasets/test_samsum_datasets.py .                                                                                                                                                                                              [100%]\n\n============================================================================================================= warnings summary ==============================================================================================================\nsrc/llama_recipes/finetuning.py:5\n  /home/mreso/llama-recipes/src/llama_recipes/finetuning.py:5: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n    from pkg_resources import packaging\n\n../local/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/_shard/checkpoint/__init__.py:8\n  /home/mreso/local/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/_shard/checkpoint/__init__.py:8: DeprecationWarning: torch.distributed._shard.checkpoint will be deprecated, use torch.distributed.checkpoint instead\n    warnings.warn(\n\ntests/test_train_utils.py::test_gradient_accumulation\n  /home/mreso/local/miniconda3/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n    warnings.warn(\n\ntests/datasets/test_samsum_datasets.py::test_custom_dataset\n  /home/mreso/local/miniconda3/envs/llama/lib/python3.10/site-packages/dill/_dill.py:412: PicklingWarning: Cannot locate reference to <class 'unittest.mock.MagicMock'>.\n    StockPickler.save(self, obj, save_persistent_id)\n\ntests/datasets/test_samsum_datasets.py::test_custom_dataset\n  /home/mreso/local/miniconda3/envs/llama/lib/python3.10/site-packages/dill/_dill.py:412: PicklingWarning: Cannot pickle <class 'unittest.mock.MagicMock'>: unittest.mock.MagicMock has recursive self-references that trigger a RecursionError.\n    StockPickler.save(self, obj, save_persistent_id)\n\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n====================================================================================================== 8 passed, 5 warnings in 30.32s =======================================================================================================\n```\n\n\n## Before submitting\n- [X] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [X] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "577": "Updates to benchmarks codeOriginal commits by @WuhanMonkey ", "563": "None# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "588": "fix typo# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "585": "Add experimental folder to README# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [x ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "591": "fix llama3 cookbook with llamaindex and groq# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [x ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "220": "Feature/plotting# What does this PR do?\nAdding support to save fine tuning metrics to json. Adding script to plot saved metrics.\nTracks loss and perplexity, both at the step and epoch level and saves them to a json file. Tracks both train and validation metrics, for comparison after finetuning.\n\nThis requires matplotlib to be added as a requirement\n\n## Feature/Issue validation/testing\n\n- [x] Run finetuning on a single GPU with the default dataset to verify everything is working correctly\n\n- [x] Run finetuning on multiple GPUs with default dataset to verify it continues working\n\n1. Running tests:\n![image](https://github.com/facebookresearch/llama-recipes/assets/222731/9870a984-92b6-40ec-9c29-baa9b7fc6717)\n\n2. Command on a fully configured environment: \n```\ntorchrun --nnodes 1 --nproc_per_node 4  examples/finetuning.py --enable_fsdp --use_peft --peft_method lora --mode\nl_name ../Llama-2-7b-chat-hf/ --pure_bf16 --output_dir ../Llama-2-7b-peft-samsum-10 --batch_size_training 2 --gradient_accumulation_steps 2 --num_epochs 15 --save_metrics\n```\n\n3. Results:\n![metrics_data_3-2023-09-22_03-11-30_train_and_validation_loss](https://github.com/facebookresearch/llama-recipes/assets/222731/4dd16610-f173-48ee-bf61-09f9589e3baf)\n![metrics_data_3-2023-09-22_03-11-30_train_and_validation_perplexity](https://github.com/facebookresearch/llama-recipes/assets/222731/40af8396-ee77-4535-808c-bccbaf1c31bf)\n\nA test was added to verify that when the save_metrics is used in train_config, the file gets generated.\n", "546": "replace groq llama 2 with replicate# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "552": "Add groq cookbook entries using Llama3# What does this PR do?\n\n<!--\nThis PR adds several recipes for using Llama3 with Groq via cookbook posts. These posts cover use cases such as RAG, Feature Extraction with JSON Mode and Function Calling.\n\n-->\n\n\n## Feature/Issue validation/testing\n\nThese cookbooks can be run as jupyter notebooks provided that `GROQ_API_KEY` is set as an environment variable (Groq API Keys can be curated here: https://console.groq.com/keys\n\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "787": "fix walkthrough.ipynb render# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "793": "fix typo in auto wrap policyI think this might be a typo; there is no reason to write MllamaSelfAttnDecoderLayer twice. Let me know if it was intentional.\n\n# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nThis fixes a typo as mentioned above.\n\n## Feature/Issue validation/testing\n\n\n\nI tested with my local training script.\n\nThis is a minor change; I didn't write new tests.\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "744": "Append epoch rather than best val. loss to val_loss# Problem\nCurrently, we're val_loss.append(best_val_loss) in each epoch. This is misleading because we're appending the corresponding epoch (not best across epochs) quantities in train_loss, train_prep, and val_prep. This is also inconvenient, as one often would like to plot both train and validation losses as a function of the epochs to look for overfitting.\n\n# Solution\nval_loss.append(eval_epoch_loss)\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ X ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "750": "add support for ingesting content from websites, audio files, YouTube, etc.This update include adding support for multiple input formats, improving documentation, and updating dependencies.\n\n### Major Enhancements:\n\n#### Support for Multiple Input Formats:\n1. **Ingestion Module**: Added an ingestion module (`ingestion.py`) that supports PDF, website, YouTube, and audio file formats. This module includes classes like `PDFIngestor`, `WebsiteIngestor`, `YouTubeIngestor`, and `AudioIngestor` to handle different types of input sources.\n2. **Usage Examples**: Updated `README.md` with detailed usage examples for ingesting content from various sources, including code snippets for each input type.\n\n#### Documentation Improvements:\n1. **Quickstart Guide**: Enhanced the quickstart guide in `recipes/quickstart/NotebookLlama/README.md` with step-by-step instructions, updated terminology, and added links to relevant resources.\n\n#### Dependency Updates:\n1. **Requirements**: Updated `requirements.txt` to include new dependencies like `langchain`, `openai-whisper`, and `youtube-dl`, and updated versions for existing dependencies.\n\n### Detailed Changes:\n\n#### Ingestion Module:\n* [`ingestion.py`](diffhunk://#diff-9d033d9dd26822042c7c18555b4e074741d4a0ad9ed36656ecbbfca3dd4040a2R1-R224): Added support for multiple input formats with classes for PDF, website, YouTube, and audio ingestion.\n\n#### Documentation:\n* [`README.md`](diffhunk://#diff-b335630551682c19a781afebcf4d07bf978fb1f8ac04c6bf87428ed5106870f5R183-R321): Added sections for supported input formats and usage examples.\n* [`recipes/quickstart/NotebookLlama/README.md`](diffhunk://#diff-da91333d653b44bc80cb8e064fcb49fc5747068923863ff929a64755bee935fdL11-R105): Improved the quickstart guide with detailed steps and updated terminology.\n\n#### Dependencies:\n* [`requirements.txt`](diffhunk://#diff-30d2d84cc55d86dc4c1dab93b80e5b3f3c7f747e6bcd071bcd3020c4a70562d9L2-R16): Updated and added new dependencies to support the new ingestion functionalities.", "778": "default to trusted_code for Samsum dataset# What does this PR do?\nupdates load dataset params to default to trust remote code.  This is blocking AWS from using the dataset. \nThis is really a dummy PR to just ensure the branch AWS is using has the update.  This resolves their issue and closing this PR. ", "57": "adding issue tempaltePR to add an issue tempelate.", "803": "fix links# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "630": "SFT attempt", "94": "Fix filename typo# What does this PR do?\nFixes typo in filename of .md file; changed \"docs/mutli_gpu.md\" to \"docs/multi_gpu.md\" and updated link in README.md accordingly\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "618": "[Recipe] Example featuring built-in tool calling capabilities - Wolfram Alpha, Interpreter, Brave Search# What does this PR do?\n\nMeta's latest Llama3.1 models offer unique function calling capabilities. In particular they offer built-in tool calling capabilities for the following 3 external tools:\n* Brave Search: internet search\n* Code Interpreter: Python code interpreter\n* Wolfram Alpha: mathematical and scientific knowledge tool\n\nTo sell the benefits of the built in tools, let's look at what one would get back from an LLM with or without tool calling capabilities. In particular:\n\n### Code Interpreter\nUser Query: `I just got a 25 year mortgage of 400k at a fixed rate of 5.14% and paid 20% down. How much will I pay in interest?`\n* Answer without tool calls (wrong): `Total paid interest: $184,471`\n* Answer with tool calls (correct): `you will pay a total of $249,064.70 in interest`\n\n### Brave Search\nUser Query: `What caused a wordlwide outage in the airlines industry in July of 2024?`\n* Answer without tool calls: `I'm not aware of anything that would have caused a worldwide outage in the airlines industry.`\n* Answer with tool calls: `The global technology outage was caused by a faulty software update that affected Windows programs running cybersecurity technology from CrowdStrike. The outage disrupted flights, media outlets, hospitals, small businesses, and government offices, highlighting the vulnerability of the world's interconnected systems.`\n\n### Wolfram Alpha\nUser Query: `Derive the prime factorization of 892041`\n* Answer without tool calls (wrong): `The prime factorization of 892041 is:\\n\\n2 \u00d7 2 \u00d7 2 \u00d7 3 \u00d7 3 \u00d7 3 \u00d7 5 \u00d7 13 \u00d7 17 \u00d7 17`\n* Answer with tool calls (correct): `The prime factorization of 892041 is 3 \u00d7 17 \u00d7 17491.`\n\nThis PR introduces an example powered by OctoAI that showcases Llama3.1's tool calling capabilities.\n\n## Feature/Issue validation/testing\n\nNotebook was run locally and in colab.\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [x] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "397": "Fix dead links after directory structure refactor# What does this PR do?\n\n* Fixes dead links  arising from #394\n* replaces web URLs with relative paths", "354": "Add option to enable Llamaguard content safety check in chat_completion# What does this PR do?\nThis update introduces the enable_llamaguard_content_safety boolean option in the chat_completion.py module.\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [x] run python examples/chat_completion/chat_completion.py --model_name Llama-2-7b-chat-hf  --prompt_file examples/chat_completion/chats.json  --quantization\n\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "432": "Changes for aegis fine-tuning# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "369": "Introducing GH Actions workflow to run llama-recipes PyTest tests# What does this PR do?\n\nIntroduction of GitHub Actions CI/CD workflow to run PyTest tests on generic Ubuntu 20.04 GHA hosted Runners.\n\n## Feature/Issue validation/testing\n\nWorkflow is configured to be triggered on each PR submitted to be merged with *main* branch when following paths are affected:\n\n- src/llama-recipes/configs/*.py\n- src/llama-recipes/utils/*.py\n- src/llama-recipes/datasets/*.py\n- src/llama-recipes/data/*.py\n- src/llama-recipes/*.py\n\nHere is the most recent PyTest test run executed on llama-recipes fork:  https://github.com/maximgroshev/llama-recipes/actions/runs/7906759630/job/21582327220\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "427": "fix: assigned correct path to custom dataset example file# What does this PR do?\n\n- Assigns right path to example custom dataset function", "355": "Fix broken format in preview for RAG chatbot example# What does this PR do?\n\nFix formatting issue in the RAG chatbot example. As shown below\n\n![image](https://github.com/facebookresearch/llama-recipes/assets/5740295/1316b150-e6be-4ef6-85bf-d07085ea9a29)\n\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [X] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [X] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "433": "Added a feature that allow users to use pytorch profiler or flop_counter to measure the performance during fine-tuning.# What does this PR do?\n\nAdded a feature that allow users to use pytorch profiler or flop_counter to measure the performance during fine-tuning. For pytorch profiler, use --use_profiler to turn on pytorch profiler, the default profiling schedule is  \"wait_step, warmup_step, active_step = 1, 2, 3\" , use --profiler_dir SOME_PATH to save the profiler output into the directory. For flop counter, use --flop_counter to turn on flop counter which will only measure the flop of 1 step and print the table. The default step that will be counted is 3, use --flop_counter_startpoint to change the startpoint, but please allows some warmup steps before counting.  --use_profiler and --flop_counter **can not** be turned on the same time to ensure the measurement is accurate. Please make sure the --max_train_steps is long enough to allow the measurement activities. \n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [x] Test only --use_profiler option \nLogs:\n```\n~/work/llama-recipes (feature/flop_counter)]$ torchrun --nnodes 1 --nproc_per_node 2  recipes/finetuning/finetuning.py --use_profiler --profiler_dir /home/kaiwu/work/finetune-output --max_train_step 8 --max_eval_step 2 --use_peft --peft_method lora  --model_name /home/kaiwu/work/llama2-7b --enable_fsdp --output_dir /home/kaiwu/work/finetune-output\n[2024-04-29 16:31:12,562] torch.distributed.run: [WARNING] \n[2024-04-29 16:31:12,562] torch.distributed.run: [WARNING] *****************************************\n[2024-04-29 16:31:12,562] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n[2024-04-29 16:31:12,562] torch.distributed.run: [WARNING] *****************************************\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/utils/generic.py:485: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n  _torch_pytree._register_pytree_node(\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/utils/generic.py:485: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n  _torch_pytree._register_pytree_node(\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/utils/generic.py:342: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n  _torch_pytree._register_pytree_node(\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/utils/generic.py:342: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n  _torch_pytree._register_pytree_node(\nClearing GPU cache for all ranks\n--> Running with torch dist debug set to detail\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:07<00:00,  3.58s/it]\n--> Model /home/kaiwu/work/llama2-7b\n\n--> /home/kaiwu/work/llama2-7b has 6738.415616 Million params\n\ntrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\nbFloat16 enabled for mixed precision - using bfSixteen policy\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:06<00:00,  3.48s/it]\ntrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n--> applying fsdp activation checkpointing...\n--> applying fsdp activation checkpointing...\nReusing dataset samsum (/home/kaiwu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\nParameter 'function'=<function get_preprocessed_samsum.<locals>.apply_prompt_template at 0x7f9dec107250> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\nLoading cached processed dataset at /home/kaiwu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-1c80317fa3b1799d.arrow\nLoading cached processed dataset at /home/kaiwu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-bdd640fb06671ad1.arrow\nReusing dataset samsum (/home/kaiwu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\nParameter 'function'=<function get_preprocessed_samsum.<locals>.apply_prompt_template at 0x7f941400b250> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\nLoading cached processed dataset at /home/kaiwu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-1c80317fa3b1799d.arrow\nLoading cached processed dataset at /home/kaiwu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-bdd640fb06671ad1.arrow\n--> Training Set Length = 14732\nReusing dataset samsum (/home/kaiwu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\nLoading cached processed dataset at /home/kaiwu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-3eb13b9046685257.arrow\nLoading cached processed dataset at /home/kaiwu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-23b8c1e9392456de.arrow\nPreprocessing dataset:  12%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                                                                                                                                                                           | 1817/14732 [00:00<00:02, 5967.38it/s]Reusing dataset samsum (/home/kaiwu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\nLoading cached processed dataset at /home/kaiwu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-3eb13b9046685257.arrow\nLoading cached processed dataset at /home/kaiwu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-23b8c1e9392456de.arrow\n--> Validation Set Length = 818\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:02<00:00, 5900.94it/s]\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 818/818 [00:00<00:00, 5699.45it/s]\nPreprocessing dataset:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 14470/14732 [00:02<00:00, 5942.71it/s]/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                                                                                                                                                                                                          | 0/96 [00:00<?, ?it/s]pytorch profiling is activated and results will be saved in /home/kaiwu/work/finetune-output\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:02<00:00, 5869.94it/s]\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 818/818 [00:00<00:00, 5462.13it/s]\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                                                                                                                                                                                                          | 0/96 [00:00<?, ?it/s]pytorch profiling is activated and results will be saved in /home/kaiwu/work/finetune-output\nNCCL version 2.19.3+cuda12.1\nTraining Epoch: 1/3, step 1/96 completed (loss: 1.3550630807876587):   3%|\u2588\u2588\u2588\u2588\u2588\u258c                                                                                                                                                                          | 3/96 [00:15<07:11,  4.64s/it]STAGE:2024-04-29 16:31:45 648753:648753 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\nTraining Epoch: 1/3, step 1/96 completed (loss: 1.487619400024414):   3%|\u2588\u2588\u2588\u2588\u2588\u258c                                                                                                                                                                           | 3/96 [00:14<07:03,  4.55s/it]STAGE:2024-04-29 16:31:45 648752:648752 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\nTraining Epoch: 1/3, step 4/96 completed (loss: 1.346050500869751):   6%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                                                                                                      | 6/96 [00:25<05:51,  3.90s/it]STAGE:2024-04-29 16:31:57 648753:648753 ActivityProfilerController.cpp:320] Completed Stage: Collection\nSTAGE:2024-04-29 16:31:57 648752:648752 ActivityProfilerController.cpp:320] Completed Stage: Collection\nSTAGE:2024-04-29 16:31:57 648753:648753 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\nSTAGE:2024-04-29 16:31:57 648752:648752 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\nTraining Epoch: 1/3, step 7/96 completed (loss: 1.2207212448120117):   8%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                                                                                                                                 | 8/96 [00:48<10:36,  7.23s/it]max training steps reached, stopping training, total train steps finished:  8\nTraining Epoch: 1/3, step 7/96 completed (loss: 1.2207212448120117):   8%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                                                                                                                                 | 8/96 [00:48<08:57,  6.11s/it]\nTraining Epoch: 1/3, step 7/96 completed (loss: 1.1940990686416626):   8%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                                                                                                                                 | 8/96 [00:49<09:02,  6.16s/it]\nMax CUDA memory allocated was 23 GB\nMax CUDA memory reserved was 27 GB\nPeak active CUDA memory was 24 GB\nCUDA Malloc retries : 0\nCPU Total Peak Memory consumed during the train (max): 10 GB\nevaluating Epoch:  10%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                                                                                                                                                                                             | 2/21 [00:00<00:07,  2.60it/s]max eval steps reached, stopping evaluation, total_eval_steps:  2\nevaluating Epoch:  10%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                                                                                                                                                                                             | 2/21 [00:00<00:08,  2.21it/s]\nevaluating Epoch:  10%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                                                                                                                                                                                             | 2/21 [00:00<00:08,  2.31it/s]\n eval_ppl=tensor(1.1119, device='cuda:0') eval_epoch_loss=tensor(0.1061, device='cuda:0')\nwe are about to save the PEFT modules\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /home/kaiwu/work/llama2-7b - will assume that the vocabulary was not modified.\n  warnings.warn(\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /home/kaiwu/work/llama2-7b - will assume that the vocabulary was not modified.\n  warnings.warn(\nPEFT modules are saved in /home/kaiwu/work/finetune-output directory\nbest eval loss on epoch 1 is 0.10606812685728073\nEpoch 1: train_perplexity=1.1164, train_epoch_loss=0.1101, epoch time 50.1332068501506s\nKey: avg_train_prep, Value: 1.1164158582687378\nKey: avg_train_loss, Value: 0.11012342572212219\nKey: avg_eval_prep, Value: 1.1118977069854736\nKey: avg_eval_loss, Value: 0.10606812685728073\nKey: avg_epoch_time, Value: 50.1332068501506\nKey: avg_checkpoint_time, Value: 0.2338113421574235\n```\n- [x] Test only --flop_counter option\nLogs:\n```\n ~/work/llama-recipes (feature/flop_counter)]$ torchrun --nnodes 1 --nproc_per_node 2  recipes/finetuning/finetuning.py --flop_counter --flop_counter_start 4 --max_train_step 6 --max_eval_step 2 --use_peft --peft_method lora  --model_name /home/kaiwu/work/llama2-7b --enable_fsdp --output_dir /home/kaiwu/work/finetune-output\n[2024-04-29 16:27:28,475] torch.distributed.run: [WARNING] \n[2024-04-29 16:27:28,475] torch.distributed.run: [WARNING] *****************************************\n[2024-04-29 16:27:28,475] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n[2024-04-29 16:27:28,475] torch.distributed.run: [WARNING] *****************************************\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/utils/generic.py:485: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n  _torch_pytree._register_pytree_node(\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/utils/generic.py:485: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n  _torch_pytree._register_pytree_node(\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/utils/generic.py:342: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n  _torch_pytree._register_pytree_node(\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/utils/generic.py:342: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n  _torch_pytree._register_pytree_node(\nClearing GPU cache for all ranks\n--> Running with torch dist debug set to detail\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:07<00:00,  3.62s/it]\n--> Model /home/kaiwu/work/llama2-7b\n\n--> /home/kaiwu/work/llama2-7b has 6738.415616 Million params\n\ntrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\nbFloat16 enabled for mixed precision - using bfSixteen policy\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:07<00:00,  3.65s/it]\ntrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n--> applying fsdp activation checkpointing...\nReusing dataset samsum (/home/kaiwu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\nParameter 'function'=<function get_preprocessed_samsum.<locals>.apply_prompt_template at 0x7f084c903250> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\nLoading cached processed dataset at /home/kaiwu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-1c80317fa3b1799d.arrow\nLoading cached processed dataset at /home/kaiwu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-bdd640fb06671ad1.arrow\n--> Training Set Length = 14732\nReusing dataset samsum (/home/kaiwu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\nLoading cached processed dataset at /home/kaiwu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-3eb13b9046685257.arrow\nLoading cached processed dataset at /home/kaiwu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-23b8c1e9392456de.arrow\n--> Validation Set Length = 818\nPreprocessing dataset:  16%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                                                                                                                                                   | 2387/14732 [00:00<00:02, 5801.65it/s]--> applying fsdp activation checkpointing...\nPreprocessing dataset:  41%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                                                                                                              | 6026/14732 [00:01<00:01, 5932.50it/s]Reusing dataset samsum (/home/kaiwu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\nParameter 'function'=<function get_preprocessed_samsum.<locals>.apply_prompt_template at 0x7f4294107250> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\nLoading cached processed dataset at /home/kaiwu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-1c80317fa3b1799d.arrow\nLoading cached processed dataset at /home/kaiwu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-bdd640fb06671ad1.arrow\nPreprocessing dataset:  57%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                                                           | 8467/14732 [00:01<00:01, 6030.72it/s]Reusing dataset samsum (/home/kaiwu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\nLoading cached processed dataset at /home/kaiwu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-3eb13b9046685257.arrow\nLoading cached processed dataset at /home/kaiwu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-23b8c1e9392456de.arrow\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:02<00:00, 5815.79it/s]\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 818/818 [00:00<00:00, 5647.75it/s]\nPreprocessing dataset:  54%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                                                                                  | 7948/14732 [00:01<00:01, 6142.03it/s]/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nPreprocessing dataset:  58%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                                                                         | 8563/14732 [00:01<00:01, 6113.39it/s]NCCL version 2.19.3+cuda12.1\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:02<00:00, 5933.88it/s]\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 818/818 [00:00<00:00, 5699.45it/s]\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1/3, step 5/96 completed (loss: 1.2147760391235352):   6%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                                                                                                     | 6/96 [00:26<05:51,  3.90s/it]max training steps reached, stopping training, total train steps finished:  6\nTraining Epoch: 1/3, step 5/96 completed (loss: 1.2147760391235352):   6%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                                                                                                     | 6/96 [00:26<06:32,  4.36s/it]\nTotal time used in this flop counting step is: 3.6876885890960693\nThe total TFlop per second is: 204.77765344817058\nThe tflop_count table is below:\nModule                                                        FLOP    % Total\n--------------------------------------------------------  --------  ---------\nGlobal                                                    755.156T    100.00%\n - aten.mm                                                596.827T     79.03%\n - aten.bmm                                                 0.000T      0.00%\n - aten._scaled_dot_product_efficient_attention            70.369T      9.32%\n - aten._scaled_dot_product_efficient_attention_backward   87.961T     11.65%\nTraining Epoch: 1/3, step 5/96 completed (loss: 1.2408324480056763):   6%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                                                                                                     | 6/96 [00:24<06:10,  4.11s/it]\nMax CUDA memory allocated was 23 GB\nMax CUDA memory reserved was 27 GB\nPeak active CUDA memory was 24 GB\nCUDA Malloc retries : 0\nCPU Total Peak Memory consumed during the train (max): 8 GB\nevaluating Epoch:  10%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                                                                                                                                                                                             | 2/21 [00:00<00:06,  3.01it/s]max eval steps reached, stopping evaluation, total_eval_steps:  2\nevaluating Epoch:  10%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                                                                                                                                                                                             | 2/21 [00:00<00:07,  2.62it/s]\nevaluating Epoch:  10%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                                                                                                                                                                                             | 2/21 [00:00<00:07,  2.56it/s]\n eval_ppl=tensor(1.1157, device='cuda:0') eval_epoch_loss=tensor(0.1095, device='cuda:0')\nwe are about to save the PEFT modules\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /home/kaiwu/work/llama2-7b - will assume that the vocabulary was not modified.\n  warnings.warn(\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /home/kaiwu/work/llama2-7b - will assume that the vocabulary was not modified.\n  warnings.warn(\nPEFT modules are saved in /home/kaiwu/work/finetune-output directory\nbest eval loss on epoch 1 is 0.10951244831085205\nEpoch 1: train_perplexity=1.0888, train_epoch_loss=0.0851, epoch time 26.664336079964414s\nKey: avg_train_prep, Value: 1.0888429880142212\nKey: avg_train_loss, Value: 0.0851157158613205\nKey: avg_eval_prep, Value: 1.1157338619232178\nKey: avg_eval_loss, Value: 0.10951244831085205\nKey: avg_epoch_time, Value: 26.664336079964414\nKey: avg_checkpoint_time, Value: 0.22542023286223412\nKey: model_tflops, Value: 204.77765344817058\n```\n\n- [x] Test both --use_profiler and --flop_counter on(which should give error)\nLogs:\n```\n~/work/llama-recipes (feature/flop_counter)]$ torchrun --nnodes 1 --nproc_per_node 2  recipes/finetuning/finetuning.py --use_profiler --profiler_dir /home/kaiwu/work/finetune-output --max_train_step 8 --max_eval_step 2 --use_peft --peft_method lora  --model_name /home/kaiwu/work/llama2-7b --enable_fsdp --use_fast_kernels --output_dir /home/kaiwu/work/finetune-output --flop_counter\nW0415 13:31:26.286000 140564662125056 torch/distributed/run.py:757]\nW0415 13:31:26.286000 140564662125056 torch/distributed/run.py:757] *****************************************\nW0415 13:31:26.286000 140564662125056 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.\nW0415 13:31:26.286000 140564662125056 torch/distributed/run.py:757] *****************************************\nClearing GPU cache for all ranks\n--> Running with torch dist debug set to detail\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:06<00:00,  3.27s/it]\n--> Model /home/kaiwu/work/llama2-7b\n\n--> /home/kaiwu/work/llama2-7b has 6738.415616 Million params\n\ntrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\nbFloat16 enabled for mixed precision - using bfSixteen policy\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:06<00:00,  3.28s/it]\ntrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n--> applying fsdp activation checkpointing...\n--> applying fsdp activation checkpointing...\n--> Training Set Length = 14732\n--> Validation Set Length = 818\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:02<00:00, 6240.78it/s]\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 818/818 [00:00<00:00, 6480.99it/s]\nPreprocessing dataset:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f            | 13285/14732 [00:02<00:00, 6080.47it/s]/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nPreprocessing dataset:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a       | 13919/14732 [00:02<00:00, 6153.91it/s][rank0]: Traceback (most recent call last):\n[rank0]:   File \"/home/kaiwu/work/llama-recipes/recipes/finetuning/finetuning.py\", line 8, in <module>\n[rank0]:     fire.Fire(main)\n[rank0]:   File \"/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/fire/core.py\", line 143, in Fire\n[rank0]:     component_trace = _Fire(component, args, parsed_flag_args, context, name)\n[rank0]:   File \"/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/fire/core.py\", line 477, in _Fire\n[rank0]:     component, remaining_args = _CallAndUpdateTrace(\n[rank0]:   File \"/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/fire/core.py\", line 693, in _CallAndUpdateTrace\n[rank0]:     component = fn(*varargs, **kwargs)\n[rank0]:   File \"/home/kaiwu/work/llama-recipes/src/llama_recipes/finetuning.py\", line 268, in main\n[rank0]:     results = train(\n[rank0]:   File \"/home/kaiwu/work/llama-recipes/src/llama_recipes/utils/train_utils.py\", line 130, in train\n[rank0]:     with throughput_measure_context(train_config,local_rank) as measure_context:\n[rank0]:   File \"/home/kaiwu/miniconda3/envs/llama/lib/python3.10/contextlib.py\", line 135, in __enter__\n[rank0]:     return next(self.gen)\n[rank0]:   File \"/home/kaiwu/work/llama-recipes/src/llama_recipes/utils/train_utils.py\", line 38, in throughput_measure_context\n[rank0]:     raise ValueError(\"Cannot use both profiler and flop counter\")\n[rank0]: ValueError: Cannot use both profiler and flop counter\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:02<00:00, 6187.44it/s]\nTraining Epoch: 1:   0%|                                                                                                                                                       | 0/97 [00:00<?, ?it/s]\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 818/818 [00:00<00:00, 6373.49it/s]\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                                                                                                                       | 0/97 [00:00<?, ?it/s][rank1]: Traceback (most recent call last):\n[rank1]:   File \"/home/kaiwu/work/llama-recipes/recipes/finetuning/finetuning.py\", line 8, in <module>\n[rank1]:     fire.Fire(main)\n[rank1]:   File \"/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/fire/core.py\", line 143, in Fire\n[rank1]:     component_trace = _Fire(component, args, parsed_flag_args, context, name)\n[rank1]:   File \"/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/fire/core.py\", line 477, in _Fire\n[rank1]:     component, remaining_args = _CallAndUpdateTrace(\n[rank1]:   File \"/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/fire/core.py\", line 693, in _CallAndUpdateTrace\n[rank1]:     component = fn(*varargs, **kwargs)\n[rank1]:   File \"/home/kaiwu/work/llama-recipes/src/llama_recipes/finetuning.py\", line 268, in main\n[rank1]:     results = train(\n[rank1]:   File \"/home/kaiwu/work/llama-recipes/src/llama_recipes/utils/train_utils.py\", line 130, in train\n[rank1]:     with throughput_measure_context(train_config,local_rank) as measure_context:\n[rank1]:   File \"/home/kaiwu/miniconda3/envs/llama/lib/python3.10/contextlib.py\", line 135, in __enter__\n[rank1]:     return next(self.gen)\n[rank1]:   File \"/home/kaiwu/work/llama-recipes/src/llama_recipes/utils/train_utils.py\", line 38, in throughput_measure_context\n[rank1]:     raise ValueError(\"Cannot use both profiler and flop counter\")\n[rank1]: ValueError: Cannot use both profiler and flop counter\nTraining Epoch: 1:   0%|                                                                                                                                                       | 0/97 [00:00<?, ?it/s]\nE0415 13:31:51.312000 140564662125056 torch/distributed/elastic/multiprocessing/api.py:826] failed (exitcode: 1) local_rank: 0 (pid: 3476619) of binary: /home/kaiwu/miniconda3/envs/llama/bin/python\nTraceback (most recent call last):\n  File \"/home/kaiwu/miniconda3/envs/llama/bin/torchrun\", line 8, in <module>\n    sys.exit(main())\n  File \"/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 347, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/run.py\", line 879, in main\n    run(args)\n  File \"/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/run.py\", line 870, in run\n    elastic_launch(\n  File \"/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 132, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n  File \"/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 263, in launch_agent\n    raise ChildFailedError(\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError:\n============================================================\nrecipes/finetuning/finetuning.py FAILED\n------------------------------------------------------------\nFailures:\n[1]:\n  time      : 2024-04-15_13:31:51\n  host      : devgpu003.cco3.facebook.com\n  rank      : 1 (local_rank: 1)\n  exitcode  : 1 (pid: 3476620)\n  error_file: <N/A>\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n```\n\n\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [x] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [x] Did you make sure to update the documentation with your changes?  \n- [x] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "619": "Llamaguard notebook colab link fixFix colab link to account for new URL", "143": "pulled changes from fb main branch# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "157": "[WIP] prep sagemaker settings (dont review pls)# What does this PR do?\n\nStarting this PR to add Sagemaker recipe for FSDP \n\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "4": "Doc requirements updateUpdating requirements Transformer version and docs for int8 support in FSDP", "180": "Use OpenAssistent/oasst1 dataset for custom dataset example# What does this PR do?\n\nThis PR replaces the samsum dataset in the custom dataset examples and uses the OpenAssistent/oasst1 dataset instead\n\nFixes # (issue)\nN/A\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [X] pytest tests/\n```\n===================================================================================================== test session starts ======================================================================================================\nplatform linux -- Python 3.10.11, pytest-7.3.2, pluggy-1.0.0\nrootdir: /home/ubuntu/llama-recipes\nplugins: mock-3.11.1, anyio-3.7.0, hydra-core-1.0.7\ncollected 7 items\n\ntests/test_finetuning.py ...                                                                                                                                                                                             [ 42%]\ntests/test_train_utils.py .                                                                                                                                                                                              [ 57%]\ntests/datasets/test_custom_dataset.py ..                                                                                                                                                                                 [ 85%]\ntests/datasets/test_samsum_datasets.py .                                                                                                                                                                                 [100%]\n\n======================================================================================================= warnings summary =======================================================================================================\nsrc/llama_recipes/finetuning.py:5\n  /home/ubuntu/llama-recipes/src/llama_recipes/finetuning.py:5: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n    from pkg_resources import packaging\n\n../miniconda3/envs/llama/lib/python3.10/site-packages/pkg_resources/__init__.py:2871\n  /home/ubuntu/miniconda3/envs/llama/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.\n  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n    declare_namespace(pkg)\n\n../miniconda3/envs/llama/lib/python3.10/site-packages/pkg_resources/__init__.py:2871\n  /home/ubuntu/miniconda3/envs/llama/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n    declare_namespace(pkg)\n\n../miniconda3/envs/llama/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:147\n  /home/ubuntu/miniconda3/envs/llama/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:147: UserWarning: /home/ubuntu/miniconda3/envs/llama did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n    warn(msg)\n\n../miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/_shard/checkpoint/__init__.py:8\n  /home/ubuntu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/_shard/checkpoint/__init__.py:8: DeprecationWarning: torch.distributed._shard.checkpoint will be deprecated, use torch.distributed.checkpoint instead\n    warnings.warn(\n\ntests/test_train_utils.py::test_gradient_accumulation\n  /home/ubuntu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:329: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n    warnings.warn(\n\ntests/datasets/test_samsum_datasets.py::test_custom_dataset\n  /home/ubuntu/miniconda3/envs/llama/lib/python3.10/site-packages/dill/_dill.py:1705: PicklingWarning: Cannot locate reference to <class 'unittest.mock.MagicMock'>.\n    warnings.warn('Cannot locate reference to %r.' % (obj,), PicklingWarning)\n\ntests/datasets/test_samsum_datasets.py::test_custom_dataset\n  /home/ubuntu/miniconda3/envs/llama/lib/python3.10/site-packages/dill/_dill.py:1707: PicklingWarning: Cannot pickle <class 'unittest.mock.MagicMock'>: unittest.mock.MagicMock has recursive self-references that trigger a RecursionError.\n    warnings.warn('Cannot pickle %r: %s.%s has recursive self-references that trigger a RecursionError.' % (obj, obj.__module__, obj_name), PicklingWarning)\n\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n================================================================================================ 7 passed, 8 warnings in 20.12s ================================================================================================\n```\n\n## Before submitting\n- [X] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [X] Did you make sure to update the documentation with your changes?  \n- [X] Did you write any new necessary tests?\n", "751": "Add files via upload\n[\u041f\u0435\u0440\u0435\u0447\u0435\u043d\u044c \u0432\u043e\u043f\u0440\u043e\u0441\u043e\u0432 \u043a \u044d\u043a\u0437\u0430\u043c\u0435\u043d\u0443.pdf](https://github.com/user-attachments/files/17539942/default.pdf)\n", "745": "Update wordlist.txt", "962": "Fix README links for use cases# What does this PR do?\n\nFixes links for the use cases in the README\n", "786": "[draft] [bug] Fix ipynb rendering# What does this PR do?\n\nThe notebooks appears to have rendering issues on the Github UI. Downloading the raw data and notebook works fine. Trying to save and reupload in an attempt to fix the rendering.\n\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "553": "Add Groq/Llama3 recipes (cookbook and command line examples)# What does this PR do?\n\n<!--\nThis PR adds several recipes for using Llama3 with Groq via cookbook posts. These posts cover use cases such as RAG, Feature Extraction with JSON Mode and Function Calling.\n\n(Note: this is a followup to https://github.com/meta-llama/llama-recipes/pull/552 which I closed out due to it being out of date with main)\n\n-->\n\n\n## Feature/Issue validation/testing\n\nThese cookbooks can be run as jupyter notebooks provided that `GROQ_API_KEY` is set as an environment variable (Groq API Keys can be curated here: https://console.groq.com/keys\n\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n\n\n6/6 update: I am failing on lint/spellcheck tests but I don't believe that I should be:\n- https://console.groq.com/keys is a valid address (you do need to be logged in)\n- Words such as 'Groq', 'GroqCloud', 'DuckDB' and 'Replit' are valid in the context of my README files\n", "547": "Remove pkg_resources.packaging# What does this PR do?\nThis PR removes the usage of pkg_resources.packaging which was remove in setuptools 70.0.0  https://github.com/openai/CLIP/issues/446\n\nFixes # (issue)\n\n## Before submitting\n- [X] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n\nThanks for contributing \ud83c\udf89!\n", "590": "Updating chatbot folder namesUpdating chatbot folder names under customerservice_chatbot folder", "584": "Update hello_llama_cloud.ipynbFix: LangChai->LangChain\n\n# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "586": "Update 3p_integration README.md# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "237": "Create codeql.ymlAdd security scanning\n\n# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "551": "Update hf weight conversion script to llama 3# What does this PR do?\n\nThe convert_hf_weights_to_llama.py script convert huggingface llama checkpoint to consolidated llama format. This PR updates the script so that it can also handle llama 3.\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ X] Convert Llama3 70B\n```\nmkdir test70B; cp ~/workspace/github/llama3/Meta-Llama-3-70B/params.json test70B/\npython -m llama_recipes.tools.convert_hf_weights_to_llama --model-path meta-llama/Meta-Llama-3-70B-Instruct  --output-dir test70B --model-size 70B\n\npython compare_llama_weights.py ./test70B ~/workspace/github/llama3/Meta-Llama-3-70B-Instruct\nComparing shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:43<00:00, 43.96s/it]\nTop 10 largest deltas:\n  shard 6 layers.54.attention_norm.weight: 0.005859375 vs 0.1533203125\n  shard 1 layers.31.attention_norm.weight: 0.00390625 vs 0.22265625\n  shard 1 layers.53.attention_norm.weight: 0.00390625 vs 0.197265625\n```\n\n- [X ] Convert Llama3 8B\n```\nmkdir test8B; cp ~/workspace/github/llama3/Meta-Llama-3-8B/params.json test8B/\npython -m llama_recipes.tools.convert_hf_weights_to_llama --model-path meta-llama/Meta-Llama-3-8B-Instruct --output-dir test8B --model-size 8B\npython compare_llama_weights.py ./test8B/ ~/workspace/github/llama3/Meta-Llama-3-8B-Instruct\nTop 10 largest deltas:\n  shard 0 layers.0.attention.wk.weight: 0.0 vs 0.014404296875\n  shard 0 layers.0.attention.wo.weight: 0.0 vs 0.005645751953125\n```\n\n- [X ] Converted 70B inference test\n```\ntorchrun --nproc_per_node 8 example_chat_completion.py     --ckpt_dir ~/workspace/github/llama-recipes/src/llama_recipes/utils/hf_llama_conversion/test70B --tokenizer_path Meta-Llama-3-70B-Instruct/tokenizer.model     --max_seq_len 512 --max_batch_size 6\n\n==================================\nUser: I am going to Paris, what should I see?\n\nAssistant: Paris, the capital of France, is known for its stunning architecture, art museums, historical landmarks, and romantic atmosphere. Here are some of the top attractions to see in Paris:\n\n1. The Eiffel Tower: The iconic Eiffel Tower is one of the most recognizable landmarks in the world and offers breathtaking views of the city.\n2. The Louvre Museum: The Louvre is one of the world's largest and most famous museums, housing an impressive collection of art and artifacts, including the Mona Lisa.\n3. Notre-Dame Cathedral: This beautiful cathedral is one of the most famous landmarks in Paris and is known for its Gothic architecture and stunning stained glass windows.\n\nThese are just a few of the many attractions that Paris has to offer. With so much to see and do, it's no wonder that Paris is one of the most popular tourist destinations in the world.\n\nUser: What is so great about #1?\n\n> Assistant: The Eiffel Tower! It's an iconic symbol of Paris and one of the most recognizable landmarks in the world. Here are some reasons why it's so great:\n\n1. **Engineering marvel**: When it was built for the 1889 World's Fair, it was the tallest structure in the world (324 meters/1,063 feet) and a technological marvel. It's still an impressive feat of engineering today.\n2. **Breathtaking views**: From the top, you can see the entire city of Paris, including many of its famous landmarks like the Seine River, Notre-Dame Cathedral, and the Arc de Triomphe.\n3. **Romantic atmosphere**: The Eiffel Tower is often associated with romance and love. It's a popular spot for proposals, weddings, and romantic dinners.\n4. **History**: The tower has played a significant role in French history, serving as a communication tower during World War I and a symbol of French resistance during World War II.\n5. **Aesthetic appeal**: The tower's lattice-like design is both functional and beautiful, making it a work of art in its own right.\n6. **Day and night experiences**: The tower is stunning during the day, but it's even more magical at night when it's illuminated with thousands of twinkling lights.\n7. **Accessibility**: You can take the stairs or elevator to the top, making it accessible to people of all ages and abilities.\n8. **\n```\n\n- [X ] Converted 8B inference test\n```\ntorchrun --nproc_per_node 1 example_chat_completion.py     --ckpt_dir ~/workspace/github/llama-recipes/src/llama_recipes/utils/hf_llama_conversion/test8B --tokenizer_path Meta-Llama-3-8B-Instruct/tokenizer.model     --max_seq_len 512 --max_batch_size 6\n\n==================================\n\nUser: I am going to Paris, what should I see?\n\nAssistant: Paris, the capital of France, is known for its stunning architecture, art museums, historical landmarks, and romantic atmosphere. Here are some of the top attractions to see in Paris:\n\n1. The Eiffel Tower: The iconic Eiffel Tower is one of the most recognizable landmarks in the world and offers breathtaking views of the city.\n2. The Louvre Museum: The Louvre is one of the world's largest and most famous museums, housing an impressive collection of art and artifacts, including the Mona Lisa.\n3. Notre-Dame Cathedral: This beautiful cathedral is one of the most famous landmarks in Paris and is known for its Gothic architecture and stunning stained glass windows.\n\nThese are just a few of the many attractions that Paris has to offer. With so much to see and do, it's no wonder that Paris is one of the most popular tourist destinations in the world.\n\nUser: What is so great about #1?\n\n> Assistant: The Eiffel Tower! It's one of the most iconic landmarks in the world, and for good reason. Here are some reasons why it's so great:\n\n1. **Engineering marvel**: The Eiffel Tower was the tallest structure in the world when it was built for the 1889 World's Fair. Its innovative design and construction were a marvel of engineering at the time, and it remains an impressive feat to this day.\n2. **Panoramic views**: The Eiffel Tower offers stunning 360-degree views of the City of Light from its observation decks on the first and second floors. On a clear day, you can see up to 59 kilometers (37 miles) in every direction.\n3. **Romantic atmosphere**: The Eiffel Tower is often associated with romance, and it's easy to see why. The tower's iron latticework is beautifully lit up at night, creating a magical atmosphere that's perfect for couples.\n4. **Historical significance**: The Eiffel Tower has played a significant role in French history, serving as a symbol of French culture and engineering prowess. It's also been the site of many historic events, including the signing of the Treaty of Versailles in 1919.\n5. **Iconic status**: The Eiffel Tower is instantly recognizable, and its image is synonymous with Paris and France. It's a must-see attraction for anyone visiting the city.\n6. **Unique architecture\n```\n\n- [ X] Convert Llama2 70B\n```\nmkdir llama2-70b; cp ~/llama2/llama-2-70b-chat/params.json llama2-70b/\npython -m llama_recipes.tools.convert_hf_weights_to_llama --model-path  meta-llama/Llama-2-70b-chat-hf --output-dir llama2-70b --model-size 70B\npython compare_llama_weights.py llama2-70b ~/llama2/llama-2-70b-chat\nTop 10 largest deltas:\n  shard 0 layers.0.attention.wk.weight: 2.9802322387695312e-08 vs 0.01544189453125\n  shard 0 layers.0.attention.wo.weight: 2.9802322387695312e-08 vs 0.005828857421875\n  shard 0 layers.0.attention.wq.weight: 2.9802322387695312e-08 vs 0.006591796875\n```\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [X ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "545": "Fix typo in Getting_to_know_Llama.ipynb# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n\n\n", "790": "colab links fix# What does this PR do?\n\nChanged the Colab links accordingly after the DLAI agent notebooks folder was changed.\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "753": "Update hello_llama_cloud.ipynbCell no. 3, showing error\n\n# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "196": "Fix vocab size mismatch in inference due to added pad token# What does this PR do?\nThis PR changes the method to set the padding token in the inference example. The previous method increases the vocab size which leads to this error in the newest accelerate version. As we're not using padding in this example the alternative method should be okay. Refer to [this discussion](https://github.com/huggingface/transformers/issues/22312) for differences on the methods.\nWe'll should revisit this and provide a batched inference example as well.\nFixes # (issue)\n#179 \n\n## Feature/Issue validation/testing\nBefore:\n```\npython examples/inference.py meta-llama/Llama-2-7b-chat-hf --prompt_file examples/samsum_prompt.txt\n\n===================================BUG REPORT===================================\nWelcome to bitsandbytes. For bug reports, please run\n\npython -m bitsandbytes\n\n and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n================================================================================\n/home/ubuntu/miniconda3/envs/llama/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:147: UserWarning: /home/ubuntu/miniconda3/envs/llama did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n  warn(msg)\nCUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\nCUDA SETUP: Highest compute capability among GPUs detected: 8.6\nCUDA SETUP: Detected CUDA version 118\nCUDA SETUP: Loading binary /home/ubuntu/miniconda3/envs/llama/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\nDownloading (\u2026)lve/main/config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 614/614 [00:00<00:00, 7.15MB/s]\nDownloading (\u2026)fetensors.index.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 26.8k/26.8k [00:00<00:00, 406kB/s]\nDownloading (\u2026)of-00002.safetensors: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9.98G/9.98G [00:39<00:00, 255MB/s]\nDownloading (\u2026)of-00002.safetensors: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.50G/3.50G [00:13<00:00, 264MB/s]\nDownloading shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:52<00:00, 26.30s/it]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:04<00:00,  2.27s/it]\nDownloading (\u2026)neration_config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 188/188 [00:00<00:00, 2.59MB/s]\nDownloading tokenizer.model: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 500k/500k [00:00<00:00, 529MB/s]\nDownloading (\u2026)cial_tokens_map.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 414/414 [00:00<00:00, 4.41MB/s]\nDownloading (\u2026)okenizer_config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 776/776 [00:00<00:00, 6.08MB/s]\nYou are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32001. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\nUser prompt deemed safe.\nUser prompt:\nSummarize this dialog:\n\nA: Hi Tom, are you busy tomorrow\u2019s afternoon?\n\nB: I\u2019m pretty sure I am. What\u2019s up?\n\nA: Can you go with me to the animal shelter?.\n\nB: What do you want to do?\n\nA: I want to get a puppy for my son.\n\nB: That will make him so happy.\n\nA: Yeah, we\u2019ve discussed it many times. I think he\u2019s ready now.\n\nB: That\u2019s good. Raising a dog is a tough issue. Like having a baby ;-)\n\nA: I'll get him one of those little dogs.\n\nB: One that won't grow up too big;-)\n\nA: And eat too much;-))\n\nB: Do you know which one he would like?\n\nA: Oh, yes, I took him there last Monday. He showed me one that he really liked.\n\nB: I bet you had to drag him away.\n\nA: He wanted to take it home right away ;-).\n\nB: I wonder what he'll name it.\n\nA: He said he\u2019d name it after his dead hamster \u2013 Lemmy  - he's  a great Motorhead fan :-)))\n\n---\n\nSummary:\nAsking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\nAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\nTraceback (most recent call last):\n  File \"/home/ubuntu/llama-recipes/examples/inference.py\", line 140, in <module>\n    fire.Fire(main)\n  File \"/home/ubuntu/miniconda3/envs/llama/lib/python3.10/site-packages/fire/core.py\", line 141, in Fire\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n  File \"/home/ubuntu/miniconda3/envs/llama/lib/python3.10/site-packages/fire/core.py\", line 475, in _Fire\n    component, remaining_args = _CallAndUpdateTrace(\n  File \"/home/ubuntu/miniconda3/envs/llama/lib/python3.10/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\n    component = fn(*varargs, **kwargs)\n  File \"/home/ubuntu/llama-recipes/examples/inference.py\", line 108, in main\n    outputs = model.generate(\n  File \"/home/ubuntu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"/home/ubuntu/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1648, in generate\n    return self.sample(\n  File \"/home/ubuntu/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2730, in sample\n    outputs = self(\n  File \"/home/ubuntu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/ubuntu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/ubuntu/miniconda3/envs/llama/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n  File \"/home/ubuntu/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 838, in forward\n    logits = self.lm_head(hidden_states)\n  File \"/home/ubuntu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/ubuntu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/ubuntu/miniconda3/envs/llama/lib/python3.10/site-packages/accelerate/hooks.py\", line 160, in new_forward\n    args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs)\n  File \"/home/ubuntu/miniconda3/envs/llama/lib/python3.10/site-packages/accelerate/hooks.py\", line 286, in pre_forward\n    set_module_tensor_to_device(\n  File \"/home/ubuntu/miniconda3/envs/llama/lib/python3.10/site-packages/accelerate/utils/modeling.py\", line 281, in set_module_tensor_to_device\n    raise ValueError(\nValueError: Trying to set a tensor of shape torch.Size([32000, 4096]) in \"weight\" (which has shape torch.Size([32001, 4096])), this look incorrect.\n```\nAfter this PR:\n```\n python examples/inference.py meta-llama/Llama-2-7b-chat-hf --prompt_file examples/samsum_prompt.txt\n\n===================================BUG REPORT===================================\nWelcome to bitsandbytes. For bug reports, please run\n\npython -m bitsandbytes\n\n and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n================================================================================\n/home/ubuntu/miniconda3/envs/llama/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:147: UserWarning: /home/ubuntu/miniconda3/envs/llama did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n  warn(msg)\nCUDA SETUP: CUDA runtime path found: /usr/local/cuda/targets/x86_64-linux/lib/libcudart.so.11.0\nCUDA SETUP: Highest compute capability among GPUs detected: 8.6\nCUDA SETUP: Detected CUDA version 118\nCUDA SETUP: Loading binary /home/ubuntu/miniconda3/envs/llama/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:04<00:00,  2.22s/it]\nUser prompt deemed safe.\nUser prompt:\nSummarize this dialog:\n\nA: Hi Tom, are you busy tomorrow\u2019s afternoon?\n\nB: I\u2019m pretty sure I am. What\u2019s up?\n\nA: Can you go with me to the animal shelter?.\n\nB: What do you want to do?\n\nA: I want to get a puppy for my son.\n\nB: That will make him so happy.\n\nA: Yeah, we\u2019ve discussed it many times. I think he\u2019s ready now.\n\nB: That\u2019s good. Raising a dog is a tough issue. Like having a baby ;-)\n\nA: I'll get him one of those little dogs.\n\nB: One that won't grow up too big;-)\n\nA: And eat too much;-))\n\nB: Do you know which one he would like?\n\nA: Oh, yes, I took him there last Monday. He showed me one that he really liked.\n\nB: I bet you had to drag him away.\n\nA: He wanted to take it home right away ;-).\n\nB: I wonder what he'll name it.\n\nA: He said he\u2019d name it after his dead hamster \u2013 Lemmy  - he's  a great Motorhead fan :-)))\n\n---\n\nSummary:\nAsking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\nAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\nthe inference time is 69418.26907899986 ms\nUser input and model output deemed safe.\nModel output:\nSummarize this dialog:\n\nA: Hi Tom, are you busy tomorrow\u2019s afternoon?\n\nB: I\u2019m pretty sure I am. What\u2019s up?\n\nA: Can you go with me to the animal shelter?.\n\nB: What do you want to do?\n\nA: I want to get a puppy for my son.\n\nB: That will make him so happy.\n\nA: Yeah, we\u2019ve discussed it many times. I think he\u2019s ready now.\n\nB: That\u2019s good. Raising a dog is a tough issue. Like having a baby ;-)\n\nA: I'll get him one of those little dogs.\n\nB: One that won't grow up too big;-)\n\nA: And eat too much;-))\n\nB: Do you know which one he would like?\n\nA: Oh, yes, I took him there last Monday. He showed me one that he really liked.\n\nB: I bet you had to drag him away.\n\nA: He wanted to take it home right away ;-).\n\nB: I wonder what he'll name it.\n\nA: He said he\u2019d name it after his dead hamster \u2013 Lemmy  - he's  a great Motorhead fan :-)))\n\n---\n\nSummary: The speaker A wants to take their son to the animal shelter to get a puppy for their son. Their friend B is hesitant but agrees to go with them. They discuss the process of raising a dog and the different types of dogs that are available. They also make jokes about the size and appetite of the dog. The speaker A mentions that their son wants to name the dog after his dead hamster, Lemmy.\n```\n\n## Before submitting\n- [X] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n\nThanks for contributing \ud83c\udf89!\n", "40": "Fix cuda id for using quantizationThis PR addresses issue #38 ", "627": "Eval reproduce recipe using lm-evaluation-harness and our 3.1 evals datasetsThis tutorial provides a detailed guide on how to reproduce the Meta Llama 3.1 evaluation metrics using the lm-evaluation-harness and our 3.1 evals datasets. By following the steps outlined, users can replicate our evaluation process for specific tasks and compare their results with our reported metrics. While slight variations in results are expected due to differences in implementation and model behavior, this guide aims to provide a transparent and reproducible method for evaluating Meta Llama 3 models.\n\n\nFixes # (issue)\n[issue 613](https://github.com/meta-llama/llama-recipes/issues/613)\n\n\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [x] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [x] Did you make sure to update the documentation with your changes?  \n- [x] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "97": "adding flash attention and xformer memory efficient through PT SDPA# What does this PR do?\n\nThis PR adds the Flash Attention and Xformer mem-efficient kernel through PT SDPA, this work has been integrated with `optimum` library of HF, read more about [here](https://pytorch.org/blog/out-of-the-box-acceleration/).\n\nTested on 7B for FSDP only had a nice 30% speed up, for FSDP+PEFT 5% and not much on PEFT+quantization/1 gpu.\n\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\nNo related issue.\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A : Logs/ perf number of with out this feature with 10 steps : avg epoch time 55.17s\n[Logs for Test A\n](https://gist.github.com/HamidShojanazeri/e6cc9eb3ca9a3bf5a83174c8eb2c5a18)\n\n\n- [ ] Test B : Logs/ perf number of with this feature with 10 steps : avg epoch time 42.44\n[Logs for Test B](https://gist.github.com/HamidShojanazeri/587755cebc0aa0e67edf0995a70f8127)\n\n\n## Before submitting\n- [x ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [x ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "394": "Refactor the folder structure to organize recipes by topic# What does this PR do?\n\nThe PR refactors recipes in the current repo to be organized by topic. Contents within demo_apps/ and examples/ are moved into recipes/.\n\nWith the new folders, the PR also creates new folder-level READMEs to provide information about the recipes in the same folder. The main README has also been minimized and the existing content has been refactored into the relevant sections of the repo.\n\nThese changes are part of the broader effort to improve the developer experience of using this repository.\n\nThis PR supersedes the older closed one: https://github.com/meta-llama/llama-recipes/pull/391", "343": "Adding test cases for the bugs found, local_rank None and output dir \u2026# What does this PR do?\n\nFixes 2 bugs introduced on #220:\n1. None value was used in the json file name when the local_rank was not informed. \n2. Directory structure was not created before saving the metrics json file.\n\nCreated new tests to reproduce these errors and fixed them\n## Testing\nRunning unit tests and running finetuning for an epoch:\n1. Tests:\n![image](https://github.com/facebookresearch/llama-recipes/assets/222731/9c8d03e6-8212-4c0a-a7cf-ac74d031b759)\n\n2. Finetuning command: \n`python -m llama_recipes.finetuning  --use_peft --peft_method lora --quantization --model_name ../llama/models_hf/7B/ --output_dir ../llama/models_ft/Llama-2-7b-peft-samsum-1 --batch_size_training 2  --gradient_accumulation_steps 2  --num_epochs 1 --save_model --save_metrics  --dataset samsum_dataset`\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [x] Did you write any new necessary tests?\n\n\n", "431": "Llama2 on EC2 tutorial using Runhouse.# What does this PR do?\nUpdated version of [old PR](https://github.com/meta-llama/llama-recipes/pull/375).\n\nA simple code file to stand up a Llama2 model on an A10G running in EC2. Uses the Runhouse Python library to easily provision an instance, copy code over, and serve the model.\n\nThe example is also commented with details on what different Runhouse components are doing.\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [x] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "357": "Flop counter, profiling and GC# What does this PR do?\nThis PR help to add profling capture metrics in terms of FLops and use garbage collection to speed up the communication in FSDP mutli-node.\n\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "419": "Add note on CUDA version + remove 'test' from pytorch whl url# What does this PR do?\n\n- Adds a note on using the correct CUDA version for pip installs.\n- Removes 'test' from the pytorch whl install\n", "418": "Remove openai from example notebook and llm.py class. Simplify notebook layout# What does this PR do?\n\nRemove openai from example notebook and llm.py class. Simplify notebook layout and highlight additional packages that need to be installed.\nAs discussed with @HamidShojanazeri \n\n\n## Feature/Issue validation/testing\n\nRan both examples on the notebook ( recipes/llama_api_providers/Using_Externally_Hosted_LLMs.ipynb ) \n(I used my own access tokens)", "356": "Eval harness # What does this PR do?\nAdding Eval harness from EleutherAI. \n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\n[eval-test-logs ](https://gist.github.com/HamidShojanazeri/d492c10d5bf579d3229c56103f1e00b4)\n[accelerate-logs-parallelize](https://gist.github.com/HamidShojanazeri/63224baf901b84d50d317a4c2745fb47)\n[vllm test logs](https://gist.github.com/HamidShojanazeri/9d36d95b19fea39598cfa989cd24b796)\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "342": "Update Installation section of README.md* Move `Install with optional dependencies` section above `Install from source` since it should go next to the section about installing with pip\n* Add `git clone` and `cd llama-recipes` to `Install from source` section", "395": "Adding open in colab option for notebook## What does this PR do?\n* Adding an \"Open in Colab\" button for HF notebook\n* Updating deprecated links\n* Update git clone command in README\n", "632": "Updating llama 3 references to 3.1 model# What does this PR do?\n\nUpdates llama 3.1 links and model references in quickstart notebooks\n", "801": "Add llama 3.2 mmlu, math, gpqa evals to meta_eval harness# What does this PR do?\n\nResolves #732\n\nAdds in `mmlu`, `math` and `gpqa` evaluation for llama 3.2-1B and 3.2-3B eval datasets.\n\n### Pretrain models:\n\nLlama 3.2-1B:\n\n```\n|    Tasks    |Version|Filter|n-shot| Metric |   |Value |   |Stderr|\n|-------------|-------|------|-----:|--------|---|-----:|---|-----:|\n| - meta_mmlu |      1|none  |     0|acc     |\u2191  |0.3146|\u00b1  |0.0039|\n|             |       |none  |     0|acc_norm|\u2191  |0.3146|\u00b1  |0.0039|\n|meta_pretrain|N/A    |none  |     0|acc     |\u2191  |0.3146|\u00b1  |0.0039|\n|             |       |none  |     0|acc_norm|\u2191  |0.3146|\u00b1  |0.0039|\n\n|   Groups    |Version|Filter|n-shot| Metric |   |Value |   |Stderr|\n|-------------|-------|------|-----:|--------|---|-----:|---|-----:|\n|meta_pretrain|N/A    |none  |     0|acc     |\u2191  |0.3146|\u00b1  |0.0039|\n|             |       |none  |     0|acc_norm|\u2191  |0.3146|\u00b1  |0.0039|\n```\n\nLlama 3.2-3B:\n\n```\n|    Tasks    |Version|Filter|n-shot| Metric |   |Value |   |Stderr|\n|-------------|-------|------|-----:|--------|---|-----:|---|-----:|\n| - meta_mmlu |      1|none  |     0|acc     |\u2191  |0.5643|\u00b1  |0.0042|\n|             |       |none  |     0|acc_norm|\u2191  |0.5643|\u00b1  |0.0042|\n|meta_pretrain|N/A    |none  |     0|acc     |\u2191  |0.5643|\u00b1  |0.0042|\n|             |       |none  |     0|acc_norm|\u2191  |0.5643|\u00b1  |0.0042|\n\n|   Groups    |Version|Filter|n-shot| Metric |   |Value |   |Stderr|\n|-------------|-------|------|-----:|--------|---|-----:|---|-----:|\n|meta_pretrain|N/A    |none  |     0|acc     |\u2191  |0.5643|\u00b1  |0.0042|\n|             |       |none  |     0|acc_norm|\u2191  |0.5643|\u00b1  |0.0042|\n```\n\nPretty close to the meta reported numbers:\n\n|| This mmlu Eval | Meta mmlu Eval |\n| --- | --- | --- |\n| Llama 3.2-1B | 0.315 | 0.317 [[1](https://huggingface.co/datasets/meta-llama/Llama-3.2-1B-evals/viewer/Llama-3.2-1B-evals__metrics/latest?f%5Bmetric_type%5D%5Bvalue%5D=%27task_metric%27&row=545)]  |\n| Llama 3.2-3B | 0.56 | 0.565 [[1](https://huggingface.co/datasets/meta-llama/Llama-3.2-3B-evals/viewer/Llama-3.2-3B-evals__metrics/latest?f%5Bmetric_type%5D%5Bvalue%5D=%27task_metric%27&row=545)] |\n\n### Instruct Models\n\n| | this eval | reported |\n| --- | --- | --- |\n| 3.2-1B-Instruct MMLU | 0.462 | 0.485 [[1](https://huggingface.co/datasets/meta-llama/Llama-3.2-1B-Instruct-evals/viewer/Llama-3.2-1B-Instruct-evals__metrics/latest?f%5Bmetric_type%5D%5Bvalue%5D=%27task_metric%27&row=1005)] |\n| 3.2-1B-Instruct MATH | 0.287 | 0.304 [[1](https://huggingface.co/datasets/meta-llama/Llama-3.2-1B-Instruct-evals/viewer/Llama-3.2-1B-Instruct-evals__metrics/latest?f%5Bmetric_type%5D%5Bvalue%5D=%27task_metric%27&sort%5Bcolumn%5D=benchmark_label&sort%5Bdirection%5D=asc&p=1&row=1063)] |\n| 3.2-1B-Instruct GPQA | 0.257 | 0.272 [[1](https://huggingface.co/datasets/meta-llama/Llama-3.2-1B-Instruct-evals/viewer/Llama-3.2-1B-Instruct-evals__metrics/latest?f%5Bmetric_type%5D%5Bvalue%5D=%27task_metric%27&sort%5Bcolumn%5D=benchmark_label&sort%5Bdirection%5D=asc&row=1227)] |\n\n```\n|    Tasks    |Version|   Filter   |n-shot|  Metric   |   |Value |   |Stderr|\n|-------------|-------|------------|-----:|-----------|---|-----:|---|-----:|\n| - meta_gpqa |      1|strict-match|     0|exact_match|\u2191  |0.2567|\u00b1  |0.0207|\n|meta_instruct|N/A    |none        |     0|acc        |\u2191  |0.4618|\u00b1  |0.0042|\n|             |       |none        |     0|acc_norm   |\u2191  |0.4618|\u00b1  |0.0042|\n|             |       |none        |     0|exact_match|\u2191  |0.2872|\u00b1  |0.0064|\n|             |       |strict-match|     0|exact_match|\u2191  |0.2567|\u00b1  |0.0207|\n| - meta_math |      1|none        |     0|exact_match|\u2191  |0.2872|\u00b1  |0.0064|\n| - meta_mmlu |      1|none        |     0|acc        |\u2191  |0.4618|\u00b1  |0.0042|\n|             |       |none        |     0|acc_norm   |\u2191  |0.4618|\u00b1  |0.0042|\n\n|   Groups    |Version|   Filter   |n-shot|  Metric   |   |Value |   |Stderr|\n|-------------|-------|------------|-----:|-----------|---|-----:|---|-----:|\n|meta_instruct|N/A    |none        |     0|acc        |\u2191  |0.4618|\u00b1  |0.0042|\n|             |       |none        |     0|acc_norm   |\u2191  |0.4618|\u00b1  |0.0042|\n|             |       |none        |     0|exact_match|\u2191  |0.2872|\u00b1  |0.0064|\n|             |       |strict-match|     0|exact_match|\u2191  |0.2567|\u00b1  |0.0207|\n```\n| | this eval | reported |\n| --- | --- | --- |\n| 3.2-3B-Instruct MMLU | 0.607 | 0.637 [[1](https://huggingface.co/datasets/meta-llama/Llama-3.2-3B-Instruct-evals/viewer/Llama-3.2-3B-Instruct-evals__metrics/latest?f%5Bmetric_type%5D%5Bvalue%5D=%27task_metric%27&row=1005)] |\n| 3.2-3B-Instruct MATH | 0.451 | 0.475 [[1](https://huggingface.co/datasets/meta-llama/Llama-3.2-3B-Instruct-evals/viewer/Llama-3.2-3B-Instruct-evals__metrics/latest?f%5Bmetric_type%5D%5Bvalue%5D=%27task_metric%27&row=1063)] |\n| 3.2-3B-Instruct GPQA | 0.333 | 0.328 [[1](https://huggingface.co/datasets/meta-llama/Llama-3.2-3B-Instruct-evals/viewer/Llama-3.2-3B-Instruct-evals__metrics/latest?f%5Bmetric_type%5D%5Bvalue%5D=%27task_metric%27&p=2&row=1227)] |\n\n```\n|    Tasks    |Version|   Filter   |n-shot|  Metric   |   |Value |   |Stderr|\n|-------------|-------|------------|-----:|-----------|---|-----:|---|-----:|\n| - meta_gpqa |      1|strict-match|     0|exact_match|\u2191  |0.3326|\u00b1  |0.0223|\n|meta_instruct|N/A    |none        |     0|acc        |\u2191  |0.6065|\u00b1  |0.0041|\n|             |       |none        |     0|acc_norm   |\u2191  |0.6065|\u00b1  |0.0041|\n|             |       |none        |     0|exact_match|\u2191  |0.4514|\u00b1  |0.0070|\n|             |       |strict-match|     0|exact_match|\u2191  |0.3326|\u00b1  |0.0223|\n| - meta_math |      1|none        |     0|exact_match|\u2191  |0.4514|\u00b1  |0.0070|\n| - meta_mmlu |      1|none        |     0|acc        |\u2191  |0.6065|\u00b1  |0.0041|\n|             |       |none        |     0|acc_norm   |\u2191  |0.6065|\u00b1  |0.0041|\n\n|   Groups    |Version|   Filter   |n-shot|  Metric   |   |Value |   |Stderr|\n|-------------|-------|------------|-----:|-----------|---|-----:|---|-----:|\n|meta_instruct|N/A    |none        |     0|acc        |\u2191  |0.6065|\u00b1  |0.0041|\n|             |       |none        |     0|acc_norm   |\u2191  |0.6065|\u00b1  |0.0041|\n|             |       |none        |     0|exact_match|\u2191  |0.4514|\u00b1  |0.0070|\n|             |       |strict-match|     0|exact_match|\u2191  |0.3326|\u00b1  |0.0223|\n```\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n### [x] Test A\n\nUpdate `llama-recipes/tools/benchmarks/llm_eval_harness/meta_eval/eval_config.yaml` to:\n\n```yaml\nmodel_name: \"meta-llama/Llama-3.2-1B\"\nevals_dataset: \"meta-llama/Llama-3.2-1B-evals\"\n...\n```\n\nThen run:\n\n```bash\npython prepare_meta_eval.py --config_path ./eval_config.yaml\n```\n\nThen run the generated command\n\nLogs for Test A:\n- Command logs: [test-A-logs.txt](https://github.com/user-attachments/files/17878778/test-A-logs.txt)\n- Evaluation data: https://huggingface.co/datasets/aidando73/llama3-recipe-3.2-evals-mmlu\n\n### [x] Test B\n\nUpdate `llama-recipes/tools/benchmarks/llm_eval_harness/meta_eval/eval_config.yaml` to:\n\n```yaml\nmodel_name: \"meta-llama/Llama-3.2-3B\"\nevals_dataset: \"meta-llama/Llama-3.2-3B-evals\"\n```\n\nThen run:\n\n```bash\npython prepare_meta_eval.py --config_path ./eval_config.yaml\n```\n\nThen run the generated command\n\nLogs for Test B:\n- Command logs: [test-B-logs.txt](https://github.com/user-attachments/files/17878860/test-B-logs.txt)\n- Evaluation data: https://huggingface.co/datasets/aidando73/llama3-recipe-3.2-evals-mmlu\n\n\n\n## Before submitting\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests), Pull Request section?\n\n[N/A] Was this discussed/approved via a Github issue? Please add a link to it if that's the case.\n\n- [x] Did you make sure to update the documentation with your changes?  \n\n[N/A] Did you write any new necessary tests? Seems like we're mainly testing the meta_eval harness manually at this stage\n", "41": "Improve FSDP LoRA Memory Usage**Overview**\nThis PR improves the FSDP LoRA fine-tuning memory usage and fixes some typos.\n- We enable the FSDP rate limiter (which was intentionally disabled for some reason). This decreases reserved memory by 5.31 GB and active memory by 5.66 GB on 4 GPUs.\n- We do not keep `logits` (as well as `past_key_values`, `hidden_states`, and `attentions`) alive after forward / through backward by avoiding any references to those tensors via `loss = model(*batch).loss` instead of `outputs = model(*batch); loss = outputs.loss`. This further decreases reserved memory by 8.00 GB and active memory by 4.98 GB on 4 GPUs. (thanks @albanD for the help!)\n- Overall, this **decreases peak reserved memory by 13.31 GB and peak active memory by 10.64 GB** on 4 GPUs for pure bf16 training. In the end, this only uses 15.22 GB peak active memory and 19.36 GB peak reserved memory.\n\n\n**Test Plan**\n```\ntorchrun --nnodes 1 --nproc_per_node 4  llama_finetuning.py --enable_fsdp --use_peft --peft_method lora --model_name llama-2-7b-hf/ --pure_bf16 --output_dir output_dir\n```\n\nFor my AWS setup with 4 40 GB A100 GPUs:\n- With `limit_all_gathers=False`:\n    - Peak active: 25.86 GB\n    - Peak reserved: 32.67 GB\n- With `limit_all_gathers=True`:\n    - Peak active: 20.20 GB\n    - Peak reserved: 27.36 GB\n- With `limit_all_gathers=True` and not keeping logits alive:\n    - Peak active: 15.22 GB\n    - Peak reserved: 19.36 GB\n\n```\nmem_stats = torch.cuda.memory_stats()\npeak_active_gb = mem_stats[\"active_bytes.all.peak\"] / (1024 ** 3)\npeak_reserved_gb = mem_stats[\"reserved_bytes.all.peak\"] / (1024 ** 3)\nprint(f\"peak active: {peak_active_gb} GB | peak reserved: {peak_reserved_gb} GB\") \n```\n", "197": "pass weight_decay into optimizer# What does this PR do?\n\n@mreso Fixes #191\n\n## Feature/Issue validation/testing\n\nThis is a bug fix on actually using weight decay and should not impact any existing workload.\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [x] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "829": "update parsing of dataset_config.file to prevent custom-function-name from clobbering data-collator name. # What does this PR do?\n\nMakes a minor fix to the parsing of the `--custom_dataset.file` flag. The documentation says you can add a colon in this value to specify a custom name to replace the `get_custom_dataset` function.\n\nUnfortunately, the string after the colon is ALSO used to set a custom data collator name. This update forces the data collator name to always be \"get_data_collator\", and updates the documentionation to reflect that. \n\nFixes #828 \n\n## Feature/Issue validation/testing\n\nUse the \"custom_dataset\" provided from the recipes. Copy it to a local directory as\n\n`cp ../llama-recipes/recipes/quickstart/finetuning/datasets/custom_dataset.py .`\n\n- [ ] Verify that the custom_dataset.py works correctly, when the colon is not present. \n\n```\n(llama-dev) azureuser@yh-a100:~/cloudfiles/code/test_llama$ ./working.sh \n++ python -m llama_recipes.finetuning --dataset custom_dataset --custom_dataset.file custom_dataset.py --model_name meta-llama/Llama-3.2-1B-Instruct --use_peft --peft_method lora --num_epochs 1 --max_train_step 2 --max_eval_step 3\n/mnt/batch/tasks/shared/LS_root/mounts/clusters/yh-a100/code/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n  from torch.distributed._shard.checkpoint import (\n--> Model meta-llama/Llama-3.2-1B-Instruct\n\n--> meta-llama/Llama-3.2-1B-Instruct has 1235.8144 Million params\n\ntrainable params: 851,968 || all params: 1,236,666,368 || trainable%: 0.0689\nParameter 'function'=<function get_custom_dataset.<locals>.<lambda> at 0x7fe42c924790> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9846/9846 [00:00<00:00, 10038.77 examples/s]\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9846/9846 [00:00<00:00, 23799.51 examples/s]\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 44042/44042 [00:01<00:00, 27242.59 examples/s]\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 44042/44042 [00:52<00:00, 837.00 examples/s]\n--> Training Set Length = 44042\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 518/518 [00:00<00:00, 13515.03 examples/s]\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 518/518 [00:00<00:00, 22014.00 examples/s]\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2241/2241 [00:00<00:00, 31612.27 examples/s]\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2241/2241 [00:02<00:00, 848.35 examples/s]\n--> Validation Set Length = 2241\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 44042/44042 [00:15<00:00, 2856.91it/s]\nlength of dataset_train 3974\nCan not find the custom data_collator in the dataset.py file (custom_dataset.py).\nUsing the default data_collator instead.\n--> Num of Training Set Batches loaded = 993\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2241/2241 [00:00<00:00, 2811.06it/s]\n--> Num of Validation Set Batches loaded = 206\n--> Num of Validation Set Batches loaded = 206\nStarting epoch 0/1\ntrain_config.max_train_step: 2\n/anaconda/envs/llama-dev/lib/python3.10/site-packages/torch/cuda/memory.py:365: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                                                                           | 0/993 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nTraining Epoch: 1/1, step 1/993 completed (loss: 1.5411758422851562):   0%|                                                | 2/993 [00:02<16:33,  1.00s/it]max training steps reached, stopping training, total train steps finished:  2\nTraining Epoch: 1/1, step 1/993 completed (loss: 1.5411758422851562):   0%|                                                | 2/993 [00:02<20:15,  1.23s/it]\nMax CUDA memory allocated was 54 GB\nMax CUDA memory reserved was 55 GB\nPeak active CUDA memory was 54 GB\nCUDA Malloc retries : 0\nCPU Total Peak Memory consumed during the train (max): 4 GB\nevaluating Epoch:   0%|                                                                                                            | 0/206 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nevaluating Epoch:   1%|\u2588\u258d                                                                                                  | 3/206 [00:00<00:22,  9.05it/s]max eval steps reached, stopping evaluation, total_eval_steps:  3\nevaluating Epoch:   1%|\u2588\u258d                                                                                                  | 3/206 [00:00<00:26,  7.54it/s]\n eval_ppl=tensor(1.0240, device='cuda:0') eval_epoch_loss=tensor(0.0237, device='cuda:0')\nwe are about to save the PEFT modules\nPEFT modules are saved in PATH/to/save/PEFT/model directory\nbest eval loss on epoch 1 is 0.02374742180109024\nEpoch 1: train_perplexity=1.0031, train_epoch_loss=0.0031, epoch time 3.264616988000853s\nKey: avg_train_prep, Value: 1.0031403303146362\nKey: avg_train_loss, Value: 0.003135421546176076\nKey: avg_eval_prep, Value: 1.024031639099121\nKey: avg_eval_loss, Value: 0.02374742180109024\nKey: avg_epoch_time, Value: 3.264616988000853\nKey: avg_checkpoint_time, Value: 0.5262662229997659\n```\n\n- [ ] From main, run this script to verify that an error occurs when using the colon to try to set a custom name to replace `get_custom_dataset`. Notice that the function `get_custom_dataset` is called by the codepath that's actually trying to call the data collator. \n\n```\nrecipes/src/llama_recipes/datasets/custom_dataset.py\", line 53, in get_data_collator\n    return getattr(module, func_name)(dataset_processer)\nTypeError: get_custom_dataset() missing 2 required positional arguments: 'tokenizer' and 'split'\n```\n\n```\n(llama-dev) azureuser@yh-a100:~/cloudfiles/code/test_llama$ ./not-working.sh \n++ python -m llama_recipes.finetuning --dataset custom_dataset --custom_dataset.file custom_dataset.py:get_custom_dataset --model_name meta-llama/Llama-3.2-1B-Instruct --use_peft --peft_method lora --num_epochs 1 --max_train_step 2 --max_eval_step 3\n/mnt/batch/tasks/shared/LS_root/mounts/clusters/yh-a100/code/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n  from torch.distributed._shard.checkpoint import (\n--> Model meta-llama/Llama-3.2-1B-Instruct\n\n--> meta-llama/Llama-3.2-1B-Instruct has 1235.8144 Million params\n\ntrainable params: 851,968 || all params: 1,236,666,368 || trainable%: 0.0689\nParameter 'function'=<function get_custom_dataset.<locals>.<lambda> at 0x7f94d384c700> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9846/9846 [00:00<00:00, 10114.99 examples/s]\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9846/9846 [00:00<00:00, 23359.21 examples/s]\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 44042/44042 [00:01<00:00, 28572.73 examples/s]\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 44042/44042 [00:51<00:00, 858.74 examples/s]\n--> Training Set Length = 44042\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 518/518 [00:00<00:00, 13564.99 examples/s]\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 518/518 [00:00<00:00, 21441.96 examples/s]\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2241/2241 [00:00<00:00, 30807.72 examples/s]\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2241/2241 [00:02<00:00, 858.85 examples/s]\n--> Validation Set Length = 2241\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 44042/44042 [00:15<00:00, 2798.59it/s]\nlength of dataset_train 3974\nTraceback (most recent call last):\n  File \"/anaconda/envs/llama-dev/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/anaconda/envs/llama-dev/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/yh-a100/code/llama-recipes/src/llama_recipes/finetuning.py\", line 428, in <module>\n    fire.Fire(main)\n  File \"/anaconda/envs/llama-dev/lib/python3.10/site-packages/fire/core.py\", line 135, in Fire\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n  File \"/anaconda/envs/llama-dev/lib/python3.10/site-packages/fire/core.py\", line 468, in _Fire\n    component, remaining_args = _CallAndUpdateTrace(\n  File \"/anaconda/envs/llama-dev/lib/python3.10/site-packages/fire/core.py\", line 684, in _CallAndUpdateTrace\n    component = fn(*varargs, **kwargs)\n  File \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/yh-a100/code/llama-recipes/src/llama_recipes/finetuning.py\", line 346, in main\n    custom_data_collator = get_custom_data_collator(dataset_processer, dataset_config)\n  File \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/yh-a100/code/llama-recipes/src/llama_recipes/utils/dataset_utils.py\", line 36, in get_custom_data_collator\n    return DATALOADER_COLLATE_FUNC[dataset_config.dataset](\n  File \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/yh-a100/code/llama-recipes/src/llama_recipes/datasets/custom_dataset.py\", line 53, in get_data_collator\n    return getattr(module, func_name)(dataset_processer)\nTypeError: get_custom_dataset() missing 2 required positional arguments: 'tokenizer' and 'split'\n```\n\n- [ ] Finally, from this PR's branch, we run the same command-line flags as above. notice the key line, indicating that the code is searching for a `get_data_collator` function, instead of `get_custom_dataset`, does not find it, and uses the default. Then it successfully fine tunes the model using the custom dataset.\n\n```\nCan not find the custom data_collator in the dataset.py file (custom_dataset.py).\nUsing the default data_collator instead.\n```\n\n```\n(llama-dev) azureuser@yh-a100:~/cloudfiles/code/test_llama$ ./not-working.sh \n++ python -m llama_recipes.finetuning --dataset custom_dataset --custom_dataset.file custom_dataset.py:get_custom_dataset --model_name meta-llama/Llama-3.2-1B-Instruct --use_peft --peft_method lora --num_epochs 1 --max_train_step 2 --max_eval_step 3\n/mnt/batch/tasks/shared/LS_root/mounts/clusters/yh-a100/code/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n  from torch.distributed._shard.checkpoint import (\n--> Model meta-llama/Llama-3.2-1B-Instruct\n\n--> meta-llama/Llama-3.2-1B-Instruct has 1235.8144 Million params\n\ntrainable params: 851,968 || all params: 1,236,666,368 || trainable%: 0.0689\nParameter 'function'=<function get_custom_dataset.<locals>.<lambda> at 0x7feb0de7c790> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9846/9846 [00:00<00:00, 10401.56 examples/s]\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9846/9846 [00:00<00:00, 26194.96 examples/s]\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 44042/44042 [00:01<00:00, 29545.34 examples/s]\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 44042/44042 [00:51<00:00, 847.30 examples/s]\n--> Training Set Length = 44042\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 518/518 [00:00<00:00, 12966.32 examples/s]\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 518/518 [00:00<00:00, 20466.57 examples/s]\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2241/2241 [00:00<00:00, 27766.67 examples/s]\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2241/2241 [00:02<00:00, 801.33 examples/s]\n--> Validation Set Length = 2241\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 44042/44042 [00:15<00:00, 2846.23it/s]\nlength of dataset_train 3974\nCan not find the custom data_collator in the dataset.py file (custom_dataset.py).\nUsing the default data_collator instead.\n--> Num of Training Set Batches loaded = 993\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2241/2241 [00:00<00:00, 2736.16it/s]\n--> Num of Validation Set Batches loaded = 206\n--> Num of Validation Set Batches loaded = 206\nStarting epoch 0/1\ntrain_config.max_train_step: 2\n/anaconda/envs/llama-dev/lib/python3.10/site-packages/torch/cuda/memory.py:365: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                                                                           | 0/993 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nTraining Epoch: 1/1, step 1/993 completed (loss: 1.5411139726638794):   0%|                                                | 2/993 [00:02<16:21,  1.01it/s]max training steps reached, stopping training, total train steps finished:  2\nTraining Epoch: 1/1, step 1/993 completed (loss: 1.5411139726638794):   0%|                                                | 2/993 [00:02<19:56,  1.21s/it]\nMax CUDA memory allocated was 54 GB\nMax CUDA memory reserved was 55 GB\nPeak active CUDA memory was 54 GB\nCUDA Malloc retries : 0\nCPU Total Peak Memory consumed during the train (max): 4 GB\nevaluating Epoch:   0%|                                                                                                            | 0/206 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nevaluating Epoch:   1%|\u2588\u258d                                                                                                  | 3/206 [00:00<00:22,  9.08it/s]max eval steps reached, stopping evaluation, total_eval_steps:  3\nevaluating Epoch:   1%|\u2588\u258d                                                                                                  | 3/206 [00:00<00:26,  7.54it/s]\n eval_ppl=tensor(1.0240, device='cuda:0') eval_epoch_loss=tensor(0.0238, device='cuda:0')\nwe are about to save the PEFT modules\nPEFT modules are saved in PATH/to/save/PEFT/model directory\nbest eval loss on epoch 1 is 0.0237506702542305\nEpoch 1: train_perplexity=1.0031, train_epoch_loss=0.0031, epoch time 3.2322842220000894s\nKey: avg_train_prep, Value: 1.0031403303146362\nKey: avg_train_loss, Value: 0.0031353593803942204\nKey: avg_eval_prep, Value: 1.0240349769592285\nKey: avg_eval_loss, Value: 0.0237506702542305\nKey: avg_epoch_time, Value: 3.2322842220000894\nKey: avg_checkpoint_time, Value: 0.5249914010000793\n```\n\n\n## Before submitting\n- [Y] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [Y] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [Y] Did you make sure to update the documentation with your changes?  \n- [N/A] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "183": "bugfix: update tqdm bar with the fixed gradient_accumulation_steps# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\nWe should use `gradient_accumulation_steps` to call `pbar.update`, instead of the current progress. tqdm will automatically calculate the progress. \n\nOfficial doc link\uff1ahttps://github.com/tqdm/tqdm#manual\n\n<details >\n<summary>The following code can be used to validate this conclusion.</summary>\n\n```\nfrom tqdm import tqdm\nimport time\n\nnum_epochs = 2\nfor epoch in range(num_epochs):\n    total_length = 16000\n    pbar = tqdm(colour=\"blue\", desc=f\"Training Epoch: {epoch}\", total=total_length)\n    for i in range(total_length):\n        if i % 100 == 0:\n            print(i, pbar.total)\n            # bad\n            # pbar.update(i)\n            pbar.update(100)\n        time.sleep(0.001)\n    pbar.set_description(f\"Training Epoch: {epoch}/{num_epochs} completed {i}\")\n    epoch += 1\n```\n\n</details>\n\n<!-- Remove if not applicable -->\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "746": "Small notes on next stepsAdded some small notes on next steps and suggested improvements", "791": "add freeze_LLM_only option for mllama finetuning# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes #770 \n\n## Feature/Issue Validation/Testing\n\nTo follow the training settings in the original paper, as mentioned in issue #770, I added a new function to tune the vision encoder, projector, and cross-attention layers inside the LLM. By setting `train_config.freeze_LLM_only` to `True`, you can enable this functionality.\n\nI conducted two tests: \n1. Using `test_finetuning.py`.\n2. Running the finetuning script `finetuning.py` directly.\n\nBoth tests passed successfully. In detail, I ran the finetuning process on 8\u00d7H100 GPUs. The process was smooth, as shown below.\n\n- [x] python -m pytest src/tests/test_finetuning.py\n```\n=============================================================== test session starts ===============================================================\nplatform linux -- Python 3.11.9, pytest-8.3.3, pluggy-1.5.0\nrootdir: /media/Pluto/jim/opensource_contribute/llama-recipes\nconfigfile: pyproject.toml\nplugins: mock-3.14.0, anyio-4.6.2.post1\ncollected 22 items                                                                                                                                \n\nsrc/tests/test_finetuning.py ......................                                                                                         [100%]\n\n================================================================ warnings summary =================================================================\n../../llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17\n  /media/Pluto/jim/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n    from torch.distributed._shard.checkpoint import (\n\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n========================================================== 22 passed, 1 warning in 3.76s ==========================================================\n```\n\n- [x] torchrun --nnodes 1 --nproc_per_node 8  recipes/quickstart/finetuning/finetuning.py --enable_fsdp --lr 1e-5  --num_epochs 3 --batch_size_training 2 --model_name meta-llama/Llama-3.2-11B-Vision-Instruct --dist_checkpoint_root_folder ./finetuned_model --dist_checkpoint_folder fine-tuned  --use_fast_kernels --dataset \"custom_dataset\" --custom_dataset.test_split \"test\" --custom_dataset.file \"recipes/quickstart/finetuning/datasets/ocrvqa_dataset.py\"  --run_validation True --batching_strategy padding \u2014freeze_LLM_only True\n```\nW1116 16:26:22.743000 23456244184896 torch/distributed/run.py:757]\nW1116 16:26:22.743000 23456244184896 torch/distributed/run.py:757] *****************************************\nW1116 16:26:22.743000 23456244184896 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.\nW1116 16:26:22.743000 23456244184896 torch/distributed/run.py:757] *****************************************\nin oss file\nin oss file\nin oss file\nin oss file\nin oss file\nin oss file\nin oss file\nin oss file\nClearing GPU cache for all ranks\n--> Running with torch dist debug set to detail\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 10.94it/s]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 12.16it/s]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00,  9.53it/s]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00,  7.23it/s]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 11.39it/s]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00,  7.16it/s]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00,  7.32it/s]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00,  8.04it/s]\nbFloat16 enabled for mixed precision - using bfSixteen policy\n--> applying fsdp activation checkpointing...\n--> Model meta-llama/Llama-3.2-11B-Vision-Instruct\n\n--> meta-llama/Llama-3.2-11B-Vision-Instruct has 1333.777617 Million params\n\nREADME.md: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50.3k/50.3k [00:00<00:00, 1.11MB/s]\n--> applying fsdp activation checkpointing...\n--> applying fsdp activation checkpointing...\n--> applying fsdp activation checkpointing...\n--> applying fsdp activation checkpointing...\n--> applying fsdp activation checkpointing...\n--> applying fsdp activation checkpointing...\n--> applying fsdp activation checkpointing...\n(\u2026)-00000-of-00011-f83c2bdf2cf711bf.parquet: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 540M/540M [00:12<00:00, 42.0MB/s]\n(\u2026)-00001-of-00011-fef40eeeea84a563.parquet: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 580M/580M [00:13<00:00, 42.1MB/s]\n(\u2026)-00002-of-00011-c0733bedbcc41420.parquet: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 541M/541M [00:12<00:00, 42.3MB/s]\n(\u2026)-00003-of-00011-fee117dc7680fb5f.parquet: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 577M/577M [00:13<00:00, 41.2MB/s]\n(\u2026)-00004-of-00011-c01c965b3ac5c2c0.parquet:  47%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d         | 273M/581M [00:06<00:07, 42.7MB/s](\u2026)-00004-of-00011-c01c965b3ac5c2c0.parquet:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e      | 367M/581M [00:08<00:04, 47.1MB/s](\u2026)-00004-of-00011-c01c965b3ac5c2c0.parquet: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 581M/581M [00:13<00:00, 42.6MB/s]\n(\u2026)-00005-of-00011-7eb79ee48c0c4065.parquet: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 527M/527M [00:12<00:00, 42.6MB/s]\n(\u2026)-00006-of-00011-4a139e7c78fb5e47.parquet: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 519M/519M [00:12<00:00, 41.5MB/s]\n(\u2026)-00007-of-00011-8f649db4d5664766.parquet: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 559M/559M [00:24<00:00, 22.5MB/s]\n(\u2026)-00008-of-00011-23185b703995741f.parquet: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 555M/555M [00:13<00:00, 42.6MB/s]\n(\u2026)-00009-of-00011-b0bb42debccbf310.parquet: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 519M/519M [00:22<00:00, 22.7MB/s]\n(\u2026)-00010-of-00011-74ed380c1a2c83aa.parquet: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 579M/579M [00:14<00:00, 41.0MB/s]\nGenerating train split: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 165746/165746 [00:06<00:00, 27228.77 examples/s]\n--> Training Set Length = 1800\n--> Validation Set Length = 200\nlength of dataset_train 1800\ncustom_data_collator is used\n--> Num of Training Set Batches loaded = 112\nlength of dataset_train 1800\ncustom_data_collator is used\n--> Num of Training Set Batches loaded = 112\nlength of dataset_train 1800\ncustom_data_collator is used\n--> Num of Training Set Batches loaded = 112\n--> Num of Validation Set Batches loaded = 25\n--> Num of Validation Set Batches loaded = 25\nStarting epoch 0/3\ntrain_config.max_train_step: 0\nlength of dataset_train 1800\ncustom_data_collator is used\n--> Num of Training Set Batches loaded = 112\n/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                       | 0/112 [00:00<?, ?it/s]length of dataset_train 1800\ncustom_data_collator is used\n--> Num of Training Set Batches loaded = 112\n--> Num of Validation Set Batches loaded = 25\n--> Num of Validation Set Batches loaded = 25\nStarting epoch 0/3\ntrain_config.max_train_step: 0\nlength of dataset_train 1800\ncustom_data_collator is used\n--> Num of Training Set Batches loaded = 112\n/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                       | 0/112 [00:00<?, ?it/s]--> Num of Validation Set Batches loaded = 25\n--> Num of Validation Set Batches loaded = 25\nStarting epoch 0/3\ntrain_config.max_train_step: 0\n--> Num of Validation Set Batches loaded = 25\n--> Num of Validation Set Batches loaded = 25\nStarting epoch 0/3\ntrain_config.max_train_step: 0\n--> Num of Validation Set Batches loaded = 25\n--> Num of Validation Set Batches loaded = 25\nStarting epoch 0/3\ntrain_config.max_train_step: 0\n/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                       | 0/112 [00:00<?, ?it/s]length of dataset_train 1800\ncustom_data_collator is used\n--> Num of Training Set Batches loaded = 112\n/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                       | 0/112 [00:00<?, ?it/s]--> Num of Validation Set Batches loaded = 25\n--> Num of Validation Set Batches loaded = 25\nStarting epoch 0/3\ntrain_config.max_train_step: 0\nlength of dataset_train 1800\ncustom_data_collator is used\n--> Num of Training Set Batches loaded = 112\n/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                       | 0/112 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                       | 0/112 [00:00<?, ?it/s]--> Num of Validation Set Batches loaded = 25\n--> Num of Validation Set Batches loaded = 25\nStarting epoch 0/3\ntrain_config.max_train_step: 0\n--> Num of Validation Set Batches loaded = 25\n--> Num of Validation Set Batches loaded = 25\nStarting epoch 0/3\ntrain_config.max_train_step: 0\n/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                       | 0/112 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                       | 0/112 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\nTraining Epoch: 1/3, step 19/112 completed (loss: 0.032936301082372665):  18%|\u258f| 20/112 [00:48<02:44,  Training Epoch: 1/3, step 20/112 completed (loss: 0.03712736815214157):  19%|\u258f| 21/112 [00:50<02:42,  1Training Epoch: 1/3, step 22/112 completed (loss: 0.11487767100334167):  21%|\u258f| 23/112 [00:53<02:38,  \n```\n\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [x] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [x] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "578": "Add README for quickstart + update to codellama urlPart of the overall refactor", "544": "Correct the correct urlThis PR fixes a deprecated URL in Jupyter note to point to a valid URL.", "550": "     Initial work on customizing LG# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "587": "changed --pure_bf16 to --fsdp_config.pure_bf16 and corrected \"examples/\" path\n# What does this PR do?\nThis PR changed `--pure_bf16` to `--fsdp_config.pure_bf16` on all related documents, because `--pure_bf16` will give incorrect warning `Warning: unknown parameter pure_bf16` as mentioned in [this issue](https://github.com/meta-llama/llama-recipes/issues/396) . This PR also corrected any path that contains \"examples/\" as example folder no long exist. \n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes #[Issue 396](https://github.com/meta-llama/llama-recipes/issues/396)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] test log using `--fsdp_config.pure_bf16` did not give any warning\n```\ntorchrun --nnodes 1 --nproc_per_node 4 recipes/quickstart/finetuning/finetuning.py --enable_fsdp  --fsdp_config.pure_bf16 --model_name meta-Llama/Meta-Llama-3-8B-Instruct  --batch_size_training 1 --dist_checkpoint_root_folder model_checkpoints --dist_checkpoint_folder fine-tuned\nW0701 12:29:35.203000 139805787378688 torch/distributed/run.py:757] \nW0701 12:29:35.203000 139805787378688 torch/distributed/run.py:757] *****************************************\nW0701 12:29:35.203000 139805787378688 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0701 12:29:35.203000 139805787378688 torch/distributed/run.py:757] *****************************************\nClearing GPU cache for all ranks\n--> Running with torch dist debug set to detail\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:10<00:00,  2.62s/it]\nLoading checkpoint shards:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                            | 3/4 [00:09<00:03,  3.32s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n--> Model meta-Llama/Meta-Llama-3-8B-Instruct\n\n--> meta-Llama/Meta-Llama-3-8B-Instruct has 8030.261248 Million params\n\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:10<00:00,  2.68s/it]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:10<00:00,  2.69s/it]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:10<00:00,  2.74s/it]\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nbFloat16 enabled for mixed precision - using bfSixteen policy\n--> applying fsdp activation checkpointing...\n--> applying fsdp activation checkpointing...\n--> applying fsdp activation checkpointing...\n--> applying fsdp activation checkpointing...\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:04<00:00, 2995.08 examples/s]\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:04<00:00, 2968.07 examples/s]\nMap:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e         | 13514/14732 [00:04<00:00, 3054.14 examples/s]--> Training Set Length = 14732\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:04<00:00, 3033.62 examples/s]\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:04<00:00, 3037.89 examples/s]\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 818/818 [00:00<00:00, 2977.27 examples/s]\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 818/818 [00:00<00:00, 2924.48 examples/s]\n--> Validation Set Length = 818\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:02<00:00, 6988.80it/s]\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:02<00:00, 7130.44it/s]\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 818/818 [00:00<00:00, 7249.96it/s]\n--> Num of Validation Set Batches loaded = 8\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 818/818 [00:00<00:00, 7357.13it/s]\n--> Num of Validation Set Batches loaded = 8\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:02<00:00, 7015.78it/s]\nPreprocessing dataset:   0%|                                                                                                                          | 0/818 [00:00<?, ?it/s]/home/kaiwu/miniconda3/envs/raft/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nPreprocessing dataset:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c          | 740/818 [00:00<00:00, 7388.87it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 818/818 [00:00<00:00, 7397.74it/s]\n--> Num of Validation Set Batches loaded = 8\nPreprocessing dataset:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 14525/14732 [00:02<00:00, 7039.20it/s]/home/kaiwu/miniconda3/envs/raft/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:02<00:00, 7098.22it/s]\nPreprocessing dataset:   0%|                                                                                                                          | 0/818 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 818/818 [00:00<00:00, 7368.40it/s]\n--> Num of Validation Set Batches loaded = 8\n/home/kaiwu/miniconda3/envs/raft/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                                                                                              | 0/159 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/home/kaiwu/miniconda3/envs/raft/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                                                                                              | 0/159 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nNCCL version 2.20.5+cuda12.4\nTraining Epoch: 1/3, step 60/159 completed (loss: 6.978939533233643):  38%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                        | 61/159 [00:59<01:24,  1.16it/s]\n```\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "593": "colab links fixed for dlai agents notebooks# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "597": "Fix broken image link# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "583": "Fix broken images in LLM finetuning overview", "568": "RAFT fine-tuning technique with Deep Lake Dataloader# What does this PR do?\n\nI added the capability to fine-tune an LLM using RAFT, a technique designed to create a dataset that helps the model better distinguish important context from irrelevant information ([paper here](https://arxiv.org/pdf/2403.10131v1)).\n\nI followed the process used to create the Alpaca dataset and adapted it to my needs. The dataset can be saved in Deep Lake and accessed directly during training.\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\nI encounter a problem when I try to run tests, even those from the official repository. Running `python -m pytest src/tests/` generates the following errors. What am I doing wrong?\n![image](https://github.com/meta-llama/llama-recipes/assets/32709108/534e56ad-db02-4a48-958b-2b8357c10e20)\n\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [X] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "554": "Adding Torchtune recipe for fine-tuning# What does this PR do?\n\nAdding Torchtune recipe for fine-tuning so that the users can now use torchtune to fine-tune Llama models\n\n## Feature/Issue validation/testing\n\n\n\n- [x] Followed this document to use torchtune.\n```\n~/work/torchtune/recipes/configs (main)]$ tune run lora_finetune_single_device --config llama3/8B_qlora_single_device\nINFO:torchtune.utils.logging:Running LoRAFinetuneRecipeSingleDevice with resolved config:\n\nbatch_size: 2\ncheckpointer:\n  _component_: torchtune.utils.FullModelMetaCheckpointer\n  checkpoint_dir: /tmp/Meta-Llama-3-8B/original/\n  checkpoint_files:\n  - consolidated.00.pth\n  model_type: LLAMA3\n  output_dir: /tmp/Meta-Llama-3-8B/\n  recipe_checkpoint: null\ncompile: false\ndataset:\n  _component_: torchtune.datasets.alpaca_cleaned_dataset\n  train_on_input: true\ndevice: cuda\ndtype: bf16\nenable_activation_checkpointing: true\nepochs: 1\ngradient_accumulation_steps: 16\nlog_every_n_steps: 1\nloss:\n  _component_: torch.nn.CrossEntropyLoss\nlr_scheduler:\n  _component_: torchtune.modules.get_cosine_schedule_with_warmup\n  num_warmup_steps: 100\nmax_steps_per_epoch: null\nmetric_logger:\n  _component_: torchtune.utils.metric_logging.DiskLogger\n  log_dir: /tmp/qlora_finetune_output/\nmodel:\n  _component_: torchtune.models.llama3.qlora_llama3_8b\n  apply_lora_to_mlp: true\n  apply_lora_to_output: false\n  lora_alpha: 16\n  lora_attn_modules:\n  - q_proj\n  - v_proj\n  - k_proj\n  - output_proj\n  lora_rank: 8\noptimizer:\n  _component_: torch.optim.AdamW\n  lr: 0.0003\n  weight_decay: 0.01\noutput_dir: /tmp/qlora_finetune_output/\nprofiler:\n  _component_: torchtune.utils.profiler\n  enabled: false\nresume_from_checkpoint: false\nseed: null\nshuffle: true\ntokenizer:\n  _component_: torchtune.models.llama3.llama3_tokenizer\n  path: /tmp/Meta-Llama-3-8B/original/tokenizer.model\n\nDEBUG:torchtune.utils.logging:Setting manual seed to local seed 4023430205. Local seed is seed + rank = 4023430205 + 0\nWriting logs to /tmp/qlora_finetune_output/log_1717628124.txt\nINFO:torchtune.utils.logging:Model is initialized with precision torch.bfloat16.\nINFO:torchtune.utils.logging:Memory Stats after model init:\n{'peak_memory_active': 10.126081536, 'peak_memory_alloc': 10.126081536, 'peak_memory_reserved': 12.639535104}\nINFO:torchtune.utils.logging:Tokenizer is initialized from file.\nINFO:torchtune.utils.logging:Optimizer and loss are initialized.\nINFO:torchtune.utils.logging:Loss is initialized.\nDownloading readme: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11.6k/11.6k [00:00<00:00, 56.7MB/s]\nDownloading data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 44.3M/44.3M [00:00<00:00, 88.3MB/s]\nGenerating train split: 51760 examples [00:00, 214648.47 examples/s]\nINFO:torchtune.utils.logging:Dataset and Sampler are initialized.\nINFO:torchtune.utils.logging:Learning rate scheduler is initialized.\n1|33|Loss: 1.6673557758331299:   0%|                                                   | 32/25880 [00:24<5:37:10,  1.28it/s]1|33|Loss: 1.6673557758331299:   0%|                                                   | 32/25880 [00:25<5:38:58,  1.27it/s]\n```\n\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\n\n", "540": "fixed alpaca dataset evalset length and make sure len(eval_loader)>0# What does this PR do?\nFixed alpaca dataset evalset length by using 5% of the dataset as evalset so the len(eval_dataloader) >0, that means at least one batch can be loaded by dataloader. Also added a check to make sure len(eval_dataloader)>0 when run_validation=True, otherwise raise error and stop training.\n\nThis problem is raised by [issue 520](https://github.com/meta-llama/llama-recipes/issues/520)\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [x] Evaluation step working in the alpaca finetuning\n```\n~/work/llama-recipes (fix/eval_dataloader_not_loaded)]$ torchrun --rdzv-endpoint=localhost:0 --rdzv-id=111223 --nnodes 1 --nproc_per_node 8 --rdzv-backend=c10d recipes/finetuning/finetuning.py --enable_fsdp --dataset alpaca_dataset --model_name meta-llama/Meta-Llama-3-8B --use_peft --peft_method lora --output_dir PEFT_model --max_train_step 2\nW0523 10:30:35.612000 139639579272192 torch/distributed/run.py:757] \nW0523 10:30:35.612000 139639579272192 torch/distributed/run.py:757] *****************************************\nW0523 10:30:35.612000 139639579272192 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0523 10:30:35.612000 139639579272192 torch/distributed/run.py:757] *****************************************\nClearing GPU cache for all ranks\n--> Running with torch dist debug set to detail\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:13<00:00,  3.34s/it]\nLoading checkpoint shards:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                           | 3/4 [00:12<00:04,  4.11s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n--> Model meta-llama/Meta-Llama-3-8B\n\n--> meta-llama/Meta-Llama-3-8B has 8030.261248 Million params\n\ntrainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\nbFloat16 enabled for mixed precision - using bfSixteen policy\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:13<00:00,  3.27s/it]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:13<00:00,  3.25s/it]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:13<00:00,  3.34s/it]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:13<00:00,  3.35s/it]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:13<00:00,  3.36s/it]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:13<00:00,  3.36s/it]\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:13<00:00,  3.40s/it]\ntrainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\ntrainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\ntrainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\ntrainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\ntrainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\ntrainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\ntrainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n--> applying fsdp activation checkpointing...\nPreprocessing dataset:   0%|\u258d                                                                                                         | 210/49402 [00:00<00:23, 2093.11it/s]--> applying fsdp activation checkpointing...\nPreprocessing dataset:   1%|\u2588                                                                                                         | 481/49402 [00:00<00:19, 2453.73it/s]--> applying fsdp activation checkpointing...\nPreprocessing dataset:   0%|                                                                                                                      | 0/49402 [00:00<?, ?it/s]--> applying fsdp activation checkpointing...\nPreprocessing dataset:   2%|\u2588\u258c                                                                                                        | 747/49402 [00:00<00:19, 2547.50it/s]--> applying fsdp activation checkpointing...\nPreprocessing dataset:   0%|                                                                                                                      | 0/49402 [00:00<?, ?it/s]--> applying fsdp activation checkpointing...\n--> Training Set Length = 49402\nPreprocessing dataset:   3%|\u2588\u2588\u258b                                                                                                      | 1281/49402 [00:00<00:18, 2625.27it/s]--> Validation Set Length = 2600\nPreprocessing dataset:   1%|\u2588                                                                                                         | 487/49402 [00:00<00:19, 2479.20it/s]--> applying fsdp activation checkpointing...\n--> applying fsdp activation checkpointing...\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 49402/49402 [00:18<00:00, 2701.83it/s]\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 49402/49402 [00:18<00:00, 2724.37it/s]\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 49402/49402 [00:18<00:00, 2729.47it/s]\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 49402/49402 [00:18<00:00, 2664.44it/s]\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 49402/49402 [00:18<00:00, 2681.45it/s]\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 49402/49402 [00:18<00:00, 2702.83it/s]\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 49402/49402 [00:18<00:00, 2669.26it/s]\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2600/2600 [00:00<00:00, 2757.76it/s]\n--> Num of Validation Set Batches loaded = 7\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2600/2600 [00:00<00:00, 2805.26it/s]\n--> Num of Validation Set Batches loaded = 7\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 49402/49402 [00:18<00:00, 2646.96it/s]\nPreprocessing dataset:  43%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                                            | 1110/2600 [00:00<00:00, 2724.02it/s]/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2600/2600 [00:00<00:00, 2804.79it/s]\n--> Num of Validation Set Batches loaded = 7\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nPreprocessing dataset:  53%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                                 | 1383/2600 [00:00<00:00, 2695.70it/s]/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                                                                                             | 0/39 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nPreprocessing dataset:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                      | 1653/2600 [00:00<00:00, 2677.75it/s]/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nPreprocessing dataset:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                           | 1929/2600 [00:00<00:00, 2701.31it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2600/2600 [00:00<00:00, 2733.80it/s]\n--> Num of Validation Set Batches loaded = 7\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2600/2600 [00:00<00:00, 2745.22it/s]\n--> Num of Validation Set Batches loaded = 7\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2600/2600 [00:00<00:00, 2729.47it/s]\n--> Num of Validation Set Batches loaded = 7\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2600/2600 [00:00<00:00, 2703.83it/s]\n--> Num of Validation Set Batches loaded = 7\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nPreprocessing dataset:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                             | 1876/2600 [00:00<00:00, 2641.07it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nPreprocessing dataset:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                  | 2147/2600 [00:00<00:00, 2660.76it/s]/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                                                                                             | 0/39 [00:00<?, ?it/s]/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                                                                                             | 0/39 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nPreprocessing dataset:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a       | 2423/2600 [00:00<00:00, 2689.39it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2600/2600 [00:00<00:00, 2672.48it/s]\n--> Num of Validation Set Batches loaded = 7\nNCCL version 2.20.5+cuda12.4\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                                                                                             | 0/39 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nTraining Epoch: 1/3, step 1/39 completed (loss: 1.6738725900650024):   5%|\u2588\u2588\u2588\u258d                                                               | 2/39 [00:16<04:29,  7.28s/it]max training steps reached, stopping training, total train steps finished:  2\nTraining Epoch: 1/3, step 1/39 completed (loss: 1.6738725900650024):   5%|\u2588\u2588\u2588\u258d                                                               | 2/39 [00:16<05:08,  8.33s/it]\nTraining Epoch: 1/3, step 1/39 completed (loss: 1.5186713933944702):   5%|\u2588\u2588\u2588\u258d                                                               | 2/39 [00:17<05:22,  8.72s/it]\nTraining Epoch: 1/3, step 1/39 completed (loss: 1.543288230895996):   5%|\u2588\u2588\u2588\u258d                                                                | 2/39 [00:16<05:09,  8.37s/it]\nTraining Epoch: 1/3, step 1/39 completed (loss: 1.486218810081482):   5%|\u2588\u2588\u2588\u258d                                                                | 2/39 [00:16<05:12,  8.43s/it]\nTraining Epoch: 1/3, step 1/39 completed (loss: 1.473335862159729):   5%|\u2588\u2588\u2588\u258d                                                                | 2/39 [00:16<05:10,  8.38s/it]\nTraining Epoch: 1/3, step 1/39 completed (loss: 1.4909212589263916):   5%|\u2588\u2588\u2588\u258d                                                               | 2/39 [00:17<05:20,  8.65s/it]\nTraining Epoch: 1/3, step 1/39 completed (loss: 1.5622209310531616):   5%|\u2588\u2588\u2588\u258d                                                               | 2/39 [00:16<04:59,  8.09s/it]\nTraining Epoch: 1/3, step 1/39 completed (loss: 1.5295921564102173):   5%|\u2588\u2588\u2588\u258d                                                               | 2/39 [00:17<05:25,  8.80s/it]\nMax CUDA memory allocated was 34 GB\nMax CUDA memory reserved was 41 GB\nPeak active CUDA memory was 34 GB\nCUDA Malloc retries : 0\nCPU Total Peak Memory consumed during the train (max): 9 GB\nevaluating Epoch:   0%|                                                                                                                               | 0/7 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nevaluating Epoch:   0%|                                                                                                                               | 0/7 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7/7 [00:02<00:00,  3.16it/s]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7/7 [00:02<00:00,  3.16it/s]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7/7 [00:02<00:00,  3.18it/s]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7/7 [00:02<00:00,  3.22it/s]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7/7 [00:02<00:00,  3.15it/s]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7/7 [00:02<00:00,  3.22it/s]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7/7 [00:02<00:00,  3.22it/s]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7/7 [00:02<00:00,  3.14it/s]\n eval_ppl=tensor(4.4443, device='cuda:0') eval_epoch_loss=tensor(1.4916, device='cuda:0')\nwe are about to save the PEFT modules\nPEFT modules are saved in PEFT_model directory\nbest eval loss on epoch 1 is 1.4916136264801025\nEpoch 1: train_perplexity=1.0835, train_epoch_loss=0.0802, epoch time 17.234853900037706s\nKey: avg_train_prep, Value: 1.0834850072860718\nKey: avg_train_loss, Value: 0.08018267154693604\nKey: avg_eval_prep, Value: 4.444261074066162\nKey: avg_eval_loss, Value: 1.4916136264801025\nKey: avg_epoch_time, Value: 17.234853900037706\nKey: avg_checkpoint_time, Value: 0.6513841110281646\n```\n\n- [x] Manually cause len(eval_loader) == 0, error will be raised\n```\n~/work/llama-recipes (fix/eval_dataloader_not_loaded)]$ torchrun --rdzv-endpoint=localhost:0 --rdzv-id=111223 --nnodes 1 --nproc_per_node 8 --rdzv-backend=c10d recipes/finetuning/finetuning.py --enable_fsdp --dataset alpaca_dataset --model_name meta-llama/Meta-Llama-3-8B --use_peft --peft_method lora --output_dir PEFT_model --max_train_step 2\nW0523 10:40:23.991000 139842235315200 torch/distributed/run.py:757] \nW0523 10:40:23.991000 139842235315200 torch/distributed/run.py:757] *****************************************\nW0523 10:40:23.991000 139842235315200 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0523 10:40:23.991000 139842235315200 torch/distributed/run.py:757] *****************************************\nClearing GPU cache for all ranks\n--> Running with torch dist debug set to detail\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:13<00:00,  3.41s/it]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:12<00:00,  3.20s/it]\nLoading checkpoint shards:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                           | 3/4 [00:12<00:04,  4.31s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n--> Model meta-llama/Meta-Llama-3-8B\n\n--> meta-llama/Meta-Llama-3-8B has 8030.261248 Million params\n\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\ntrainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\nbFloat16 enabled for mixed precision - using bfSixteen policy\ntrainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:13<00:00,  3.46s/it]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:13<00:00,  3.49s/it]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:14<00:00,  3.50s/it]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:13<00:00,  3.48s/it]\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:14<00:00,  3.55s/it]\ntrainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:14<00:00,  3.56s/it]\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\ntrainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\ntrainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\ntrainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\ntrainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\ntrainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n--> applying fsdp activation checkpointing...\nPreprocessing dataset:   2%|\u2588\u2588                                                                                                       | 1038/51802 [00:00<00:19, 2666.96it/s]--> applying fsdp activation checkpointing...\nPreprocessing dataset:   3%|\u2588\u2588\u258b                                                                                                      | 1316/51802 [00:00<00:18, 2703.66it/s]--> applying fsdp activation checkpointing...\nPreprocessing dataset:   3%|\u2588\u2588\u2588\u258f                                                                                                     | 1587/51802 [00:00<00:18, 2687.06it/s]--> Training Set Length = 51802\nPreprocessing dataset:   4%|\u2588\u2588\u2588\u258a                                                                                                     | 1863/51802 [00:00<00:18, 2710.21it/s]--> applying fsdp activation checkpointing...\n--> Validation Set Length = 200\nPreprocessing dataset:   0%|                                                                                                                      | 0/51802 [00:00<?, ?it/s]--> applying fsdp activation checkpointing...\nPreprocessing dataset:   4%|\u2588\u2588\u2588\u2588\u258e                                                                                                    | 2152/51802 [00:00<00:17, 2765.31it/s]--> applying fsdp activation checkpointing...\nPreprocessing dataset:   1%|\u2589                                                                                                         | 485/51802 [00:00<00:20, 2476.53it/s]--> applying fsdp activation checkpointing...\n--> applying fsdp activation checkpointing...\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51802/51802 [00:19<00:00, 2657.61it/s]\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [00:00<00:00, 2701.83it/s]\n[rank7]: Traceback (most recent call last):\n[rank7]:   File \"/home/kaiwu/work/llama-recipes/recipes/finetuning/finetuning.py\", line 8, in <module>\n[rank7]:     fire.Fire(main)\n[rank7]:   File \"/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/fire/core.py\", line 143, in Fire\n[rank7]:     component_trace = _Fire(component, args, parsed_flag_args, context, name)\n[rank7]:   File \"/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/fire/core.py\", line 477, in _Fire\n[rank7]:     component, remaining_args = _CallAndUpdateTrace(\n[rank7]:   File \"/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/fire/core.py\", line 693, in _CallAndUpdateTrace\n[rank7]:     component = fn(*varargs, **kwargs)\n[rank7]:   File \"/home/kaiwu/work/llama-recipes/src/llama_recipes/finetuning.py\", line 254, in main\n[rank7]:     raise ValueError(\"The eval set size is too small for dataloader to load even one batch. Please increase the size of eval set.\")\n[rank7]: ValueError: The eval set size is too small for dataloader to load even one batch. Please increase the size of eval set.\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51802/51802 [00:19<00:00, 2690.56it/s]\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51802/51802 [00:19<00:00, 2684.25it/s]\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [00:00<00:00, 2745.95it/s]\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/home/kaiwu/work/llama-recipes/recipes/finetuning/finetuning.py\", line 8, in <module>\n[rank0]:     fire.Fire(main)\n[rank0]:   File \"/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/fire/core.py\", line 143, in Fire\n[rank0]:     component_trace = _Fire(component, args, parsed_flag_args, context, name)\n[rank0]:   File \"/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/fire/core.py\", line 477, in _Fire\n[rank0]:     component, remaining_args = _CallAndUpdateTrace(\n[rank0]:   File \"/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/fire/core.py\", line 693, in _CallAndUpdateTrace\n[rank0]:     component = fn(*varargs, **kwargs)\n[rank0]:   File \"/home/kaiwu/work/llama-recipes/src/llama_recipes/finetuning.py\", line 254, in main\n[rank0]:     raise ValueError(\"The eval set size is too small for dataloader to load even one batch. Please increase the size of eval set.\")\n[rank0]: ValueError: The eval set size is too small for dataloader to load even one batch. Please increase the size of eval set.\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51802/51802 [00:18<00:00, 2732.65it/s]\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51802/51802 [00:19<00:00, 2696.89it/s]\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51802/51802 [00:19<00:00, 2704.91it/s]\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51802/51802 [00:19<00:00, 2714.17it/s]\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [00:00<00:00, 2705.17it/s]\n[rank5]: Traceback (most recent call last):\n[rank5]:   File \"/home/kaiwu/work/llama-recipes/recipes/finetuning/finetuning.py\", line 8, in <module>\n[rank5]:     fire.Fire(main)\n[rank5]:   File \"/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/fire/core.py\", line 143, in Fire\n[rank5]:     component_trace = _Fire(component, args, parsed_flag_args, context, name)\n[rank5]:   File \"/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/fire/core.py\", line 477, in _Fire\n[rank5]:     component, remaining_args = _CallAndUpdateTrace(\n[rank5]:   File \"/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/fire/core.py\", line 693, in _CallAndUpdateTrace\n[rank5]:     component = fn(*varargs, **kwargs)\n[rank5]:   File \"/home/kaiwu/work/llama-recipes/src/llama_recipes/finetuning.py\", line 254, in main\n[rank5]:     raise ValueError(\"The eval set size is too small for dataloader to load even one batch. Please increase the size of eval set.\")\n[rank5]: ValueError: The eval set size is too small for dataloader to load even one batch. Please increase the size of eval set.\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [00:00<00:00, 2800.42it/s]\n[rank3]: Traceback (most recent call last):\n[rank3]:   File \"/home/kaiwu/work/llama-recipes/recipes/finetuning/finetuning.py\", line 8, in <module>\n[rank3]:     fire.Fire(main)\n[rank3]:   File \"/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/fire/core.py\", line 143, in Fire\n[rank3]:     component_trace = _Fire(component, args, parsed_flag_args, context, name)\n[rank3]:   File \"/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/fire/core.py\", line 477, in _Fire\n[rank3]:     component, remaining_args = _CallAndUpdateTrace(\n[rank3]:   File \"/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/fire/core.py\", line 693, in _CallAndUpdateTrace\n[rank3]:     component = fn(*varargs, **kwargs)\n[rank3]:   File \"/home/kaiwu/work/llama-recipes/src/llama_recipes/finetuning.py\", line 254, in main\n[rank3]:     raise ValueError(\"The eval set size is too small for dataloader to load even one batch. Please increase the size of eval set.\")\n[rank3]: ValueError: The eval set size is too small for dataloader to load even one batch. Please increase the size of eval set.\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [00:00<00:00, 2761.45it/s]\n[rank6]: Traceback (most recent call last):\n[rank6]:   File \"/home/kaiwu/work/llama-recipes/recipes/finetuning/finetuning.py\", line 8, in <module>\n[rank6]:     fire.Fire(main)\n[rank6]:   File \"/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/fire/core.py\", line 143, in Fire\n[rank6]:     component_trace = _Fire(component, args, parsed_flag_args, context, name)\n[rank6]:   File \"/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/fire/core.py\", line 477, in _Fire\n[rank6]:     component, remaining_args = _CallAndUpdateTrace(\n[rank6]:   File \"/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/fire/core.py\", line 693, in _CallAndUpdateTrace\n[rank6]:     component = fn(*varargs, **kwargs)\n[rank6]:   File \"/home/kaiwu/work/llama-recipes/src/llama_recipes/finetuning.py\", line 254, in main\n[rank6]:     raise ValueError(\"The eval set size is too small for dataloader to load even one batch. Please increase the size of eval set.\")\n[rank6]: ValueError: The eval set size is too small for dataloader to load even one batch. Please increase the size of eval set.\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [00:00<00:00, 2770.14it/s]\n[rank4]: Traceback (most recent call last):\n[rank4]:   File \"/home/kaiwu/work/llama-recipes/recipes/finetuning/finetuning.py\", line 8, in <module>\n[rank4]:     fire.Fire(main)\n[rank4]:   File \"/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/fire/core.py\", line 143, in Fire\n[rank4]:     component_trace = _Fire(component, args, parsed_flag_args, context, name)\n[rank4]:   File \"/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/fire/core.py\", line 477, in _Fire\n[rank4]:     component, remaining_args = _CallAndUpdateTrace(\n[rank4]:   File \"/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/fire/core.py\", line 693, in _CallAndUpdateTrace\n[rank4]:     component = fn(*varargs, **kwargs)\n[rank4]:   File \"/home/kaiwu/work/llama-recipes/src/llama_recipes/finetuning.py\", line 254, in main\n[rank4]:     raise ValueError(\"The eval set size is too small for dataloader to load even one batch. Please increase the size of eval set.\")\n[rank4]: ValueError: The eval set size is too small for dataloader to load even one batch. Please increase the size of eval set.\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [00:00<00:00, 2771.16it/s]\n[rank1]: Traceback (most recent call last):\n[rank1]:   File \"/home/kaiwu/work/llama-recipes/recipes/finetuning/finetuning.py\", line 8, in <module>\n[rank1]:     fire.Fire(main)\n[rank1]:   File \"/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/fire/core.py\", line 143, in Fire\n[rank1]:     component_trace = _Fire(component, args, parsed_flag_args, context, name)\n[rank1]:   File \"/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/fire/core.py\", line 477, in _Fire\n[rank1]:     component, remaining_args = _CallAndUpdateTrace(\n[rank1]:   File \"/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/fire/core.py\", line 693, in _CallAndUpdateTrace\n[rank1]:     component = fn(*varargs, **kwargs)\n[rank1]:   File \"/home/kaiwu/work/llama-recipes/src/llama_recipes/finetuning.py\", line 254, in main\n[rank1]:     raise ValueError(\"The eval set size is too small for dataloader to load even one batch. Please increase the size of eval set.\")\n[rank1]: ValueError: The eval set size is too small for dataloader to load even one batch. Please increase the size of eval set.\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51802/51802 [00:19<00:00, 2712.80it/s]\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [00:00<00:00, 2761.09it/s]\n[rank2]: Traceback (most recent call last):\n[rank2]:   File \"/home/kaiwu/work/llama-recipes/recipes/finetuning/finetuning.py\", line 8, in <module>\n[rank2]:     fire.Fire(main)\n[rank2]:   File \"/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/fire/core.py\", line 143, in Fire\n[rank2]:     component_trace = _Fire(component, args, parsed_flag_args, context, name)\n[rank2]:   File \"/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/fire/core.py\", line 477, in _Fire\n[rank2]:     component, remaining_args = _CallAndUpdateTrace(\n[rank2]:   File \"/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/fire/core.py\", line 693, in _CallAndUpdateTrace\n[rank2]:     component = fn(*varargs, **kwargs)\n[rank2]:   File \"/home/kaiwu/work/llama-recipes/src/llama_recipes/finetuning.py\", line 254, in main\n[rank2]:     raise ValueError(\"The eval set size is too small for dataloader to load even one batch. Please increase the size of eval set.\")\n[rank2]: ValueError: The eval set size is too small for dataloader to load even one batch. Please increase the size of eval set.\nE0523 10:41:14.260000 139842235315200 torch/distributed/elastic/multiprocessing/api.py:826] failed (exitcode: 1) local_rank: 0 (pid: 280537) of binary: /home/kaiwu/miniconda3/envs/llama/bin/python\n```\n\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [x] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [x] Did you make sure to update the documentation with your changes?  \n- [x] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "959": "Created an api inference script with its supporting documentationCreated an API inference script where users could pick a llama model, set their api keys, prompt the model and see the results on a Gradio UI.\n\nTested the system and attached a snap shot of the UI in action.\n<img width=\"1728\" alt=\"Screenshot 2025-05-29 at 7 20 42\u202fPM\" src=\"https://github.com/user-attachments/assets/65c9deb7-a054-4735-9a9d-a044e2bbfab0\" />\n", "756": "Update Step-1 PDF-Pre-Processing-Logic.ipynbCorrected the file name\n\n# What does this PR do?\n\nFix the incorrect file name\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "742": "add llama3 support for alpaca dataset# What does this PR do?\nThis PR added llama3 support for alpaca dataset so that people can do alpaca finetune, model conversion and inference.\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # ([634](https://github.com/meta-llama/llama-recipes/issues/634))\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] finetune+conversion+inference works with llama3 now\n```\n(llama) [kaiwu@devgpu003.cco3 ~/work/llama-recipes (main)]$ torchrun --nnodes 1 --nproc_per_node 8    ./recipes/quickstart/finetuning/finetuning.py  --model_name meta-llama/Meta-Llama-3.1-8B-Instruct --output_dir ./fsdp_fine_tune_results/output_model_1_8 --dist_checkpoint_root_folder ./fsdp_fine_tune_results/fsdp_model_finetuned_1_8  --enable_fsdp  --num_epochs 1 --batch_size_training 2 --dataset alpaca_dataset\nW1021 17:51:38.391000 140142795121664 torch/distributed/run.py:779] \nW1021 17:51:38.391000 140142795121664 torch/distributed/run.py:779] *****************************************\nW1021 17:51:38.391000 140142795121664 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW1021 17:51:38.391000 140142795121664 torch/distributed/run.py:779] *****************************************\n/home/kaiwu/work/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n  from torch.distributed._shard.checkpoint import (\n/home/kaiwu/work/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n  from torch.distributed._shard.checkpoint import (\n/home/kaiwu/work/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n  from torch.distributed._shard.checkpoint import (\n/home/kaiwu/work/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n  from torch.distributed._shard.checkpoint import (\n/home/kaiwu/work/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n  from torch.distributed._shard.checkpoint import (\n/home/kaiwu/work/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n  from torch.distributed._shard.checkpoint import (\n/home/kaiwu/work/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n  from torch.distributed._shard.checkpoint import (\n/home/kaiwu/work/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n  from torch.distributed._shard.checkpoint import (\nClearing GPU cache for all ranks\n--> Running with torch dist debug set to detail\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  4.24it/s]\nLoading checkpoint shards:  25%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                                                          | 1/4 [00:00<00:00,  4.45it/s]--> Model meta-llama/Meta-Llama-3.1-8B-Instruct\n\n--> meta-llama/Meta-Llama-3.1-8B-Instruct has 8030.261248 Million params\n\nbFloat16 enabled for mixed precision - using bfSixteen policy\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  4.50it/s]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  4.93it/s]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  4.49it/s]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  5.06it/s]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  4.64it/s]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  4.75it/s]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  4.29it/s]\n--> applying fsdp activation checkpointing...\n--> Training Set Length = 49402\n--> Validation Set Length = 2600\nPreprocessing dataset:   0%|                                                                                                           | 0/49402 [00:00<?, ?it/s]--> applying fsdp activation checkpointing...\nPreprocessing dataset:   2%|\u2588\u258b                                                                                             | 883/49402 [00:00<00:15, 3042.66it/s]--> applying fsdp activation checkpointing...\nPreprocessing dataset:   0%|\u258d                                                                                              | 214/49402 [00:00<00:23, 2137.70it/s]--> applying fsdp activation checkpointing...\n--> applying fsdp activation checkpointing...\nPreprocessing dataset:   1%|\u2588                                                                                              | 544/49402 [00:00<00:17, 2820.94it/s]--> applying fsdp activation checkpointing...\nPreprocessing dataset:   0%|                                                                                                           | 0/49402 [00:00<?, ?it/s]--> applying fsdp activation checkpointing...\nPreprocessing dataset:   2%|\u2588\u2588\u258e                                                                                           | 1192/49402 [00:00<00:15, 3097.85it/s]--> applying fsdp activation checkpointing...\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 49402/49402 [00:14<00:00, 3294.56it/s]\nlength of dataset_train 1665\n--> Num of Training Set Batches loaded = 104\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 49402/49402 [00:14<00:00, 3309.27it/s]\nlength of dataset_train 1665\n--> Num of Training Set Batches loaded = 104\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 49402/49402 [00:15<00:00, 3285.11it/s]\nlength of dataset_train 1665\n--> Num of Training Set Batches loaded = 104\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 49402/49402 [00:15<00:00, 3261.47it/s]\nlength of dataset_train 1665\n--> Num of Training Set Batches loaded = 104\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2600/2600 [00:00<00:00, 3319.79it/s]\n--> Num of Validation Set Batches loaded = 10\n--> Num of Validation Set Batches loaded = 10\nStarting epoch 0/1\ntrain_config.max_train_step: 0\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 49402/49402 [00:15<00:00, 3218.45it/s]\nlength of dataset_train 1665\n--> Num of Training Set Batches loaded = 104\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 49402/49402 [00:15<00:00, 3244.37it/s]\nlength of dataset_train 1665\n--> Num of Training Set Batches loaded = 104\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 49402/49402 [00:15<00:00, 3272.76it/s]\nlength of dataset_train 1665\n--> Num of Training Set Batches loaded = 104\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2600/2600 [00:00<00:00, 3379.44it/s]\n--> Num of Validation Set Batches loaded = 10\n--> Num of Validation Set Batches loaded = 10\nStarting epoch 0/1\ntrain_config.max_train_step: 0\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 49402/49402 [00:15<00:00, 3216.85it/s]\nlength of dataset_train 1665\n--> Num of Training Set Batches loaded = 104\nPreprocessing dataset:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                    | 2056/2600 [00:00<00:00, 3379.07it/s]/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nPreprocessing dataset:  38%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                                           | 998/2600 [00:00<00:00, 3278.66it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2600/2600 [00:00<00:00, 3360.82it/s]\n--> Num of Validation Set Batches loaded = 10\n--> Num of Validation Set Batches loaded = 10\nPreprocessing dataset:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                    | 2051/2600 [00:00<00:00, 3331.89it/s]Starting epoch 0/1\ntrain_config.max_train_step: 0\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nPreprocessing dataset:  52%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                             | 1348/2600 [00:00<00:00, 3342.51it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2600/2600 [00:00<00:00, 3300.57it/s]\n--> Num of Validation Set Batches loaded = 10\n--> Num of Validation Set Batches loaded = 10\nStarting epoch 0/1\ntrain_config.max_train_step: 0\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2600/2600 [00:00<00:00, 3272.28it/s]\n--> Num of Validation Set Batches loaded = 10\n--> Num of Validation Set Batches loaded = 10\nStarting epoch 0/1\ntrain_config.max_train_step: 0\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2600/2600 [00:00<00:00, 3304.81it/s]\n--> Num of Validation Set Batches loaded = 10\n--> Num of Validation Set Batches loaded = 10\nStarting epoch 0/1\ntrain_config.max_train_step: 0\nPreprocessing dataset:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c         | 2343/2600 [00:00<00:00, 3250.42it/s]NCCL version 2.20.5+cuda12.4\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2600/2600 [00:00<00:00, 3198.02it/s]\n--> Num of Validation Set Batches loaded = 10\n--> Num of Validation Set Batches loaded = 10\nStarting epoch 0/1\ntrain_config.max_train_step: 0\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2600/2600 [00:00<00:00, 3276.55it/s]\n--> Num of Validation Set Batches loaded = 10\n--> Num of Validation Set Batches loaded = 10\nStarting epoch 0/1\ntrain_config.max_train_step: 0\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                                                                                 | 0/104 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                                                                                 | 0/104 [00:00<?, ?it/s]/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                                                                                 | 0/104 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                                                                                 | 0/104 [00:00<?, ?it/s]/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                                                                                 | 0/104 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\nTraining Epoch: 1/1, step 103/104 completed (loss: 1.224412202835083): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 104/104 [03:30<00:00,  2.03s/it]\nTraining Epoch: 1/1, step 103/104 completed (loss: 1.383886456489563): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 104/104 [03:30<00:00,  2.02s/it]\nTraining Epoch: 1/1, step 103/104 completed (loss: 1.1475244760513306): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 104/104 [03:30<00:00,  2.02s/it]\nTraining Epoch: 1/1, step 103/104 completed (loss: 1.1846411228179932): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 104/104 [03:30<00:00,  2.03s/it]\nTraining Epoch: 1/1, step 103/104 completed (loss: 1.167246699333191): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 104/104 [03:30<00:00,  2.02s/it]\nTraining Epoch: 1/1, step 103/104 completed (loss: 1.2066352367401123): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 104/104 [03:30<00:00,  2.02s/it]\nTraining Epoch: 1/1, step 103/104 completed (loss: 1.2416218519210815): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 104/104 [03:30<00:00,  2.02s/it]\nTraining Epoch: 1/1, step 103/104 completed (loss: 1.371912956237793): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 104/104 [03:30<00:00,  2.02s/it]\nMax CUDA memory allocated was 21 GB\nMax CUDA memory reserved was 30 GB\nPeak active CUDA memory was 22 GB\nCUDA Malloc retries : 0\nCPU Total Peak Memory consumed during the train (max): 9 GB\nevaluating Epoch:   0%|                                                                                                                   | 0/10 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:02<00:00,  4.06it/s]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:02<00:00,  4.02it/s]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:02<00:00,  4.03it/s]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:02<00:00,  4.00it/s]\n\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:02<00:00,  4.25it/s]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:02<00:00,  4.02it/s]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:02<00:00,  4.01it/s]\n eval_ppl=tensor(3.5136, device='cuda:0') eval_epoch_loss=tensor(1.2566, device='cuda:0')\n Saving the FSDP model checkpoints and optimizer using SHARDED_STATE_DICT\n=====================================================\n Saving the FSDP model checkpoints and optimizer using SHARDED_STATE_DICT Saving the FSDP model checkpoints and optimizer using SHARDED_STATE_DICT Saving the FSDP model checkpoints and optimizer using SHARDED_STATE_DICT\n Saving the FSDP model checkpoints and optimizer using SHARDED_STATE_DICT Saving the FSDP model checkpoints and optimizer using SHARDED_STATE_DICT Saving the FSDP model checkpoints and optimizer using SHARDED_STATE_DICT Saving the FSDP model checkpoints and optimizer using SHARDED_STATE_DICT\n\n=====================================================\n\n\n\n==========================================================================================================\n\n====================================================================================================================================================================================================================\n\n\n\n\nSaving model to /home/kaiwu/work/llama-recipes/fsdp_fine_tune_results/fsdp_model_finetuned_1_8/fine-tuned-meta-llama/Meta-Llama-3.1-8B-Instruct\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n  warnings.warn(\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n  warnings.warn(\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n  warnings.warn(\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n  warnings.warn(\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n  warnings.warn(\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n  warnings.warn(\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n  warnings.warn(\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n  warnings.warn(\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:737: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n  local_shape = tensor.shape\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:737: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n  local_shape = tensor.shape\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:749: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n  tensor.shape,\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:749: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n  tensor.shape,\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:751: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n  tensor.dtype,\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:751: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n  tensor.dtype,\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:752: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n  tensor.device,\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:752: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n  tensor.device,\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:737: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n  local_shape = tensor.shape\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:737: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n  local_shape = tensor.shape\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:749: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n  tensor.shape,\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:737: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n  local_shape = tensor.shape\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:751: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n  tensor.dtype,\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:749: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n  tensor.shape,\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:752: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n  tensor.device,\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:751: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n  tensor.dtype,\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:749: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n  tensor.shape,\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:752: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n  tensor.device,\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:751: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n  tensor.dtype,\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:752: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n  tensor.device,\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:737: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n  local_shape = tensor.shape\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:749: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n  tensor.shape,\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:737: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n  local_shape = tensor.shape\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:751: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n  tensor.dtype,\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:752: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n  tensor.device,\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:749: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n  tensor.shape,\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:751: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n  tensor.dtype,\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:752: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n  tensor.device,\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:737: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n  local_shape = tensor.shape\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:749: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n  tensor.shape,\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:751: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n  tensor.dtype,\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:752: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n  tensor.device,\n/home/kaiwu/work/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:113: FutureWarning: `save_state_dict` is deprecated and will be removed in future versions.Please use `save` instead.\n  dist_cp.save_state_dict(\n/home/kaiwu/work/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:113: FutureWarning: `save_state_dict` is deprecated and will be removed in future versions.Please use `save` instead.\n  dist_cp.save_state_dict(\n/home/kaiwu/work/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:113: FutureWarning: `save_state_dict` is deprecated and will be removed in future versions.Please use `save` instead.\n  dist_cp.save_state_dict(\n/home/kaiwu/work/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:113: FutureWarning: `save_state_dict` is deprecated and will be removed in future versions.Please use `save` instead.\n  dist_cp.save_state_dict(\n/home/kaiwu/work/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:113: FutureWarning: `save_state_dict` is deprecated and will be removed in future versions.Please use `save` instead.\n  dist_cp.save_state_dict(\n/home/kaiwu/work/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:113: FutureWarning: `save_state_dict` is deprecated and will be removed in future versions.Please use `save` instead.\n  dist_cp.save_state_dict(\n/home/kaiwu/work/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:113: FutureWarning: `save_state_dict` is deprecated and will be removed in future versions.Please use `save` instead.\n  dist_cp.save_state_dict(\n/home/kaiwu/work/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:113: FutureWarning: `save_state_dict` is deprecated and will be removed in future versions.Please use `save` instead.\n  dist_cp.save_state_dict(\nSharded state checkpoint saved to /home/kaiwu/work/llama-recipes/fsdp_fine_tune_results/fsdp_model_finetuned_1_8/fine-tuned-meta-llama/Meta-Llama-3.1-8B-Instruct\nCheckpoint Time = 18.6865\n\nbest eval loss on epoch 1 is 1.2566323280334473\nEpoch 1: train_perplexity=4.5941, train_epoch_loss=1.5248, epoch time 211.6873932024464s\ntraining params are saved in /home/kaiwu/work/llama-recipes/fsdp_fine_tune_results/fsdp_model_finetuned_1_8/fine-tuned-meta-llama/Meta-Llama-3.1-8B-Instruct/train_params.yaml\nKey: avg_train_prep, Value: 4.594069480895996\nKey: avg_train_loss, Value: 1.524766206741333\nKey: avg_eval_prep, Value: 3.513568878173828\nKey: avg_eval_loss, Value: 1.2566323280334473\nKey: avg_epoch_time, Value: 211.6873932024464\nKey: avg_checkpoint_time, Value: 18.68949384521693\n[rank0]:[W1021 17:55:59.339848938 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n\n\n(llama) [kaiwu@devgpu003.cco3 ~/work/llama-recipes (main)]$ python ./src/llama_recipes/inference/checkpoint_converter_fsdp_hf.py --fsdp_checkpoint_path ./fsdp_fine_tune_results/fsdp_model_finetuned_1_8/fine-tuned-meta-llama/Meta-Llama-3.1-8B-Instruct/ --consolidated_model_path ./fsdp_fine_tune_results/fsdp_model_finetune\nd_1_8_hf \n/home/kaiwu/work/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n  from torch.distributed._shard.checkpoint import (\nModel name: meta-llama/Meta-Llama-3.1-8B-Instruct\nmodel is loaded from config\n/home/kaiwu/work/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:259: FutureWarning: `load_state_dict` is deprecated and will be removed in future versions. Please use `load` instead.\n  dist_cp.load_state_dict(\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/checkpoint/filesystem.py:657: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  torch.load(cast(IO[bytes], file_slice), map_location=\"cpu\"),\nSharded state checkpoint loaded from ./fsdp_fine_tune_results/fsdp_model_finetuned_1_8/fine-tuned-meta-llama/Meta-Llama-3.1-8B-Instruct/\nmodel is loaded from FSDP checkpoints\nHuggingFace llama tokenizer has been saved in ./fsdp_fine_tune_results/fsdp_model_finetuned_1_8_hf\nHuggingFace model checkpoints has been saved in ./fsdp_fine_tune_results/fsdp_model_finetuned_1_8_hf\n\n\n(llama) [kaiwu@devgpu003.cco3 ~/work/llama-recipes (main)]$ python ./recipes/quickstart/inference/local_inference/inference.py --model_name ./fsdp_fine_tune_results/fsdp_model_finetuned_1_8_hf --prompt_file prompt_for_test.txt\n/home/kaiwu/work/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n  from torch.distributed._shard.checkpoint import (\nuse_fast_kernelsFalse\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7/7 [00:06<00:00,  1.08it/s]\nUser prompt deemed safe.\nUser prompt:\nI have tomatoes, basil and cheese at home. What can I cook for dinner?\\n\n\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nStarting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\nthe inference time is 7803.556815721095 ms\nUser input and model output deemed safe.\nModel output:\nI have tomatoes, basil and cheese at home. What can I cook for dinner?\\n\nYou can make a delicious margherita-style pizza. Preheat your oven to 350 degrees F and create the pizza base with your tomatoes. Top it with the creamy mozzarella and the fresh basil. Bake for 10 minutes and enjoy!\n\n```\n\n\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "805": "Llama Email Agent# What does this PR do?\n\nAn app that shows how to build a Gmail agent app powered by Llama 3.1 8B running locally via Ollama, with custom tool calling natively supported in Llama 3.1. \n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "51": "fixing the full state path in checkpoint handler+loss report calculationThis PR fixes \n- the checkpoint path for full_state_dict and also removes the local_state_dict code\n- loss report calculation, issue #68 .", "178": "Allow easier use of custom datasets# What does this PR do?\nThis PR creates a mechanism to allow users to provide custom datasets by providing a single .py file instead of modifying the source code or llama-recipes. This allows users to simply install llama-recipes through pip while still training on their custom data. \n\nFixes # (issue)\nN/A\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [X] python -m pytest tests/\n```\n=============================================================================================================== test session starts ================================================================================================================\nplatform linux -- Python 3.10.11, pytest-7.3.2, pluggy-1.0.0\nrootdir: /home/ubuntu/llama-recipes\nplugins: mock-3.11.1, anyio-3.7.0, hydra-core-1.0.7\ncollected 7 items\n\ntests/test_finetuning.py ...                                                                                                                                                                                                                 [ 42%]\ntests/test_train_utils.py .                                                                                                                                                                                                                  [ 57%]\ntests/datasets/test_custom_dataset.py ..                                                                                                                                                                                                     [ 85%]\ntests/datasets/test_samsum_datasets.py .                                                                                                                                                                                                     [100%]\n\n================================================================================================================= warnings summary =================================================================================================================\nsrc/llama_recipes/finetuning.py:5\n  /home/ubuntu/llama-recipes/src/llama_recipes/finetuning.py:5: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n    from pkg_resources import packaging\n\n../miniconda3/envs/llama/lib/python3.10/site-packages/pkg_resources/__init__.py:2871\n  /home/ubuntu/miniconda3/envs/llama/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.\n  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n    declare_namespace(pkg)\n\n../miniconda3/envs/llama/lib/python3.10/site-packages/pkg_resources/__init__.py:2871\n  /home/ubuntu/miniconda3/envs/llama/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n    declare_namespace(pkg)\n\n../miniconda3/envs/llama/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:147\n  /home/ubuntu/miniconda3/envs/llama/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:147: UserWarning: /home/ubuntu/miniconda3/envs/llama did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n    warn(msg)\n\n../miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/_shard/checkpoint/__init__.py:8\n  /home/ubuntu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/_shard/checkpoint/__init__.py:8: DeprecationWarning: torch.distributed._shard.checkpoint will be deprecated, use torch.distributed.checkpoint instead\n    warnings.warn(\n\ntests/test_train_utils.py::test_gradient_accumulation\n  /home/ubuntu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:329: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n    warnings.warn(\n\ntests/datasets/test_custom_dataset.py::test_custom_dataset\ntests/datasets/test_samsum_datasets.py::test_custom_dataset\n  /home/ubuntu/miniconda3/envs/llama/lib/python3.10/site-packages/dill/_dill.py:1705: PicklingWarning: Cannot locate reference to <class 'unittest.mock.MagicMock'>.\n    warnings.warn('Cannot locate reference to %r.' % (obj,), PicklingWarning)\n\ntests/datasets/test_custom_dataset.py::test_custom_dataset\ntests/datasets/test_samsum_datasets.py::test_custom_dataset\n  /home/ubuntu/miniconda3/envs/llama/lib/python3.10/site-packages/dill/_dill.py:1707: PicklingWarning: Cannot pickle <class 'unittest.mock.MagicMock'>: unittest.mock.MagicMock has recursive self-references that trigger a RecursionError.\n    warnings.warn('Cannot pickle %r: %s.%s has recursive self-references that trigger a RecursionError.' % (obj, obj.__module__, obj_name), PicklingWarning)\n\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n========================================================================================================= 7 passed, 10 warnings in 10.68s ==========================================================================================================\n```\n\n## Before submitting\n- [X] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [X] Did you make sure to update the documentation with your changes?  \n- [X] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "144": "adding llama code inference# What does this PR do?\n\nThis PR adds the code Llama examples for code completion and infilling.\n\n\n- [ ] Test A\n[Logs for infilling and completion ](https://gist.github.com/HamidShojanazeri/959e5b8f1ee96fe83870fdfb0b8ca23e)\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [x ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "622": "Update checkpoint_converter_fsdp_hf.py# What does this PR do?\n\nThis PR refactors the existing script to improve modularity, readability, and error handling.\n[src/llama_recipes/inference/checkpoint_converter_fsdp_hf.py]\n\n### Description of the Change\n- **Modularization**: Introduced `get_model_name_from_yaml` and `load_and_save_model` functions to enhance code structure.\n- **Exception Handling**: Added specific exception handling for improved error reporting.\n- **Code Readability**: Grouped imports and added docstrings for better understanding.\n- **Logging**: Included print statements for logging key steps (consider future use of the `logging` module for more control).\n\n### Motivation and Context\nThese changes improve the readability, maintainability, and robustness of the code, making it easier to understand and extend. The refactored code also handles errors more gracefully, providing clearer messages to the user.\n\n### Dependencies\nNo new dependencies are required for this change.\n\n\n# Summary of Changes\n\n## Key Improvements\n\n1. **Modularization**:\n   - Introduced `get_model_name_from_yaml` and `load_and_save_model` functions to improve code modularity and readability.\n\n2. **Exception Handling**:\n   - Enhanced exception handling to be more specific and informative.\n\n3. **Code Readability**:\n   - Grouped similar imports together and added docstrings for better understanding.\n\n4. **Logging**:\n   - Included print statements for logging key steps (consider future use of the `logging` module for more control).\n\n## Detailed Changes\n\n### Import Optimizations\n- Grouped standard library, third-party, and local application imports.\n- Removed unnecessary commented-out imports.\n\n### Function Decomposition\n- Split the main logic into smaller functions: `get_model_name_from_yaml` and `load_and_save_model`.\n\n### Exception Handling\n- Improved error handling in `get_model_name_from_yaml` to handle specific exceptions like `FileNotFoundError` and general exceptions.\n\n### Docstrings and Comments\n- Added docstrings to functions to describe their purpose and usage.\n- Improved inline comments for clarity.\n\n\n", "636": "Add preprocessor to patch PromptGuard scores for inserted characters**Problem**: Inserting spaces between characters in given prompts causes misclassifications in PromptGuard. See https://github.com/meta-llama/llama-models/issues/50 for more context.\n\n**Solution:** Tokenize the string with all spaces removed, to ensure that larger tokens (for example, `[\u201cignore\u201d]`) are not broken up into smaller tokens (for example, `[\u201ci\u201d, \u201cg\u201d, \u201cn\u201d, \u201co\u201d, \u201cr\u201d, \u201ce\u201d]` . Add back spaces between the larger tokens if spaces exist in the original string.\n\nThis approach showed a slight positive impact on all of our evaluation datasets, suggesting that making the system more robust to jailbreaks that disrupt tokenization like this one will be an important part of improving model quality. Notably, simply subtracting spaces from the string lead to a moderate quality regression on some datasets, which is why we don\u2019t take that simpler approach here.\n\nThis solution only targets jailbreaks enabled by inserted spaces and not other special characters. For a more complete approach longer term, we\u2019re continuing to work on building more adversarial examples into our dataset.\n\nThe preprocessor is used by default by our inference utilities.\n", "391": "Refactor the folder structure to organize recipes by topic# What does this PR do?\nThe PR refactors recipes in the current repo to be organized by topic. Contents within `demo_apps/` and `examples/` are moved into `recipes/`. \n\nWith the new folders, the PR also creates new folder-level READMEs to provide information about the recipes in the same folder.  The main README has also been minimized and the existing content has been refactored into the relevant sections of the repo.\n\nThese changes are part of the broader effort to improve the developer experience of using this repository.\n", "346": "typo fix in Purple_Llama_Anyscale.ipynb# What does this PR do?\ntypo fix in Purple_Llama_Anyscale.ipynb\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "434": "Bump gradio from 4.16.0 to 4.19.2 in /recipes/llama_api_providers/OctoAI_API_examples/RAG_Chatbot_exampleBumps [gradio](https://github.com/gradio-app/gradio) from 4.16.0 to 4.19.2.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/gradio-app/gradio/releases\">gradio's releases</a>.</em></p>\n<blockquote>\n<h2><code>@\u200bgradio/model3d</code><a href=\"https://github.com/0\"><code>@\u200b0</code></a>.8.10</h2>\n<h3>Dependency updates</h3>\n<ul>\n<li><code>@\u200bgradio/utils</code><a href=\"https://github.com/0\"><code>@\u200b0</code></a>.3.1</li>\n<li><code>@\u200bgradio/atoms</code><a href=\"https://github.com/0\"><code>@\u200b0</code></a>.6.2</li>\n<li><code>@\u200bgradio/statustracker</code><a href=\"https://github.com/0\"><code>@\u200b0</code></a>.4.11</li>\n<li><code>@\u200bgradio/upload</code><a href=\"https://github.com/0\"><code>@\u200b0</code></a>.8.4</li>\n<li><code>@\u200bgradio/client</code><a href=\"https://github.com/0\"><code>@\u200b0</code></a>.15.1</li>\n</ul>\n<h2><code>@\u200bgradio/model3d</code><a href=\"https://github.com/0\"><code>@\u200b0</code></a>.8.9</h2>\n<h3>Dependency updates</h3>\n<ul>\n<li><code>@\u200bgradio/upload</code><a href=\"https://github.com/0\"><code>@\u200b0</code></a>.8.3</li>\n<li><code>@\u200bgradio/client</code><a href=\"https://github.com/0\"><code>@\u200b0</code></a>.15.0</li>\n</ul>\n<h2><code>@\u200bgradio/model3d</code><a href=\"https://github.com/0\"><code>@\u200b0</code></a>.8.8</h2>\n<h3>Dependency updates</h3>\n<ul>\n<li><code>@\u200bgradio/atoms</code><a href=\"https://github.com/0\"><code>@\u200b0</code></a>.6.1</li>\n<li><code>@\u200bgradio/statustracker</code><a href=\"https://github.com/0\"><code>@\u200b0</code></a>.4.10</li>\n<li><code>@\u200bgradio/icons</code><a href=\"https://github.com/0\"><code>@\u200b0</code></a>.3.4</li>\n<li><code>@\u200bgradio/upload</code><a href=\"https://github.com/0\"><code>@\u200b0</code></a>.8.2</li>\n</ul>\n</blockquote>\n</details>\n<details>\n<summary>Changelog</summary>\n<p><em>Sourced from <a href=\"https://github.com/gradio-app/gradio/blob/main/CHANGELOG.md\">gradio's changelog</a>.</em></p>\n<blockquote>\n<h2>4.19.2</h2>\n<h3>Features</h3>\n<ul>\n<li><a href=\"https://redirect.github.com/gradio-app/gradio/pull/7495\">#7495</a> <a href=\"https://github.com/gradio-app/gradio/commit/ddd4d3e4d3883fb7540d1df240fb08202fc77705\"><code>ddd4d3e</code></a> - Enable Ruff S101.  Thanks <a href=\"https://github.com/abidlabs\"><code>@\u200babidlabs</code></a>!</li>\n<li><a href=\"https://redirect.github.com/gradio-app/gradio/pull/7443\">#7443</a> <a href=\"https://github.com/gradio-app/gradio/commit/b7a97f29b84a72678a717db03d2932ed6caae6ce\"><code>b7a97f2</code></a> - Update <code>httpx</code> to <code>httpx&gt;=0.24.1</code> in <code>requirements.txt</code>.  Thanks <a href=\"https://github.com/abidlabs\"><code>@\u200babidlabs</code></a>!</li>\n<li><a href=\"https://redirect.github.com/gradio-app/gradio/pull/7465\">#7465</a> <a href=\"https://github.com/gradio-app/gradio/commit/16fbe9cd0cffa9f2a824a0165beb43446114eec7\"><code>16fbe9c</code></a> - Prevent components from working with non-uploaded files.  Thanks <a href=\"https://github.com/aliabid94\"><code>@\u200baliabid94</code></a>!</li>\n<li><a href=\"https://redirect.github.com/gradio-app/gradio/pull/7503\">#7503</a> <a href=\"https://github.com/gradio-app/gradio/commit/84802ee6a4806c25287344dce581f9548a99834a\"><code>84802ee</code></a> - Tighten CORS rules.  Thanks <a href=\"https://github.com/abidlabs\"><code>@\u200babidlabs</code></a>!</li>\n</ul>\n<h3>Fixes</h3>\n<ul>\n<li><a href=\"https://redirect.github.com/gradio-app/gradio/pull/7466\">#7466</a> <a href=\"https://github.com/gradio-app/gradio/commit/98a2719bfb9c64338caf9009891b6c6b0b33ea89\"><code>98a2719</code></a> - Fix z-index layer of orange generating border.  Thanks <a href=\"https://github.com/daviirodrig\"><code>@\u200bdaviirodrig</code></a>!</li>\n<li><a href=\"https://redirect.github.com/gradio-app/gradio/pull/7507\">#7507</a> <a href=\"https://github.com/gradio-app/gradio/commit/9c36572e32aeec6e6352a861dfea6ee0f9a15e79\"><code>9c36572</code></a> - Quick fix: File height overflow.  Thanks <a href=\"https://github.com/dawoodkhan82\"><code>@\u200bdawoodkhan82</code></a>!</li>\n<li><a href=\"https://redirect.github.com/gradio-app/gradio/pull/7495\">#7495</a> <a href=\"https://github.com/gradio-app/gradio/commit/ddd4d3e4d3883fb7540d1df240fb08202fc77705\"><code>ddd4d3e</code></a> - ensure Dataframe headers are aligned with content when scrolling.  Thanks <a href=\"https://github.com/abidlabs\"><code>@\u200babidlabs</code></a>!</li>\n<li><a href=\"https://redirect.github.com/gradio-app/gradio/pull/7470\">#7470</a> <a href=\"https://github.com/gradio-app/gradio/commit/ba3ec1300e81e64be7389d759b89284c66473158\"><code>ba3ec13</code></a> - Tab select fix.  Thanks <a href=\"https://github.com/aliabid94\"><code>@\u200baliabid94</code></a>!</li>\n<li><a href=\"https://redirect.github.com/gradio-app/gradio/pull/7505\">#7505</a> <a href=\"https://github.com/gradio-app/gradio/commit/b18676774448f44a2ef3a9490224703254cffa7c\"><code>b186767</code></a> - Fix <code>Gallery</code> preview overlay and backdrop.  Thanks <a href=\"https://github.com/MMP0\"><code>@\u200bMMP0</code></a>!</li>\n<li><a href=\"https://redirect.github.com/gradio-app/gradio/pull/7511\">#7511</a> <a href=\"https://github.com/gradio-app/gradio/commit/33f68cb6c22897f7996b6c84b0e528c47fae00b5\"><code>33f68cb</code></a> - Fix Canvas3D/Canvas3DGS async imports.  Thanks <a href=\"https://github.com/whitphx\"><code>@\u200bwhitphx</code></a>!</li>\n</ul>\n<h2>4.19.1</h2>\n<h3>Features</h3>\n<ul>\n<li><a href=\"https://redirect.github.com/gradio-app/gradio/pull/7453\">#7453</a> <a href=\"https://github.com/gradio-app/gradio/commit/ba747adb87e1937c0a791186eee3997d034363e6\"><code>ba747ad</code></a> - Make fix in <a href=\"https://redirect.github.com/gradio-app/gradio/issues/7444\">#7444</a> (Block /file= filepaths that could expose credentials on Windows) more general.  Thanks <a href=\"https://github.com/abidlabs\"><code>@\u200babidlabs</code></a>!</li>\n<li><a href=\"https://redirect.github.com/gradio-app/gradio/pull/7416\">#7416</a> <a href=\"https://github.com/gradio-app/gradio/commit/c88290d90a81811911361e26fa9523c9b13db527\"><code>c88290d</code></a> - WIP: Optimize /file route.  Thanks <a href=\"https://github.com/freddyaboulton\"><code>@\u200bfreddyaboulton</code></a>!</li>\n<li><a href=\"https://redirect.github.com/gradio-app/gradio/pull/7440\">#7440</a> <a href=\"https://github.com/gradio-app/gradio/commit/e329f1fd38935213fe0e73962e8cbd5d3af6e87b\"><code>e329f1f</code></a> - Prevent timing attacks to guess Gradio passwords.  Thanks <a href=\"https://github.com/abidlabs\"><code>@\u200babidlabs</code></a>!</li>\n<li><a href=\"https://redirect.github.com/gradio-app/gradio/pull/7425\">#7425</a> <a href=\"https://github.com/gradio-app/gradio/commit/3e4e680a52ba5a73c108ef1b328dacd7b6e4b566\"><code>3e4e680</code></a> - Fixes to the <code>.key_up()</code> method to make it usable for a dynamic dropdown autocomplete.  Thanks <a href=\"https://github.com/abidlabs\"><code>@\u200babidlabs</code></a>!</li>\n</ul>\n<h3>Fixes</h3>\n<ul>\n<li><a href=\"https://redirect.github.com/gradio-app/gradio/pull/7444\">#7444</a> <a href=\"https://github.com/gradio-app/gradio/commit/4faf8a7e86bfe811ef3d90ac5abdbd41409fafb1\"><code>4faf8a7</code></a> - Block <code>/file=</code> filepaths that could expose credentials on Windows.  Thanks <a href=\"https://github.com/abidlabs\"><code>@\u200babidlabs</code></a>!</li>\n<li><a href=\"https://redirect.github.com/gradio-app/gradio/pull/7441\">#7441</a> <a href=\"https://github.com/gradio-app/gradio/commit/f52cab634b94638d7f4625d40bf3d9afbe68040b\"><code>f52cab6</code></a> - Dispatch change event for file explorer.  Thanks <a href=\"https://github.com/aliabid94\"><code>@\u200baliabid94</code></a>!</li>\n<li><a href=\"https://redirect.github.com/gradio-app/gradio/pull/7327\">#7327</a> <a href=\"https://github.com/gradio-app/gradio/commit/fb1f6befad12106faafd94d221c1ed0e50b0a037\"><code>fb1f6be</code></a> - Run pre/post processing in threadpool.  Thanks <a href=\"https://github.com/freddyaboulton\"><code>@\u200bfreddyaboulton</code></a>!</li>\n<li><a href=\"https://redirect.github.com/gradio-app/gradio/pull/7431\">#7431</a> <a href=\"https://github.com/gradio-app/gradio/commit/6b8a7e5d36887cdfcfbfec1536a915128df0d6b2\"><code>6b8a7e5</code></a> - Ensure <code>gr.Dropdown</code> can have an empty initial value.  Thanks <a href=\"https://github.com/hannahblair\"><code>@\u200bhannahblair</code></a>!</li>\n<li><a href=\"https://redirect.github.com/gradio-app/gradio/pull/6991\">#6991</a> <a href=\"https://github.com/gradio-app/gradio/commit/f1917867916647d383b8d7ce15e0c17f2abbdec1\"><code>f191786</code></a> - Improve responsiveness of <code>gr.Audio()</code> controls.  Thanks <a href=\"https://github.com/hannahblair\"><code>@\u200bhannahblair</code></a>!</li>\n</ul>\n<h2>4.19.0</h2>\n<h3>Features</h3>\n<ul>\n<li><a href=\"https://redirect.github.com/gradio-app/gradio/pull/7406\">#7406</a> <a href=\"https://github.com/gradio-app/gradio/commit/3e886d8f0ac55c416dae51c1c2661e16eb34718e\"><code>3e886d8</code></a> - Model3D Gaussian Splatting.  Thanks <a href=\"https://github.com/dylanebert\"><code>@\u200bdylanebert</code></a>!</li>\n</ul>\n<h3>Fixes</h3>\n<ul>\n<li><a href=\"https://redirect.github.com/gradio-app/gradio/pull/7402\">#7402</a> <a href=\"https://github.com/gradio-app/gradio/commit/fa8225d24d86c0ec9a48cadee78dcc11b7084584\"><code>fa8225d</code></a> - Use updated component in <code>postprocess()</code>.  Thanks <a href=\"https://github.com/abidlabs\"><code>@\u200babidlabs</code></a>!</li>\n<li><a href=\"https://redirect.github.com/gradio-app/gradio/pull/7361\">#7361</a> <a href=\"https://github.com/gradio-app/gradio/commit/17fb116492f951ab66e3a39b5fdfb598f5446b6f\"><code>17fb116</code></a> - Fixes gr.Markdown() does not render spaces around links correctly.  Thanks <a href=\"https://github.com/dawoodkhan82\"><code>@\u200bdawoodkhan82</code></a>!</li>\n<li><a href=\"https://redirect.github.com/gradio-app/gradio/pull/7337\">#7337</a> <a href=\"https://github.com/gradio-app/gradio/commit/65437ce832f806da316aa074539b6263e1d8b7ac\"><code>65437ce</code></a> - Improve File Explorer performance.  Thanks <a href=\"https://github.com/aliabid94\"><code>@\u200baliabid94</code></a>!</li>\n<li><a href=\"https://redirect.github.com/gradio-app/gradio/pull/7410\">#7410</a> <a href=\"https://github.com/gradio-app/gradio/commit/c2dfc592a4988efd5a96a062eec3fb4906f71748\"><code>c2dfc59</code></a> - remove static while pending behaviour.  Thanks <a href=\"https://github.com/pngwn\"><code>@\u200bpngwn</code></a>!</li>\n<li><a href=\"https://redirect.github.com/gradio-app/gradio/pull/7389\">#7389</a> <a href=\"https://github.com/gradio-app/gradio/commit/b5c74ffadbee351b2d5d79e578246f5343255508\"><code>b5c74ff</code></a> - Fix HTTPX package crash for some values of &quot;article&quot; parameter in the interface.  Thanks <a href=\"https://github.com/YuryYakhno\"><code>@\u200bYuryYakhno</code></a>!</li>\n<li><a href=\"https://redirect.github.com/gradio-app/gradio/pull/7415\">#7415</a> <a href=\"https://github.com/gradio-app/gradio/commit/4ab399f40a300f267231f1b2dbe2a07494322d4d\"><code>4ab399f</code></a> - Allow config to include non-pickle-able values.  Thanks <a href=\"https://github.com/abidlabs\"><code>@\u200babidlabs</code></a>!</li>\n<li><a href=\"https://redirect.github.com/gradio-app/gradio/pull/7404\">#7404</a> <a href=\"https://github.com/gradio-app/gradio/commit/065c5b163c4badb9d9cbd06d627fb4ba086003e7\"><code>065c5b1</code></a> - Add <code>.key_up</code> event listener to <code>gr.Dropdown()</code>.  Thanks <a href=\"https://github.com/abidlabs\"><code>@\u200babidlabs</code></a>!</li>\n</ul>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/gradio-app/gradio/commit/bacbc70fa1936a5ca2bfd85ab493a4912004b10e\"><code>bacbc70</code></a> chore: update versions (<a href=\"https://redirect.github.com/gradio-app/gradio/issues/7463\">#7463</a>)</li>\n<li><a href=\"https://github.com/gradio-app/gradio/commit/84802ee6a4806c25287344dce581f9548a99834a\"><code>84802ee</code></a> Tighten CORS rules (<a href=\"https://redirect.github.com/gradio-app/gradio/issues/7503\">#7503</a>)</li>\n<li><a href=\"https://github.com/gradio-app/gradio/commit/b18676774448f44a2ef3a9490224703254cffa7c\"><code>b186767</code></a> Fix <code>Gallery</code> preview overlay and backdrop (<a href=\"https://redirect.github.com/gradio-app/gradio/issues/7505\">#7505</a>)</li>\n<li><a href=\"https://github.com/gradio-app/gradio/commit/9c36572e32aeec6e6352a861dfea6ee0f9a15e79\"><code>9c36572</code></a> Quick fix: File height overflow (<a href=\"https://redirect.github.com/gradio-app/gradio/issues/7507\">#7507</a>)</li>\n<li><a href=\"https://github.com/gradio-app/gradio/commit/8f050eedbc98d9c58f035d728eda8807f7c383f8\"><code>8f050ee</code></a> Streaming example for the updated OpenAI API (<a href=\"https://redirect.github.com/gradio-app/gradio/issues/7508\">#7508</a>)</li>\n<li><a href=\"https://github.com/gradio-app/gradio/commit/33f68cb6c22897f7996b6c84b0e528c47fae00b5\"><code>33f68cb</code></a> Fix Canvas3D/Canvas3DGS async imports (<a href=\"https://redirect.github.com/gradio-app/gradio/issues/7511\">#7511</a>)</li>\n<li><a href=\"https://github.com/gradio-app/gradio/commit/16fbe9cd0cffa9f2a824a0165beb43446114eec7\"><code>16fbe9c</code></a> Prevent components from working with non-uploaded files (<a href=\"https://redirect.github.com/gradio-app/gradio/issues/7465\">#7465</a>)</li>\n<li><a href=\"https://github.com/gradio-app/gradio/commit/ba3ec1300e81e64be7389d759b89284c66473158\"><code>ba3ec13</code></a> Tab select fix (<a href=\"https://redirect.github.com/gradio-app/gradio/issues/7470\">#7470</a>)</li>\n<li><a href=\"https://github.com/gradio-app/gradio/commit/98a2719bfb9c64338caf9009891b6c6b0b33ea89\"><code>98a2719</code></a> Fix z-index layer of orange generating border (<a href=\"https://redirect.github.com/gradio-app/gradio/issues/7466\">#7466</a>)</li>\n<li><a href=\"https://github.com/gradio-app/gradio/commit/09de02a9b2ccb8c61fa7359f122f6fb194f14bfd\"><code>09de02a</code></a> chore(deps): update dependency globals to v14 (<a href=\"https://redirect.github.com/gradio-app/gradio/issues/7381\">#7381</a>)</li>\n<li>Additional commits viewable in <a href=\"https://github.com/gradio-app/gradio/compare/gradio@4.16.0...gradio@4.19.2\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=gradio&package-manager=pip&previous-version=4.16.0&new-version=4.19.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/meta-llama/llama-recipes/network/alerts).\n\n</details>", "353": "Add Prompt Engineering with Llama 2Introduces *Prompt Engineering with Llama 2*: an interactive notebook combining examples from the latest research and our Llama experts.\n\ncc @jeffxtang \n\n## Feature/Issue validation/testing\n\n- [X] Reviewed & tested internally.\n- [x] Run locally with [`jupyter lab`](https://jupyter.org/).\n\n## Before submitting\n- [X] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [X] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [X] Did you make sure to update the documentation with your changes?  \n- [X] Did you write any new necessary tests?", "390": "removing legacy code for sdpa# What does this PR do?\n\nRemoving the legacy code as we integrated new way HF adapted for using SDPA such as other [examples](https://github.com/facebookresearch/llama-recipes/blob/main/examples/inference.py#L73), this example was not updated.\n", "384": "Add gradio to requirements.txtGradio web interface is used in examples/inference.py, so we need to add gradio to requirements.txt.\n\n# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "151": "adding notes how to get the HF models# What does this PR do?\n\nAdding notes where to find HF models in addition to conversion step of the original weights.\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ X] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "93": "fix some typos.# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes some typos.\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "838": "Fix a property name of list_emails function# What does this PR do?\n\nProperty name `maxResults` should be renamed as `max_results`.\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "50": "Create spellcheck.ymlAdding workflow that check edited files for typos and broken links on push.", "44": "adding active mem statsmall PR to add active memory stat to the training loop. Also setting the BF16 default to false to avoid confusion.", "743": "Create SECURITY.md# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "757": "All functionality has been consolidated into a single file for CLI/UI/Checkpointing and Added fix for issue 702 and added code for that as well, added instructions in local_inference README.md# What does this PR do?\n\nThis PR adds detailed instructions for using the `multi_modal_infer.py` script to generate text from images after fine-tuning the Llama 3.2 vision model. The script supports merging PEFT adapter weights from a specified path. The changes include:\n\n- Adding a new section in the \n\nLLM_finetuning_overview.md\n\n file under the \"Inference\" heading.\n- Providing a usage example for running the inference script with the necessary parameters.\n\nFixes # (issue)\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [x] Test A: Verified that the `code-merge-inference.py` script runs successfully with the provided example command.\n  Logs for Test A:\n  ```bash\n  python multi_modal_infer.py \\\n      --image_path \"path/to/your/image.png\" \\\n      --prompt_text \"Your prompt text here\" \\\n      --temperature 1 \\\n      --top_p 0.5 \\\n      --model_name \"meta-llama/Llama-3.2-11B-Vision-Instruct\" \\\n      --hf_token \"your_hugging_face_token\" \\\n      --finetuning_path \"path/to/your/finetuned/model\"\n  ```\nOutput:\n```bash\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:03<00:00,  1.40it/s]\nLoading adapter from 'PATH/to/save/PEFT/model'...\nAdapter merged successfully with the pre-trained model.\n```\n\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests), Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link to it if that's the case.\n- [x] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!", "958": "add vertex notebooks for gcp# What does this PR do?\nAdds notebooks to provide examples for tool calling and JSON mode via Vertex AI MaaS on GCP. ", "794": "rebaserebase", "227": "Llama 2 Connect 2023 Colab NotebookThis Colab notebook is a connect 2023 talk about Llama 2, including understanding different Llama 2 models, how and where to access them, Generative AI and Chatbot architectures, prompt engineering, RAG (Retrieval Augmented Generation), Fine-tuning and more. All this is implemented with a starter code for you to take it and use it in your Llama 2 projects.\n\n# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "555": "Minor update to README", "569": "Adding end-to-end llama chatbot recipe using Retrieval Augmented Fine Tuning (RAFT)# What does this PR do?\n\nIn this tutorial, we demonstrated how to use official documents to build a Llama Chatbot for Llama users using Retrieval Augmented Fine Tuning (RAFT). \n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "582": "Deleting Agents folder and adding llamaindexDeleting Agents folder and adding llamaindex", "596": "Fix relative links to images# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "580": "Updated the folder name 3p_integrationsUpdated the folder name to 3p_integrations and the references", "594": "Port of DLAI LlamaIndex Agent short course lessons 2-4 to use Llama 3# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "557": "add multiturn-conversation dataset process# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "796": "[docs] small typo in eval readme# What does this PR do?\n\nJust a small typo in eval readme\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n", "769": "Update Step-1 PDF-Pre-Processing-Logic.ipynb to remove \"text not defined\"# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "741": "Save the `preprocessor_config.json` and `chat_template.json` for mllama model after conversionThis PR fixed the previous bug and save the `preprocessor_config.json` and `chat_template.json` for mllama model by instantiating the processor and use save_pretrained().\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] conversion works\n```\n(llama) [kaiwu@devgpu003.cco3 ~/work/llama-recipes (fsdp_lmm)]$ python src/llama_recipes/inference/checkpoint_converter_fsdp_hf.py --fsdp_checkpoint_path finetuned_model/fine-tuned-meta-llama/Llama-3.2-11B-Vision-Instruct/ --consolidated_model_path hf_converted\n/home/kaiwu/work/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n  from torch.distributed._shard.checkpoint import (\nModel name: meta-llama/Llama-3.2-11B-Vision-Instruct\nmodel is loaded from config\n/home/kaiwu/work/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:259: FutureWarning: `load_state_dict` is deprecated and will be removed in future versions. Please use `load` instead.\n  dist_cp.load_state_dict(\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/checkpoint/filesystem.py:657: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  torch.load(cast(IO[bytes], file_slice), map_location=\"cpu\"),\nSharded state checkpoint loaded from finetuned_model/fine-tuned-meta-llama/Llama-3.2-11B-Vision-Instruct/\nmodel is loaded from FSDP checkpoints\nHuggingFace model checkpoints has been saved in hf_converted\n```\n\n- [ ] inference now works\n```\n(llama) [kaiwu@devgpu003.cco3 ~/work/llama-recipes (fsdp_lmm)]$ python recipes/quickstart/inference/local_inference/multi_modal_infer.py --image_path \"./dog.jpg\" --prompt_text \"Describe this image\" --temperature 0.5 --top_p 0.8 --model_name ./hf_converted/\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9/9 [00:08<00:00,  1.10it/s]\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\nGenerated Text: end_header_id|>\n\nThis image depicts a small dog standing on a skateboard. The dog is a small breed with a white face, brown ears, and a brown body with black and gray patches. It has white paws and a black collar. The dog is standing on a skateboard with red wheels. The background is out of focus, but it appears to be a street with a blue door.<|eot_id|>\n```\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "755": "Quickstart docs: Fix path to location of dict for custom datasets The file [recipes/quickstart/finetuning/datasets/README.md](https://github.com/meta-llama/llama-recipes/tree/main/recipes/quickstart/finetuning/datasets#adding-new-dataset) explains how to add a new dataset.\n\nIt notes to edit a dict, but that dict seems to have moved to a different file by now. This commit edits that markdown file to reference the correct file path.", "806": "Typo in Prompt_Engineering_with_Llama_3.ipynb # What does this PR do?\n\nFixes a typo in the Prompt Engineering notebook to correctly name Groq as the service being called.\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [X] Looked at the updated notebook to confirm changes were visible\n\n## Before submitting\n- [X] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "609": "Fill in one sentence in the prompt guard tutorial.Simply fills in a single sentence from the tutorial, no functionality impact.\n\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n\n", "85": "Update paddings # What does this PR do?\n\nThis PR adds the update for padding with new tokenizer changes which requires change in embedding size. \n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nWithout resizing the embedding after adding Pad token, [error_logs](https://gist.github.com/HamidShojanazeri/f5f6e16931ddf260961c4f07f881f903)\n\n- [ ] Test B\nWith resizing the embedding after adding Pad token https://gist.github.com/HamidShojanazeri/3e9030419044104988795c9b1bce6e44\n\n\n## Before submitting\n- [ X] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ X] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "153": "Package distribution# What does this PR do?\nThis PR prepared the repo in a way that llama-recipes can be distributed as a package for easier usage and development.\nFor this the following changes are made:\n- Move source files int src/llama_recipes\n- Add pyproject.toml for building wheel\n- Make script runnable with python -m\n- Adopt documentation\n- Adding basic unit tests\n\nFixes # (issue)\n-NA-\n\n\n## Before submitting\n- [X] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [X] Did you make sure to update the documentation with your changes?  \n- [X] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "635": "recipes/quickstart/Getting_to_know_Llama.ipynb,  typo fix lama -> llama line 127# What does this PR do?\n\nin line 127 of llama-recipes/recipes/quickstart/Getting_to_know_Llama.ipynb, it says \n\nlama-3-1-405b --> llama-3-1-405b-instruct,\n\ninstead of \n\nllama-3-1-405b --> llama-3-1-405b-instruct,\n\n\n<!-- Remove if not applicable -->\n\n\n\n\n## Before submitting\n- [X] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [X] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "147": "LoRa Config parameter flowthrough fix# Fix to ensure that LoRa config parameters are not excluded\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\n## Feature/Issue validation/testing\n\nThe current LoRa config includes a `ClassVar` containing a list of the target modules.  The config is parsed out using dataclasses `fields` to get the typed parameters from the config.  However, `fields` excludes ClassVars, so target_modules or other similarly implemented parameters in the configs are excluded.\n\n#### Current implementation\n```\n>>> from typing import ClassVar, List\n>>> from dataclasses import fields, dataclass\n>>> \n>>> @dataclass\n... class lora_config:\n...    ...\n...     target_modules: ClassVar[List[str]] = (\"q_proj\", \"v_proj\")\n...    ... # other params\n... \n>>> params = {k.name: getattr(lora_config, k.name) for k in fields(lora_config)}\n>>> print(params)\n{'r': 8, 'lora_alpha': 32, 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}\n```\n\n#### Proposed fix\n```\n>>> from dataclasses import dataclass, field, asdict\n>>> from typing import List\n>>> \n>>> @dataclass\n... class lora_config:\n...     ... \n...     target_modules: List[str] = field(default_factory=lambda: [\"q_proj\", \"v_proj\"])\n...    ... # other params\n... \n>>> params = asdict(lora_config())\n>>> print(params)\n{'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'v_proj'], 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}\n```\n\nThis behavior is stated in the python documentation for fields in dataclasses [here](https://docs.python.org/3/library/dataclasses.html#class-variables).\n\nI have also confirmed that this works in flowing through the config all the way into the LoraConfig, and the training script runs properly.\n\n```\n>>> from peft import LoraConfig\n>>> \n>>> params = asdict(lora_config())\n>>> config = LoraConfig(**params)\n>>> print(config)\nLoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='CAUSAL_LM', inference_mode=False, r=8, target_modules=['q_proj', 'v_proj'], lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None)\n```\n\nThis PR also includes a small typo fix.\n\n## Before submitting\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n\nThanks for contributing \ud83c\udf89!\n", "392": "Fix checkpoint converter to use arguments before default behavior of checking yamlScript ignores `HF_model_path_or_name` argument even if supplied and skips to getting model name from yaml file. Test if argument exists before using default approach.\n\nUnfortunately specifying a path causes the script to break.\n\n# What does this PR do?\n\nThis fixes the faulty default behavior of the converter script.\n\nFixes # (issue)\nNo issue\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\nNo tests needed.\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "386": "Updating the AWS Prompt_Engineering Notebook + Adding an Example of ReAct with Llama 2 on Bedrock# What does this PR do?\n\nPrompt_Engineering_with_Llama_2_On_Amazon_Bedrock.ipynb\n+ Added in a section to help better understand how to use the [INST] tags, updated some examples, and added reference to the Deeplearning.AI Prompt Engineering course\n\nReAct_Llama_2_Bedrock-WK.ipynb\n+ Adding an example of ReAct using Llama 2\n+ Short explanation of ReAct\n+ Setup and configuration to use Amazon Bedrock\n+ Example of using the Bedrock api via langchain\n+ Setup for use of DuckDuckGoSearchRun, WikipediaAPIWrapper, and PythonREPL.\n+ Created a pattern for the model to follow in order to use the tools and do reasoning similar to CoT.\n+ Cleaned up and formatted the generated text before giving it to the corresponding tool.\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [x] Did you make sure to update the documentation with your changes?  Documentation is in the notebooks\n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "379": "[Demos] Adding OctoAI API Examples (Hosted Llama solution)# What does this PR do?\n\nOctoAI provides hosted models for Llama2, LlamaGuard, CodeLlama, and embedding models among many other open source models.\n\nThis PR introduces demo examples that are ports of existing notebooks that originally used a mix of Replicate hosted Llama2, Huggingface-based Embedding model, and Anyscale hosted Llama-Guard. As a substitute, we're using OctoAI to host the Llama models and Embedding models, thus simplifying the demos and notebooks.\n\nNote also that the RAG example originally relied on hosting Llama locally; now we're just relying on OctoAI here to host the Llama model.\n\n## Feature/Issue validation/testing\n\nAll python notebooks were run on a Macbook M1 laptop successfully.\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [x] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "437": "Add a README to provide links to API providers# What does this PR do?\n\nAdd a README.md to this directory that links to API providers that support Meta Llama.\n\n## Before submitting\n- [X] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "351": "Update README.md# What does this PR do?\n\nRemove dead url in demo_apps readme\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [X] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [X] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "350": "fix of dead link in demo apps readme# What does this PR do?\nfix of dead link to 7b quantized model\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "152": "Remove micro_batch_training parameter ...and replace with gradient_accumation_steps\n\n# What does this PR do?\n\nOur current program logic does not honor the micro_batch_training parameter. Instead it only derives the gradient_accumulation_steps from it. The training itself then runs with training_batch_size which can lead to OOMs which users want to avoid by setting a smaller micro_batch_size. This PR removes the micro_batch_size and introduces gradient_accumulation_steps as a parameter directly. This way its more clear to the user how to control batch size.\n\nFixes # (issue)\n-NA-\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [X] Test A\nRun\n```\npython llama_finetuning.py  --use_peft --peft_method lora --quantization  --model_name ../llama/llama-7b-hf/ --output_dir peft_output --batch_size_training 2 --gradient_accumulation_steps 2\n```\nLogs for Test A\n```\n\n===================================BUG REPORT===================================\nWelcome to bitsandbytes. For bug reports, please run\n\npython -m bitsandbytes\n\n and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n================================================================================\n/home/ubuntu/miniconda3/envs/llama/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:147: UserWarning: /home/ubuntu/miniconda3/envs/llama did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n  warn(msg)\nCUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\nCUDA SETUP: Highest compute capability among GPUs detected: 8.6\nCUDA SETUP: Detected CUDA version 118\nCUDA SETUP: Loading binary /home/ubuntu/miniconda3/envs/llama/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\nOverriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:06<00:00,  3.29s/it]\n--> Model ../llama/llama-7b-hf/\n\n--> ../llama/llama-7b-hf/ has 262.41024 Million params\n\n/home/ubuntu/miniconda3/envs/llama/lib/python3.10/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n  warnings.warn(\ntrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\nFound cached dataset samsum (/home/ubuntu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\nLoading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-ee2b86ad0669b457.arrow\nLoading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-4a0f7fcce9aae34e.arrow\nLoading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-1140901f37f0315a.arrow\n--> Training Set Length = 1553\nFound cached dataset samsum (/home/ubuntu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\nLoading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-1b1abf041e8a4f5e.arrow\nLoading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-0a650ba686879780.arrow\nLoading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-a6fef397d865a0d5.arrow\n--> Validation Set Length = 84\n/home/ubuntu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:329: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 0:   0%|                                                                                                                                                                                                  | 0/388 [00:00<?, ?it/s]\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n/home/ubuntu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/home/ubuntu/miniconda3/envs/llama/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\nTraining Epoch: 0/3, step 4/776 completed (loss: 1.0262432098388672):   0%|\u258e                                                                                                                                   | 1/388 [00:52<2:12:07, 20.48s/it]\n```\n\n\n## Before submitting\n- [X] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [X] Did you make sure to update the documentation with your changes?  \n- [X] Did you write any new necessary tests? (will be merged later)\n\nThanks for contributing \ud83c\udf89!\n", "84": "Cogamake inference accept a file with several prompts\n\nfix a bug in alpaca dataset that could truncate the response", "608": "Add PromptGuard to safety_utils [PromptGuard](https://github.com/meta-llama/PurpleLlama/tree/main/Prompt-Guard) has been introduced as a system safety tool to be used to check LLM prompts for malicious text. This PR adds this guard to the safety_utils module and adjusts recipes to use this new check. \n\n- Implementation details\n  - PromptGuard has a limit of 512 tokens. Naive sentence splitting is performed on inputs to try to ensure that this limit is not breached. A warning is printed if a sentence is longer than 512 tokens. \n  - Only the jailbreak score of PromptGuard is acted on. The other scores are intended to be used in agentic implemetations, of which there are none currently which use the safety_utils module. \n\nGeneral simple test\n```\necho \"hello\" |python3 inference.py \"meta-llama/Meta-Llama-3.1-8B-Instruct\"  --quantization '8bit' --use_fast_kernels --enable-promptguard-safety True \n```\n\nTo test the text splitter feature\n```\npython3 inference.py \"meta-llama/Meta-Llama-3.1-8B-Instruct\"  --quantization '8bit' --use_fast_kernels --enable-promptguard-safety True < ~/longprompt \n```", "185": "Fix typo in chat_completion.py# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n\n- [ ] Test A\nLogs for Test A\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "807": "fix link# What does this PR do?\n\n- Update documentation links to langchain.\n- Fix internal notebook link.\n- Remove public LANGCHAIN_API_KEY in the notebook\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "1": "remove unneeded imports for llama_finetuning.py", "754": "Update hello_llama_cloud.ipynbError \"The Notebook Does Not Appear to Be Valid JSON Using nbformat v5.10.4 and nbconvert v7.16.1\"\n\n# What does this PR do?\n\nResolve the error \"The Notebook Does Not Appear to Be Valid JSON\nUsing nbformat v5.10.4 and nbconvert v7.16.1\"\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "768": "Add E2B AI Analyst# What does this PR do?\n\nThis PR adds an AI data analyst that uses Llama APIs or Ollama to analyze data and generate charts. It uses Llama to generate the Python code and E2B to run the actual code.\n\nA live demo is available here: https://ai-analyst.e2b.dev/\n\n![Example-ezgif com-video-to-gif-converter](https://github.com/user-attachments/assets/3bc8b017-4a09-416c-b55c-ce53da7e5560)\n\nThe app is a self contained NextJS project that can be easily self-hosted on Vercel and integrated with API providers and Ollama via the .env file.\n\nWe have also signed the CLA.", "797": "[WIP] Add FSDP2 # What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "230": "Add missing amp context if use_fp16 is enabled# What does this PR do?\n\nThis PR enables amp if use_fp16 is selected/\n\nFixes # (issue)\n#119 \n\n## Feature/Issue validation/testing\n\nRan pytest and two training runs with fp16 enable and disabled to compare iteration speed.\n\n- [X] pytest tests\n```\n============================================================================================================ test session starts =============================================================================================================platform linux -- Python 3.10.11, pytest-7.3.2, pluggy-1.0.0\nrootdir: /home/ubuntu/llama-recipes\nplugins: mock-3.11.1, anyio-3.7.0, hydra-core-1.0.7\ncollected 8 items\n\ntests/test_finetuning.py ....                                                                                                                                                                                                          [ 50%]\ntests/test_train_utils.py .                                                                                                                                                                                                            [ 62%]\ntests/datasets/test_custom_dataset.py ..                                                                                                                                                                                               [ 87%]\ntests/datasets/test_samsum_datasets.py .                                                                                                                                                                                               [100%]\n\n============================================================================================================== warnings summary ==============================================================================================================src/llama_recipes/finetuning.py:5\n  /home/ubuntu/llama-recipes/src/llama_recipes/finetuning.py:5: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n    from pkg_resources import packaging\n\n../miniconda3/envs/llama/lib/python3.10/site-packages/pkg_resources/__init__.py:2871\n../miniconda3/envs/llama/lib/python3.10/site-packages/pkg_resources/__init__.py:2871\n../miniconda3/envs/llama/lib/python3.10/site-packages/pkg_resources/__init__.py:2871\n  /home/ubuntu/miniconda3/envs/llama/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n    declare_namespace(pkg)\n\n../miniconda3/envs/llama/lib/python3.10/site-packages/pkg_resources/__init__.py:2871\n  /home/ubuntu/miniconda3/envs/llama/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.logging')`.\n  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n    declare_namespace(pkg)\n\n../miniconda3/envs/llama/lib/python3.10/site-packages/pkg_resources/__init__.py:2350\n  /home/ubuntu/miniconda3/envs/llama/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n    declare_namespace(parent)\n\n../miniconda3/envs/llama/lib/python3.10/site-packages/pkg_resources/__init__.py:2871\n  /home/ubuntu/miniconda3/envs/llama/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.\n  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n    declare_namespace(pkg)\n\n../miniconda3/envs/llama/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:147\n  /home/ubuntu/miniconda3/envs/llama/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:147: UserWarning: /home/ubuntu/miniconda3/envs/llama did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n    warn(msg)\n\n../miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/_shard/checkpoint/__init__.py:8\n  /home/ubuntu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/_shard/checkpoint/__init__.py:8: DeprecationWarning: torch.distributed._shard.checkpoint will be deprecated, use torch.distributed.checkpoint instead\n    warnings.warn(\n\ntests/datasets/test_samsum_datasets.py::test_custom_dataset\n  /home/ubuntu/miniconda3/envs/llama/lib/python3.10/site-packages/dill/_dill.py:1845: PicklingWarning: Cannot locate reference to <class 'unittest.mock.MagicMock'>.\n    warnings.warn('Cannot locate reference to %r.' % (obj,), PicklingWarning)\n\ntests/datasets/test_samsum_datasets.py::test_custom_dataset\n  /home/ubuntu/miniconda3/envs/llama/lib/python3.10/site-packages/dill/_dill.py:1847: PicklingWarning: Cannot pickle <class 'unittest.mock.MagicMock'>: unittest.mock.MagicMock has recursive self-references that trigger a RecursionError.\n    warnings.warn('Cannot pickle %r: %s.%s has recursive self-references that trigger a RecursionError.' % (obj, obj.__module__, obj_name), PicklingWarning)\n\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n====================================================================================================== 8 passed, 11 warnings in 21.18s =======================================================================================================\n```\n\n\n- [X] CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 torchrun --nnodes 1 --nproc_per_node 6 examples/finetuning.py  --enable_fsdp --model_name meta-llama/Llama-2-7b-hf --use_peft --peft_method lora  --batch_size_training 8  --output_dir output_dir\n```\ndevgpu005:4164009:15461 [0] NCCL INFO comm 0x1068fba0 rank 0 nranks 6 cudaDev 0 busId 11000 - Init COMPLETE\nTraining Epoch: 1/3, step 1/32 completed (loss: 2.0164406299591064):   6%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                                                                                                            |\n 2/32 [01:30<22:26, 44.90s/it]\n```\n\n- [X] CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 torchrun --nnodes 1 --nproc_per_node 6 examples/finetuning.py  --enable_fsdp --model_name meta-llama/Llama-2-7b-hf --use_peft --peft_method lora  --batch_size_training 8  --output_dir output_dir --use_fp16\n```\ndevgpu005:66628:100235 [0] NCCL INFO comm 0x1066fa30 rank 0 nranks 6 cudaDev 0 busId 11000 - Init COMPLETE\nTraining Epoch: 1/3, step 4/32 completed (loss: 1.8937145471572876):  16%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                                                                                                |\n 5/32 [00:39<03:18,  7.36s/it]\n```\n\n## Before submitting\n- [X] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [X] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "542": "add multiturn-conversation dataset process# add multiturn-conversation dataset process\n\nFixes # not add multiturn-conversation dataset process\n", "224": "Updated quickstart notebook to use llama_recipes package# What does this PR do?\nThis PR updated the import statements in the quickstart notebbok to use the new package structure.\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [X] Ran the notebook ceels\n<img width=\"782\" alt=\"image\" src=\"https://github.com/facebookresearch/llama-recipes/assets/13337103/4ac6d19c-fe2b-4734-950a-f88285beacf7\">\n\n\n## Before submitting\n- [X] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n\nThanks for contributing \ud83c\udf89!\n", "581": "Updating the folder name 3p_integrationsUpdating the folder name to 3p_integrations", "280": "Add gradient clipping feature# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes #277 \n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "525": "Bump tj-actions/changed-files from 29.0.4 to 41.0.0 in /.github/workflowsBumps [tj-actions/changed-files](https://github.com/tj-actions/changed-files) from 29.0.4 to 41.0.0.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/tj-actions/changed-files/releases\">tj-actions/changed-files's releases</a>.</em></p>\n<blockquote>\n<h2>v41.0.0</h2>\n<h2>\ud83d\udd25 \ud83d\udd25 BREAKING CHANGE \ud83d\udd25 \ud83d\udd25</h2>\n<p>A new <code>safe_output</code> input is now available to prevent outputting unsafe filename characters (Enabled by default). This would escape characters in the filename that could be used for command injection.</p>\n<blockquote>\n<p>[!NOTE]\nThis can be disabled by setting the <code>safe_output</code> to false this comes with a recommendation to store all outputs generated in an environment variable first before using them.</p>\n</blockquote>\n<h4>Example</h4>\n<pre lang=\"yaml\"><code>...\n    - name: Get changed files\n      id: changed-files\n      uses: tj-actions/changed-files@v40\n      with:\n        safe_output: false # set to false because we are using an environment variable to store the output and avoid command injection.\n<pre><code>- name: List all added files\n  env:\n    ADDED_FILES: ${{ steps.changed-files.outputs.added_files }}\n  run: |\n    for file in &amp;quot;$ADDED_FILES&amp;quot;; do\n      echo &amp;quot;$file was added&amp;quot;\n    done\n</code></pre>\n<p>...\n</code></pre></p>\n<h2>What's Changed</h2>\n<ul>\n<li>chore(deps): update typescript-eslint monorepo to v6.15.0 by <a href=\"https://github.com/renovate\"><code>@\u200brenovate</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/1801\">tj-actions/changed-files#1801</a></li>\n<li>Upgraded to v40.2.3 by <a href=\"https://github.com/tj-actions-bot\"><code>@\u200btj-actions-bot</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/1800\">tj-actions/changed-files#1800</a></li>\n<li>chore(deps): update dependency eslint-plugin-prettier to v5.1.0 by <a href=\"https://github.com/renovate\"><code>@\u200brenovate</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/1802\">tj-actions/changed-files#1802</a></li>\n<li>chore(deps): lock file maintenance by <a href=\"https://github.com/renovate\"><code>@\u200brenovate</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/1803\">tj-actions/changed-files#1803</a></li>\n<li>chore(deps): update dependency eslint-plugin-prettier to v5.1.1 by <a href=\"https://github.com/renovate\"><code>@\u200brenovate</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/1804\">tj-actions/changed-files#1804</a></li>\n<li>fix: update safe output regex and the docs by <a href=\"https://github.com/tj-actions-bot\"><code>@\u200btj-actions-bot</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/1805\">tj-actions/changed-files#1805</a></li>\n<li>Revert &quot;chore(deps): update actions/download-artifact action to v4&quot; by <a href=\"https://github.com/jackton1\"><code>@\u200bjackton1</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/1806\">tj-actions/changed-files#1806</a></li>\n<li>Update README.md by <a href=\"https://github.com/jackton1\"><code>@\u200bjackton1</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/1808\">tj-actions/changed-files#1808</a></li>\n<li>chore(deps): lock file maintenance by <a href=\"https://github.com/renovate\"><code>@\u200brenovate</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/1809\">tj-actions/changed-files#1809</a></li>\n<li>Updated README.md by <a href=\"https://github.com/tj-actions-bot\"><code>@\u200btj-actions-bot</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/1810\">tj-actions/changed-files#1810</a></li>\n</ul>\n<p><strong>Full Changelog</strong>: <a href=\"https://github.com/tj-actions/changed-files/compare/v40...v41.0.0\">https://github.com/tj-actions/changed-files/compare/v40...v41.0.0</a></p>\n<h2>v41</h2>\n<h1>Changes in v41.1.2</h1>\n<h2>What's Changed</h2>\n<ul>\n<li>Upgraded to v41.1.1 by <a href=\"https://github.com/tj-actions-bot\"><code>@\u200btj-actions-bot</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/1854\">tj-actions/changed-files#1854</a></li>\n<li>chore(deps): update dependency prettier to v3.2.2 by <a href=\"https://github.com/renovate\"><code>@\u200brenovate</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/1855\">tj-actions/changed-files#1855</a></li>\n<li>chore(deps): lock file maintenance by <a href=\"https://github.com/renovate\"><code>@\u200brenovate</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/1856\">tj-actions/changed-files#1856</a></li>\n<li>chore(deps): update dependency <code>@\u200btypes/node</code> to v20.11.1 by <a href=\"https://github.com/renovate\"><code>@\u200brenovate</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/1857\">tj-actions/changed-files#1857</a></li>\n<li>chore(deps): update dependency <code>@\u200btypes/node</code> to v20.11.2 by <a href=\"https://github.com/renovate\"><code>@\u200brenovate</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/1858\">tj-actions/changed-files#1858</a></li>\n<li>chore(deps): update typescript-eslint monorepo to v6.19.0 by <a href=\"https://github.com/renovate\"><code>@\u200brenovate</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/1860\">tj-actions/changed-files#1860</a></li>\n</ul>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Changelog</summary>\n<p><em>Sourced from <a href=\"https://github.com/tj-actions/changed-files/blob/main/HISTORY.md\">tj-actions/changed-files's changelog</a>.</em></p>\n<blockquote>\n<h1><a href=\"https://github.com/tj-actions/changed-files/compare/v40.2.3...v41.0.0\">41.0.0</a> - (2023-12-23)</h1>\n<h2><!-- raw HTML omitted -->\ud83d\udc1b Bug Fixes</h2>\n<ul>\n<li>Update safe output regex and the docs (<a href=\"https://redirect.github.com/tj-actions/changed-files/issues/1805\">#1805</a>) (<a href=\"https://github.com/tj-actions/changed-files/commit/ff2f6e6b91913a7be42be1b5917330fe442f2ede\">ff2f6e6</a>)  - (tj-actions[bot])</li>\n</ul>\n<h2><!-- raw HTML omitted -->\u23ea Reverts</h2>\n<ul>\n<li>Revert &quot;chore(deps): update actions/download-artifact action to v4&quot; (<a href=\"https://redirect.github.com/tj-actions/changed-files/issues/1806\">#1806</a>)</li>\n</ul>\n<p>(<a href=\"https://github.com/tj-actions/changed-files/commit/4f573fed06c9abb5da4c72f75c1c320718114ff7\">4f573fe</a>)  - (Tonye Jack)</p>\n<h2><!-- raw HTML omitted -->\ud83d\udd04 Update</h2>\n<ul>\n<li>Update README.md (<a href=\"https://github.com/tj-actions/changed-files/commit/6e79d6e3dbe48946636c2939c80ff5c84ff7f9fe\">6e79d6e</a>)  - (Tonye Jack)</li>\n<li>Update README.md (<a href=\"https://github.com/tj-actions/changed-files/commit/d13ac1942fb3c1d7d32017915bb082cebe8a272a\">d13ac19</a>)  - (Tonye Jack)</li>\n<li>Update README.md (<a href=\"https://github.com/tj-actions/changed-files/commit/bb89f97963be96b39e1a303e64d5b91a1af4c340\">bb89f97</a>)  - (Tonye Jack)</li>\n<li>Updated README.md (<a href=\"https://redirect.github.com/tj-actions/changed-files/issues/1810\">#1810</a>)</li>\n</ul>\n<p>Co-authored-by: renovate[bot] <!-- raw HTML omitted --> (<a href=\"https://github.com/tj-actions/changed-files/commit/1864078d0afadf68ba489e671ecc09fefe8b70ab\">1864078</a>)  - (tj-actions[bot])</p>\n<ul>\n<li>Update README.md (<a href=\"https://redirect.github.com/tj-actions/changed-files/issues/1808\">#1808</a>)</li>\n</ul>\n<p>(<a href=\"https://github.com/tj-actions/changed-files/commit/47371c50e97c089212d9eb92ca26c8453224e78e\">47371c5</a>)  - (Tonye Jack)</p>\n<h2><!-- raw HTML omitted -->\ud83d\udcdd Other</h2>\n<ul>\n<li>Merge pull request from GHSA-mcph-m25j-8j63</li>\n</ul>\n<ul>\n<li>\n<p>feat: add <code>safe_output</code> input enabled by default</p>\n</li>\n<li>\n<p>fix: migrate README to safe uses of interpolation</p>\n</li>\n<li>\n<p>fix: README <code>uses</code> typo</p>\n</li>\n<li>\n<p>fix: README examples to account for newlines</p>\n</li>\n<li>\n<p>fix: README examples missing <code>safe_output</code></p>\n</li>\n<li>\n<p>fix: remove sanitization of <code>'</code></p>\n</li>\n<li>\n<p>fix: also sanitize <code>|&amp;;</code> (<a href=\"https://github.com/tj-actions/changed-files/commit/0102c07446a3cad972f4afcbd0ee4dbc4b6d2d1b\">0102c07</a>)  - (Jorge)</p>\n</li>\n</ul>\n<h2><!-- raw HTML omitted -->\u2699\ufe0f Miscellaneous Tasks</h2>\n<ul>\n<li><strong>deps:</strong> Lock file maintenance (<a href=\"https://github.com/tj-actions/changed-files/commit/f495a0321d3fffa62da2573adf70b77d5eb2f57a\">f495a03</a>)  - (renovate[bot])</li>\n<li><strong>deps:</strong> Update dependency eslint-plugin-prettier to v5.1.1 (<a href=\"https://github.com/tj-actions/changed-files/commit/089842a7a899531f61a45ef6ea69c485e1d62dbe\">089842a</a>)  - (renovate[bot])</li>\n<li><strong>deps:</strong> Lock file maintenance (<a href=\"https://github.com/tj-actions/changed-files/commit/787db0612e783421667a00319cf394b649682c4c\">787db06</a>)  - (renovate[bot])</li>\n<li><strong>deps:</strong> Update dependency eslint-plugin-prettier to v5.1.0 (<a href=\"https://github.com/tj-actions/changed-files/commit/4ef6b56482141a958bd3efb05520e4df9ecf4147\">4ef6b56</a>)  - (renovate[bot])</li>\n<li><strong>deps:</strong> Update typescript-eslint monorepo to v6.15.0 (<a href=\"https://github.com/tj-actions/changed-files/commit/c9ae347dbba64d95d83f36a0568e0e25a688dd1f\">c9ae347</a>)  - (renovate[bot])</li>\n</ul>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/tj-actions/changed-files/commit/6e79d6e3dbe48946636c2939c80ff5c84ff7f9fe\"><code>6e79d6e</code></a> Update README.md</li>\n<li><a href=\"https://github.com/tj-actions/changed-files/commit/d13ac1942fb3c1d7d32017915bb082cebe8a272a\"><code>d13ac19</code></a> Update README.md</li>\n<li><a href=\"https://github.com/tj-actions/changed-files/commit/bb89f97963be96b39e1a303e64d5b91a1af4c340\"><code>bb89f97</code></a> Update README.md</li>\n<li><a href=\"https://github.com/tj-actions/changed-files/commit/1864078d0afadf68ba489e671ecc09fefe8b70ab\"><code>1864078</code></a> Updated README.md (<a href=\"https://redirect.github.com/tj-actions/changed-files/issues/1810\">#1810</a>)</li>\n<li><a href=\"https://github.com/tj-actions/changed-files/commit/f495a0321d3fffa62da2573adf70b77d5eb2f57a\"><code>f495a03</code></a> chore(deps): lock file maintenance</li>\n<li><a href=\"https://github.com/tj-actions/changed-files/commit/47371c50e97c089212d9eb92ca26c8453224e78e\"><code>47371c5</code></a> Update README.md (<a href=\"https://redirect.github.com/tj-actions/changed-files/issues/1808\">#1808</a>)</li>\n<li><a href=\"https://github.com/tj-actions/changed-files/commit/4f573fed06c9abb5da4c72f75c1c320718114ff7\"><code>4f573fe</code></a> Revert &quot;chore(deps): update actions/download-artifact action to v4&quot; (<a href=\"https://redirect.github.com/tj-actions/changed-files/issues/1806\">#1806</a>)</li>\n<li><a href=\"https://github.com/tj-actions/changed-files/commit/ff2f6e6b91913a7be42be1b5917330fe442f2ede\"><code>ff2f6e6</code></a> fix: update safe output regex and the docs (<a href=\"https://redirect.github.com/tj-actions/changed-files/issues/1805\">#1805</a>)</li>\n<li><a href=\"https://github.com/tj-actions/changed-files/commit/0102c07446a3cad972f4afcbd0ee4dbc4b6d2d1b\"><code>0102c07</code></a> Merge pull request from GHSA-mcph-m25j-8j63</li>\n<li><a href=\"https://github.com/tj-actions/changed-files/commit/089842a7a899531f61a45ef6ea69c485e1d62dbe\"><code>089842a</code></a> chore(deps): update dependency eslint-plugin-prettier to v5.1.1</li>\n<li>Additional commits viewable in <a href=\"https://github.com/tj-actions/changed-files/compare/v29.0.4...v41.0.0\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=tj-actions/changed-files&package-manager=github_actions&previous-version=29.0.4&new-version=41.0.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/meta-llama/llama-recipes/network/alerts).\n\n</details>", "243": "Llama2 demo apps - 6 notebooks to run Llama2, with llama-cpp, LangChain and LlamaIndex integration# What does this PR do?\n\nThis PR starts with three quickstart demos showing how to run Llama2 locally on a Mac, remotely in the cloud, and on a Google Colab to ask Llama2 general questions or questions about unstructured data not trained for the model.\n\nThe PR also has three demos that ask Llama2 to summarize a YouTube video, to answer questions about structured data stored in a database, and to answer questions about live search results.\n\nFor more info, please see the README [here](https://github.com/jeffxtang/llama-recipes/tree/llama-demo-apps/demo_apps).\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\nAll the demos are provided as notebooks with running results shown for each cell. To test and confirm, please clone the PR and run the notebooks locally.\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [X] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [X] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "531": "Resume the fine-tuning process from the previous PEFT checkpoint folder# What does this PR do?\nAdded a new argument `from_peft_checkpoint` in the src/llama_recipes/finetuning.py to resume the fine-tuning process from the previous PEFT checkpoint folder. By default the `from_peft_checkpoint` is \"\". Once the `from_peft_checkpoint` string has been set, the fine-tune script will load the PEFT checkpoint to continue the fine-tune process. Must set the `--use_peft = True` and pass the checkpoint folder path to `--from_peft_checkpoint [PATH_TO_CHECKPOINT]`.  Since the peft checkpoint folder contains the `adapter_config.json `, the script will ignore the `--peft_method`and continue using the same PEFT config from `adapter_config.json`.  \n\nRequested by [Issue 424](https://github.com/meta-llama/llama-recipes/issues/424)\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [x] Finetune from lora checkpoint\n```\n ~/work/main/llama-recipes (feature/resume_finetune_peft)]$ torchrun --nnodes 1 --nproc_per_node 2  recipes/finetuning/finetuning.py  --from_peft_checkpoint finetune-output --num_epochs 1 --batch_size_training 4 --use_peft --peft_method lora  --model_name  meta-llama/Meta-Llama-3-8B-Instruct --enable_fsdp --output_dir ./finetune-output\nW0521 10:23:35.680000 139849501492224 torch/distributed/run.py:757] \nW0521 10:23:35.680000 139849501492224 torch/distributed/run.py:757] *****************************************\nW0521 10:23:35.680000 139849501492224 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0521 10:23:35.680000 139849501492224 torch/distributed/run.py:757] *****************************************\nClearing GPU cache for all ranks\n--> Running with torch dist debug set to detail\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:09<00:00,  2.46s/it]\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n--> Model meta-llama/Meta-Llama-3-8B-Instruct\n\n--> meta-llama/Meta-Llama-3-8B-Instruct has 8030.261248 Million params\n\nLoading checkpoint shards:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                           | 3/4 [00:09<00:03,  3.22s/it]trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.26047588741133265\nbFloat16 enabled for mixed precision - using bfSixteen policy\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:10<00:00,  2.67s/it]\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\ntrainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.26047588741133265\n--> applying fsdp activation checkpointing...\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/datasets/load.py:1486: FutureWarning: The repository for samsum contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/samsum\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n  warnings.warn(\n--> applying fsdp activation checkpointing...\n--> Training Set Length = 14732\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/datasets/load.py:1486: FutureWarning: The repository for samsum contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/samsum\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n  warnings.warn(\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/datasets/load.py:1486: FutureWarning: The repository for samsum contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/samsum\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n  warnings.warn(\n--> Validation Set Length = 818\nPreprocessing dataset:  29%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                                                          | 4279/14732 [00:00<00:01, 7032.83it/s]/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/datasets/load.py:1486: FutureWarning: The repository for samsum contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/samsum\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n  warnings.warn(\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:02<00:00, 6980.73it/s]\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 818/818 [00:00<00:00, 7196.16it/s]\nPreprocessing dataset:  45%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                          | 6610/14732 [00:00<00:01, 7217.18it/s]/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nPreprocessing dataset:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d               | 12521/14732 [00:01<00:00, 7219.75it/s]NCCL version 2.20.5+cuda12.4\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:02<00:00, 7196.24it/s]\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 818/818 [00:00<00:00, 7337.65it/s]\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1/1, step 78/79 completed (loss: 1.0950531959533691): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 79/79 [04:30<00:00,  3.43s/it]\nTraining Epoch: 1/1, step 78/79 completed (loss: 1.1393404006958008): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 79/79 [04:29<00:00,  3.41s/it]\nMax CUDA memory allocated was 45 GB\nMax CUDA memory reserved was 54 GB\nPeak active CUDA memory was 45 GB\nCUDA Malloc retries : 0\nCPU Total Peak Memory consumed during the train (max): 9 GB\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 17/17 [00:07<00:00,  2.18it/s]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 17/17 [00:07<00:00,  2.19it/s]\n eval_ppl=tensor(2.9572, device='cuda:0') eval_epoch_loss=tensor(1.0842, device='cuda:0')\nwe are about to save the PEFT modules\nPEFT modules are saved in ./finetune-output directory\nbest eval loss on epoch 1 is 1.0842361450195312\nEpoch 1: train_perplexity=2.8781, train_epoch_loss=1.0571, epoch time 271.33888043509796s\nKey: avg_train_prep, Value: 2.8780877590179443\nKey: avg_train_loss, Value: 1.0571260452270508\nKey: avg_eval_prep, Value: 2.9571800231933594\nKey: avg_eval_loss, Value: 1.0842361450195312\nKey: avg_epoch_time, Value: 271.33888043509796\nKey: avg_checkpoint_time, Value: 0.7499661762267351\n```\n- [x] Finetune with a llama-adapter checkpoint\n```\n~/work/main/llama-recipes (feature/resume_finetune_peft)]$ python recipes/finetuning/finetuning.py --num_epochs 1 --from_peft_checkpoint finetune-output --batch_size_training 1 --use_peft --peft_method llama-adapter  --model_name  meta-llama/Meta-Llama-3-8B-Instruct --output_dir ./finetune-output\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:02<00:00,  1.95it/s]\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n--> Model meta-llama/Meta-Llama-3-8B-Instruct\n\n--> meta-llama/Meta-Llama-3-8B-Instruct has 8030.261248 Million params\n\ntrainable params: 1,228,830 || all params: 8,031,490,078 || trainable%: 0.015300149636815625\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/datasets/load.py:1486: FutureWarning: The repository for samsum contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/samsum\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n  warnings.warn(\n--> Training Set Length = 14732\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/datasets/load.py:1486: FutureWarning: The repository for samsum contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/samsum\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n  warnings.warn(\n--> Validation Set Length = 818\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:02<00:00, 7138.91it/s]\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 818/818 [00:00<00:00, 7417.18it/s]\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1/1, step 398/639 completed (loss: 1.3428374528884888):  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588         Training Epoch: 1/1, step 638/639 completed (loss: 1.2846676111221313): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 639/639 [37:51<00:00,  3.55s/it]\nMax CUDA memory allocated was 69 GB\nMax CUDA memory reserved was 70 GB\nPeak active CUDA memory was 69 GB\nCUDA Malloc retries : 0\nCPU Total Peak Memory consumed during the train (max): 8 GB\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 34/34 [00:58<00:00,  1.73s/it]\n eval_ppl=tensor(3.3925, device='cuda:0') eval_epoch_loss=tensor(1.2216, device='cuda:0')\nwe are about to save the PEFT modules\nPEFT modules are saved in ./finetune-output directory\nbest eval loss on epoch 1 is 1.2215808629989624\nEpoch 1: train_perplexity=3.5618, train_epoch_loss=1.2703, epoch time 2271.8472441458143s\nKey: avg_train_prep, Value: 3.5617709159851074\nKey: avg_train_loss, Value: 1.270257830619812\nKey: avg_eval_prep, Value: 3.3925466537475586\nKey: avg_eval_loss, Value: 1.2215808629989624\nKey: avg_epoch_time, Value: 2271.8472441458143\nKey: avg_checkpoint_time, Value: 0.3230209848843515\n```\n- [x] Finetune without a peft checkpoint\n```\n~/work/main/llama-recipes (feature/resume_finetune_peft)]$ torchrun --nnodes 1 --nproc_per_node 2  recipes/finetuning/finetuning.py  --num_epochs 1 --batch_size_training 4 --use_peft --peft_method lora  --model_name  meta-llama/Meta-Llama-3-8B-Instruct --enable_fsdp --output_dir ./finetune-output\nW0521 10:16:27.114000 140542763557888 torch/distributed/run.py:757] \nW0521 10:16:27.114000 140542763557888 torch/distributed/run.py:757] *****************************************\nW0521 10:16:27.114000 140542763557888 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0521 10:16:27.114000 140542763557888 torch/distributed/run.py:757] *****************************************\nClearing GPU cache for all ranks\n--> Running with torch dist debug set to detail\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:09<00:00,  2.45s/it]\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n--> Model meta-llama/Meta-Llama-3-8B-Instruct\n\n--> meta-llama/Meta-Llama-3-8B-Instruct has 8030.261248 Million params\n\ntrainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.26047588741133265\nbFloat16 enabled for mixed precision - using bfSixteen policy\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:09<00:00,  2.40s/it]\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\ntrainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.26047588741133265\n--> applying fsdp activation checkpointing...\n--> applying fsdp activation checkpointing...\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/datasets/load.py:1486: FutureWarning: The repository for samsum contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/samsum\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n  warnings.warn(\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/datasets/load.py:1486: FutureWarning: The repository for samsum contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/samsum\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n  warnings.warn(\nDownloading builder script: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.36k/3.36k [00:00<00:00, 29.1MB/s]\nDownloading readme: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7.04k/7.04k [00:00<00:00, 47.1MB/s]\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:00<00:00, 45080.76 examples/s]\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:00<00:00, 44998.82 examples/s]\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:04<00:00, 2998.08 examples/s]\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:05<00:00, 2747.66 examples/s]\n--> Training Set Length = 14732\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/datasets/load.py:1486: FutureWarning: The repository for samsum contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/samsum\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n  warnings.warn(\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/datasets/load.py:1486: FutureWarning: The repository for samsum contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/samsum\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n  warnings.warn(\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 818/818 [00:00<00:00, 34394.71 examples/s]\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 818/818 [00:00<00:00, 2998.68 examples/s]\nPreprocessing dataset:   5%|\u2588\u2588\u2588\u2588\u2588\u258e                                                                                                    | 734/14732 [00:00<00:01, 7336.56it/s]--> Validation Set Length = 818\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:02<00:00, 7225.25it/s]\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 818/818 [00:00<00:00, 7305.61it/s]\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:02<00:00, 7275.76it/s]\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 818/818 [00:00<00:00, 7542.15it/s]\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                                                                                             | 0/79 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                                                                                             | 0/79 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nNCCL version 2.20.5+cuda12.4\nTraining Epoch: 1/1, step 78/79 completed (loss: 1.151496410369873): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 79/79 [04:34<00:00,  3.48s/it]\nTraining Epoch: 1/1, step 78/79 completed (loss: 1.1079908609390259): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 79/79 [04:34<00:00,  3.48s/it]\nMax CUDA memory allocated was 45 GB\nMax CUDA memory reserved was 54 GB\nPeak active CUDA memory was 45 GB\nCUDA Malloc retries : 0\nCPU Total Peak Memory consumed during the train (max): 9 GB\nevaluating Epoch:   0%|                                                                                                                              | 0/17 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 17/17 [00:07<00:00,  2.26it/s]\nevaluating Epoch: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 17/17 [00:07<00:00,  2.25it/s]\n eval_ppl=tensor(2.9739, device='cuda:0') eval_epoch_loss=tensor(1.0899, device='cuda:0')\nwe are about to save the PEFT modules\nPEFT modules are saved in ./finetune-output directory\nbest eval loss on epoch 1 is 1.0898709297180176\nEpoch 1: train_perplexity=3.2454, train_epoch_loss=1.1772, epoch time 275.21213593892753s\nKey: avg_train_prep, Value: 3.24538516998291\nKey: avg_train_loss, Value: 1.1772340536117554\nKey: avg_eval_prep, Value: 2.9738903045654297\nKey: avg_eval_loss, Value: 1.0898709297180176\nKey: avg_epoch_time, Value: 275.21213593892753\nKey: avg_checkpoint_time, Value: 0.8217994091100991\n```\n\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [x] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [x] Did you make sure to update the documentation with your changes?  \n- [x] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "928": "Llama 4 api recipes# What does this PR do?\n\nUpdating Llama Cookbook for api support\n\nThanks for contributing \ud83c\udf89!\n", "900": "Bump tj-actions/changed-files from 41.0.0 to 45.0.8 in /.github/workflowsBumps [tj-actions/changed-files](https://github.com/tj-actions/changed-files) from 41.0.0 to 45.0.8.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/tj-actions/changed-files/releases\">tj-actions/changed-files's releases</a>.</em></p>\n<blockquote>\n<h2>v45.0.8</h2>\n<h2>What's Changed</h2>\n<ul>\n<li>Upgraded to v45.0.4 by <a href=\"https://github.com/tj-actions-bot\"><code>@\u200btj-actions-bot</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/2344\">tj-actions/changed-files#2344</a></li>\n<li>chore(deps): lock file maintenance by <a href=\"https://github.com/renovate\"><code>@\u200brenovate</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/2345\">tj-actions/changed-files#2345</a></li>\n<li>chore(deps): update dependency <code>@\u200bvercel/ncc</code> to v0.38.3 by <a href=\"https://github.com/renovate\"><code>@\u200brenovate</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/2348\">tj-actions/changed-files#2348</a></li>\n<li>chore(deps): lock file maintenance by <a href=\"https://github.com/renovate\"><code>@\u200brenovate</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/2349\">tj-actions/changed-files#2349</a></li>\n<li>chore(deps): update dependency <code>@\u200btypes/node</code> to v22.9.1 by <a href=\"https://github.com/renovate\"><code>@\u200brenovate</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/2352\">tj-actions/changed-files#2352</a></li>\n<li>chore(deps): bump yaml from 2.6.0 to 2.6.1 by <a href=\"https://github.com/dependabot\"><code>@\u200bdependabot</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/2353\">tj-actions/changed-files#2353</a></li>\n<li>chore(deps-dev): bump eslint-plugin-github from 5.0.2 to 5.1.1 by <a href=\"https://github.com/dependabot\"><code>@\u200bdependabot</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/2356\">tj-actions/changed-files#2356</a></li>\n<li>chore(deps): update dependency typescript to v5.7.2 by <a href=\"https://github.com/renovate\"><code>@\u200brenovate</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/2357\">tj-actions/changed-files#2357</a></li>\n<li>chore(deps): update dependency <code>@\u200btypes/node</code> to v22.9.2 by <a href=\"https://github.com/renovate\"><code>@\u200brenovate</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/2358\">tj-actions/changed-files#2358</a></li>\n<li>chore(deps): update dependency <code>@\u200btypes/node</code> to v22.9.3 by <a href=\"https://github.com/renovate\"><code>@\u200brenovate</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/2359\">tj-actions/changed-files#2359</a></li>\n<li>chore(deps): lock file maintenance by <a href=\"https://github.com/renovate\"><code>@\u200brenovate</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/2360\">tj-actions/changed-files#2360</a></li>\n<li>chore(deps): update dependency <code>@\u200btypes/node</code> to v22.9.4 by <a href=\"https://github.com/renovate\"><code>@\u200brenovate</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/2361\">tj-actions/changed-files#2361</a></li>\n<li>chore(deps): update dependency <code>@\u200btypes/node</code> to v22.10.0 by <a href=\"https://github.com/renovate\"><code>@\u200brenovate</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/2364\">tj-actions/changed-files#2364</a></li>\n<li>chore(deps): update dependency prettier to v3.4.0 by <a href=\"https://github.com/renovate\"><code>@\u200brenovate</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/2365\">tj-actions/changed-files#2365</a></li>\n<li>chore(deps): update dependency prettier to v3.4.1 by <a href=\"https://github.com/renovate\"><code>@\u200brenovate</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/2366\">tj-actions/changed-files#2366</a></li>\n<li>chore(deps): update dependency eslint-plugin-github to v5.1.3 by <a href=\"https://github.com/renovate\"><code>@\u200brenovate</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/2367\">tj-actions/changed-files#2367</a></li>\n<li>chore(deps): update dependency <code>@\u200btypes/node</code> to v22.10.1 by <a href=\"https://github.com/renovate\"><code>@\u200brenovate</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/2368\">tj-actions/changed-files#2368</a></li>\n<li>chore(deps): lock file maintenance by <a href=\"https://github.com/renovate\"><code>@\u200brenovate</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/2369\">tj-actions/changed-files#2369</a></li>\n<li>chore(deps): update dependency prettier to v3.4.2 by <a href=\"https://github.com/renovate\"><code>@\u200brenovate</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/2370\">tj-actions/changed-files#2370</a></li>\n<li>chore(deps): update dependency eslint-plugin-github to v5.1.4 by <a href=\"https://github.com/renovate\"><code>@\u200brenovate</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/2372\">tj-actions/changed-files#2372</a></li>\n<li>Upgraded to v45.0.5 by <a href=\"https://github.com/tj-actions-bot\"><code>@\u200btj-actions-bot</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/2374\">tj-actions/changed-files#2374</a></li>\n<li>chore(deps): lock file maintenance by <a href=\"https://github.com/renovate\"><code>@\u200brenovate</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/2375\">tj-actions/changed-files#2375</a></li>\n<li>chore(deps): update dependency <code>@\u200btypes/node</code> to v22.10.2 by <a href=\"https://github.com/renovate\"><code>@\u200brenovate</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/2376\">tj-actions/changed-files#2376</a></li>\n<li>chore(deps): lock file maintenance by <a href=\"https://github.com/renovate\"><code>@\u200brenovate</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/2377\">tj-actions/changed-files#2377</a></li>\n<li>chore(deps): update dependency eslint-plugin-jest to v28.10.0 by <a href=\"https://github.com/renovate\"><code>@\u200brenovate</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/2378\">tj-actions/changed-files#2378</a></li>\n<li>chore(deps): lock file maintenance by <a href=\"https://github.com/renovate\"><code>@\u200brenovate</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/2379\">tj-actions/changed-files#2379</a></li>\n<li>chore(deps): update peter-evans/create-pull-request action to v7.0.6 by <a href=\"https://github.com/renovate\"><code>@\u200brenovate</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/2380\">tj-actions/changed-files#2380</a></li>\n<li>chore(deps): lock file maintenance by <a href=\"https://github.com/renovate\"><code>@\u200brenovate</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/2382\">tj-actions/changed-files#2382</a></li>\n<li>fix(deps): update dependency yaml to v2.7.0 by <a href=\"https://github.com/renovate\"><code>@\u200brenovate</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/2383\">tj-actions/changed-files#2383</a></li>\n<li>chore(deps): update dependency <code>@\u200btypes/node</code> to v22.10.3 by <a href=\"https://github.com/renovate\"><code>@\u200brenovate</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/2385\">tj-actions/changed-files#2385</a></li>\n<li>chore(deps): update dependency <code>@\u200btypes/node</code> to v22.10.4 by <a href=\"https://github.com/renovate\"><code>@\u200brenovate</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/2386\">tj-actions/changed-files#2386</a></li>\n<li>chore(deps): update dependency <code>@\u200btypes/node</code> to v22.10.5 by <a href=\"https://github.com/renovate\"><code>@\u200brenovate</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/2387\">tj-actions/changed-files#2387</a></li>\n<li>chore(deps): update dependency <code>@\u200btypes/lodash</code> to v4.17.14 by <a href=\"https://github.com/renovate\"><code>@\u200brenovate</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/2388\">tj-actions/changed-files#2388</a></li>\n<li>Upgraded to v45.0.6 by <a href=\"https://github.com/tj-actions-bot\"><code>@\u200btj-actions-bot</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/2389\">tj-actions/changed-files#2389</a></li>\n<li>chore(deps): lock file maintenance by <a href=\"https://github.com/renovate\"><code>@\u200brenovate</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/2390\">tj-actions/changed-files#2390</a></li>\n<li>chore(deps): update dependency eslint-plugin-github to v5.1.5 by <a href=\"https://github.com/renovate\"><code>@\u200brenovate</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/2392\">tj-actions/changed-files#2392</a></li>\n<li>chore(deps): update dependency typescript to v5.7.3 by <a href=\"https://github.com/renovate\"><code>@\u200brenovate</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/2393\">tj-actions/changed-files#2393</a></li>\n<li>fix(deps): update dependency <code>@\u200boctokit/rest</code> to v21.1.0 by <a href=\"https://github.com/renovate\"><code>@\u200brenovate</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/2394\">tj-actions/changed-files#2394</a></li>\n<li>chore(deps): lock file maintenance by <a href=\"https://github.com/renovate\"><code>@\u200brenovate</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/2395\">tj-actions/changed-files#2395</a></li>\n<li>chore(deps): update dependency eslint-config-prettier to v10 by <a href=\"https://github.com/renovate\"><code>@\u200brenovate</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/2396\">tj-actions/changed-files#2396</a></li>\n<li>chore(deps): update dependency <code>@\u200btypes/node</code> to v22.10.6 by <a href=\"https://github.com/renovate\"><code>@\u200brenovate</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/2397\">tj-actions/changed-files#2397</a></li>\n<li>chore(deps): update dependency eslint-plugin-prettier to v5.2.2 by <a href=\"https://github.com/renovate\"><code>@\u200brenovate</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/2399\">tj-actions/changed-files#2399</a></li>\n<li>chore(deps): update dependency eslint-plugin-jest to v28.11.0 by <a href=\"https://github.com/renovate\"><code>@\u200brenovate</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/2400\">tj-actions/changed-files#2400</a></li>\n<li>chore(deps): update dependency <code>@\u200btypes/node</code> to v22.10.7 by <a href=\"https://github.com/renovate\"><code>@\u200brenovate</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/2403\">tj-actions/changed-files#2403</a></li>\n<li>chore(deps): update dependency eslint-plugin-prettier to v5.2.3 by <a href=\"https://github.com/renovate\"><code>@\u200brenovate</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/2405\">tj-actions/changed-files#2405</a></li>\n<li>chore(deps): lock file maintenance by <a href=\"https://github.com/renovate\"><code>@\u200brenovate</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/2406\">tj-actions/changed-files#2406</a></li>\n<li>chore(deps): update dependency <code>@\u200btypes/node</code> to v22.10.8 by <a href=\"https://github.com/renovate\"><code>@\u200brenovate</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/2407\">tj-actions/changed-files#2407</a></li>\n<li>chore(deps): update dependency <code>@\u200btypes/node</code> to v22.10.9 by <a href=\"https://github.com/renovate\"><code>@\u200brenovate</code></a> in <a href=\"https://redirect.github.com/tj-actions/changed-files/pull/2408\">tj-actions/changed-files#2408</a></li>\n</ul>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Changelog</summary>\n<p><em>Sourced from <a href=\"https://github.com/tj-actions/changed-files/blob/main/HISTORY.md\">tj-actions/changed-files's changelog</a>.</em></p>\n<blockquote>\n<h1>Changelog</h1>\n<h1><a href=\"https://github.com/tj-actions/changed-files/compare/v46.0.0...v46.0.1\">46.0.1</a> - (2025-03-16)</h1>\n<h2><!-- raw HTML omitted -->\ud83d\udd04 Update</h2>\n<ul>\n<li>Updated README.md (<a href=\"https://redirect.github.com/tj-actions/changed-files/issues/2473\">#2473</a>)</li>\n</ul>\n<p>Co-authored-by: github-actions[bot] <!-- raw HTML omitted --> (<a href=\"https://github.com/tj-actions/changed-files/commit/2f7c5bfce28377bc069a65ba478de0a74aa0ca32\">2f7c5bf</a>)  - (github-actions[bot])</p>\n<ul>\n<li>Sync-release-version.yml to use signed commits (<a href=\"https://redirect.github.com/tj-actions/changed-files/issues/2472\">#2472</a>) (<a href=\"https://github.com/tj-actions/changed-files/commit/4189ec62c445484531e9ad97157d990be96e88ee\">4189ec6</a>)  - (Tonye Jack)</li>\n</ul>\n<h1><a href=\"https://github.com/tj-actions/changed-files/compare/v45.0.9...v46.0.0\">46.0.0</a> - (2025-03-16)</h1>\n<h2><!-- raw HTML omitted -->\ud83d\udc1b Bug Fixes</h2>\n<ul>\n<li>Update update-readme.yml to sign-commits (<a href=\"https://redirect.github.com/tj-actions/changed-files/issues/2468\">#2468</a>) (<a href=\"https://github.com/tj-actions/changed-files/commit/0f1ffe61855cb317d5fd66122c14dc0627eab141\">0f1ffe6</a>)  - (Tonye Jack)</li>\n<li>Update permission in update-readme.yml workflow (<a href=\"https://redirect.github.com/tj-actions/changed-files/issues/2467\">#2467</a>) (<a href=\"https://github.com/tj-actions/changed-files/commit/ddef03e37c84cfb9ee89fa055b86359aaf949c86\">ddef03e</a>)  - (Tonye Jack)</li>\n<li>Update github workflow update-readme.yml (<a href=\"https://redirect.github.com/tj-actions/changed-files/issues/2466\">#2466</a>) (<a href=\"https://github.com/tj-actions/changed-files/commit/9c2df0d54a911c819d7368d7e5ed7c01c0796e0a\">9c2df0d</a>)  - (Tonye Jack)</li>\n</ul>\n<h2><!-- raw HTML omitted -->\u2796 Remove</h2>\n<ul>\n<li>Deleted renovate.json (<a href=\"https://github.com/tj-actions/changed-files/commit/e37e952786556966c1fb6183c5937b3966bab099\">e37e952</a>)  - (Tonye Jack)</li>\n</ul>\n<h2><!-- raw HTML omitted -->\ud83d\udd04 Update</h2>\n<ul>\n<li>Sync-release-version.yml (<a href=\"https://redirect.github.com/tj-actions/changed-files/issues/2471\">#2471</a>) (<a href=\"https://github.com/tj-actions/changed-files/commit/4cd184a1dd542b79cca1d4d7938e4154a6520ca7\">4cd184a</a>)  - (Tonye Jack)</li>\n<li>Updated README.md (<a href=\"https://redirect.github.com/tj-actions/changed-files/issues/2469\">#2469</a>)</li>\n</ul>\n<p>Co-authored-by: github-actions[bot] <!-- raw HTML omitted --> (<a href=\"https://github.com/tj-actions/changed-files/commit/5cbf22026d05fbef0c027d1b1f118fe3a1b6e435\">5cbf220</a>)  - (github-actions[bot])</p>\n<h2><!-- raw HTML omitted -->\ud83d\udcda Documentation</h2>\n<ul>\n<li>Update docs to highlight security issues (<a href=\"https://redirect.github.com/tj-actions/changed-files/issues/2465\">#2465</a>) (<a href=\"https://github.com/tj-actions/changed-files/commit/65253327cf47481b4b1b4b9fea78e143a1353147\">6525332</a>)  - (Tonye Jack)</li>\n</ul>\n<h1><a href=\"https://github.com/tj-actions/changed-files/compare/v45.0.4...v45.0.9\">45.0.9</a> - (2025-03-15)</h1>\n<h2><!-- raw HTML omitted -->\ud83d\udc1b Bug Fixes</h2>\n<ul>\n<li><strong>deps:</strong> Update dependency <code>@\u200boctokit/rest</code> to v21.1.1 (<a href=\"https://redirect.github.com/tj-actions/changed-files/issues/2435\">#2435</a>) (<a href=\"https://github.com/tj-actions/changed-files/commit/fb8dcda5fb8954cec37773d2b275a8579c86c781\">fb8dcda</a>)  - (renovate[bot])</li>\n<li><strong>deps:</strong> Update dependency <code>@\u200boctokit/rest</code> to v21.1.0 (<a href=\"https://redirect.github.com/tj-actions/changed-files/issues/2394\">#2394</a>) (<a href=\"https://github.com/tj-actions/changed-files/commit/7b72c97d739f955f5cadca0d59799d826ae9f6c9\">7b72c97</a>)  - (renovate[bot])</li>\n<li><strong>deps:</strong> Update dependency yaml to v2.7.0 (<a href=\"https://redirect.github.com/tj-actions/changed-files/issues/2383\">#2383</a>) (<a href=\"https://github.com/tj-actions/changed-files/commit/5f974c28f5044c411f0c9e7becf3f172029cf9cf\">5f974c2</a>)  - (renovate[bot])</li>\n</ul>\n<h2><!-- raw HTML omitted -->\u2699\ufe0f Miscellaneous Tasks</h2>\n<ul>\n<li><strong>deps:</strong> Lock file maintenance (<a href=\"https://redirect.github.com/tj-actions/changed-files/issues/2460\">#2460</a>) (<a href=\"https://github.com/tj-actions/changed-files/commit/9200e69727eb73eb060652b19946b8a2fdfb654b\">9200e69</a>)  - (renovate[bot])</li>\n<li><strong>deps:</strong> Update dependency <code>@\u200btypes/node</code> to v22.13.10 (<a href=\"https://redirect.github.com/tj-actions/changed-files/issues/2459\">#2459</a>) (<a href=\"https://github.com/tj-actions/changed-files/commit/e650cfdae513481a20f538e88d98b39106523006\">e650cfd</a>)  - (renovate[bot])</li>\n<li><strong>deps:</strong> Update dependency eslint-config-prettier to v10.1.1 (<a href=\"https://redirect.github.com/tj-actions/changed-files/issues/2458\">#2458</a>) (<a href=\"https://github.com/tj-actions/changed-files/commit/82af21f4a05896ca18c950539469bee225c45a89\">82af21f</a>)  - (renovate[bot])</li>\n<li><strong>deps:</strong> Update dependency eslint-config-prettier to v10.1.0 (<a href=\"https://redirect.github.com/tj-actions/changed-files/issues/2457\">#2457</a>) (<a href=\"https://github.com/tj-actions/changed-files/commit/82fa4a6402582d5c8c9c0e95b7ff7cc88992bbb4\">82fa4a6</a>)  - (renovate[bot])</li>\n<li><strong>deps:</strong> Update peter-evans/create-pull-request action to v7.0.8 (<a href=\"https://redirect.github.com/tj-actions/changed-files/issues/2455\">#2455</a>) (<a href=\"https://github.com/tj-actions/changed-files/commit/315505acf41d2913b71af48080fb158cd01f79e7\">315505a</a>)  - (renovate[bot])</li>\n<li><strong>deps:</strong> Update dependency <code>@\u200btypes/node</code> to v22.13.9 (<a href=\"https://redirect.github.com/tj-actions/changed-files/issues/2454\">#2454</a>) (<a href=\"https://github.com/tj-actions/changed-files/commit/c8e1cdb9ea135ee549963c167ffaec5e7d4a71cd\">c8e1cdb</a>)  - (renovate[bot])</li>\n</ul>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/tj-actions/changed-files/commit/a284dc1814e3fd07f2e34267fc8f81227ed29fb8\"><code>a284dc1</code></a> Upgraded to v45.0.8 (<a href=\"https://redirect.github.com/tj-actions/changed-files/issues/2462\">#2462</a>)</li>\n<li><a href=\"https://github.com/tj-actions/changed-files/commit/9200e69727eb73eb060652b19946b8a2fdfb654b\"><code>9200e69</code></a> chore(deps): lock file maintenance (<a href=\"https://redirect.github.com/tj-actions/changed-files/issues/2460\">#2460</a>)</li>\n<li><a href=\"https://github.com/tj-actions/changed-files/commit/e650cfdae513481a20f538e88d98b39106523006\"><code>e650cfd</code></a> chore(deps): update dependency <code>@\u200btypes/node</code> to v22.13.10 (<a href=\"https://redirect.github.com/tj-actions/changed-files/issues/2459\">#2459</a>)</li>\n<li><a href=\"https://github.com/tj-actions/changed-files/commit/82af21f4a05896ca18c950539469bee225c45a89\"><code>82af21f</code></a> chore(deps): update dependency eslint-config-prettier to v10.1.1 (<a href=\"https://redirect.github.com/tj-actions/changed-files/issues/2458\">#2458</a>)</li>\n<li><a href=\"https://github.com/tj-actions/changed-files/commit/82fa4a6402582d5c8c9c0e95b7ff7cc88992bbb4\"><code>82fa4a6</code></a> chore(deps): update dependency eslint-config-prettier to v10.1.0 (<a href=\"https://redirect.github.com/tj-actions/changed-files/issues/2457\">#2457</a>)</li>\n<li><a href=\"https://github.com/tj-actions/changed-files/commit/315505acf41d2913b71af48080fb158cd01f79e7\"><code>315505a</code></a> chore(deps): update peter-evans/create-pull-request action to v7.0.8 (<a href=\"https://redirect.github.com/tj-actions/changed-files/issues/2455\">#2455</a>)</li>\n<li><a href=\"https://github.com/tj-actions/changed-files/commit/c8e1cdb9ea135ee549963c167ffaec5e7d4a71cd\"><code>c8e1cdb</code></a> chore(deps): update dependency <code>@\u200btypes/node</code> to v22.13.9 (<a href=\"https://redirect.github.com/tj-actions/changed-files/issues/2454\">#2454</a>)</li>\n<li><a href=\"https://github.com/tj-actions/changed-files/commit/bb6d1aa0029f6d912eccbd2daf94dbccf5c008b4\"><code>bb6d1aa</code></a> chore(deps): update dependency prettier to v3.5.3 (<a href=\"https://redirect.github.com/tj-actions/changed-files/issues/2453\">#2453</a>)</li>\n<li><a href=\"https://github.com/tj-actions/changed-files/commit/1f74fc96532a3560189153049046a9ac7f436f80\"><code>1f74fc9</code></a> chore(deps): lock file maintenance (<a href=\"https://redirect.github.com/tj-actions/changed-files/issues/2451\">#2451</a>)</li>\n<li><a href=\"https://github.com/tj-actions/changed-files/commit/6f0fde1f0c49ab5a61517920346ab54b09fa8fd5\"><code>6f0fde1</code></a> chore(deps): update dependency <code>@\u200btypes/node</code> to v22.13.8 (<a href=\"https://redirect.github.com/tj-actions/changed-files/issues/2450\">#2450</a>)</li>\n<li>Additional commits viewable in <a href=\"https://github.com/tj-actions/changed-files/compare/v41.0.0...v45.0.8\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=tj-actions/changed-files&package-manager=github_actions&previous-version=41.0.0&new-version=45.0.8)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/meta-llama/llama-cookbook/network/alerts).\n\n</details>", "914": "Make docs link version agnostic on README# What does this PR do?\nReplaces version-specific url with the overall docs website", "733": "Implement test loss# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "848": "update branch", "684": "Improve discoverability of 3.2 recipes# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "34": "fixing scaler for both fsdp and non fsdpimprove the scaler setting for both FSDP and non-FSDP workflows.", "20": "fix typos and spelling errorsFixing some minor typos and spelling errors which should not affect functionality but improve the overall quality of documentation.", "860": "Fixed notebookllama path readme# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [x ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/meta-llama/llama-cookbook/blob/main/CONTRIBUTING.md),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "653": "fix that help reaching 50% over binary classification of toxic chatInitial fix for # https://github.com/meta-llama/llama-recipes/issues/633\n\n", "121": "fix a bug in the config for use_fast_kernels# What does this PR do?\n\nThere was a trailing comma in the existing config that was causing the bool to get picked up as a tuple:\n\n```\nIn [3]: training.train_config\nOut[3]: configs.training.train_config\n\nIn [4]: training.train_config.use_fast_kernels\nOut[4]: (False,)\n```\n\nThis tuple was interpreted as non-False and so the fine-tuning script was runninng with BetterTransformers even when not enabled.\n\n```\nIn [5]: if training.train_config.use_fast_kernels:\n   ...:     print(\"blah\")\n   ...:\nblah\n```\n\nin a tuning run this was leading to unexpected errors:\n```\n  File \"/workspace/llama-recipes/llama_finetuning.py\", line 109, in main\n    model = BetterTransformer.transform(model) \n  File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\n    return func(*args, **kwds)\n  File \"/opt/conda/lib/python3.10/site-packages/optimum/bettertransformer/transformation.py\", line 211, in transform\n    raise NotImplementedError(\nNotImplementedError: The model type llama is not yet supported to be used with BetterTransformer. Feel free to open an issue at https://github.com/huggingface/optimum/issues if you would like this model type to be supported. Currently supported models are: dict_keys(['albert', 'bart', 'bert', 'bert-generation', 'blenderbot', 'camembert', 'clip', 'codegen', 'data2vec-text', 'deit', 'distilbert', 'electra', 'ernie', 'fsmt', 'gpt2', 'gptj', 'gpt_neo', 'gpt_neox', 'hubert', 'layoutlm', 'm2m_100', 'marian', 'markuplm', 'mbart', 'opt', 'pegasus', 'rembert', 'prophetnet', 'roberta', 'roc_bert', 'roformer', 'splinter', 'tapas', 't5', 'vilt', 'vit', 'vit_mae', 'vit_msn', 'wav2vec2', 'whisper', 'xlm-roberta', 'yolos']).\nTraceback (most recent call last):\n```\n\nafter this change:\n```\n>>> training.train_config.use_fast_kernels\nFalse\n>>>\n```\n\n\n\n## Before submitting\n- [X] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [X] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "492": "changed readme.md and parameters.json to support llama3 vllm benchmark# What does this PR do?\nTo address [issue 490](https://github.com/meta-llama/llama-recipes/issues/490), this PR changed readme.md and parameters.json to support llama3 on prem vllm benchmark.\n \n\n## Feature/Issue validation/testing\n\n- [x] benchmark test with 70B instruct\n```\n~/work/llama-recipes/recipes/benchmarks/inference_throughput/on-prem/vllm (fix/llama3_vllm_benchmark)]$ python chat_vllm_benchmark.py\nNumber of token for input prompt: 1000\n| Number of Concurrent Requests | P50 Latency (ms) | P99 Latency (ms) | RPS | Output Tokens per Second | Output Tokens per Second per GPU | Input Tokens per Second | Input Tokens per Second per GPU |Average Output Tokens per Second per Request | Number of Requests Below Threshold |\n|-------------------------------|------------------|------------------|------------------|-------------------|---------------------------|---------------------|------------------------|-------------------------------------- | ---------------------------------- |\n| 1 | 6695.15 | 6695.15 | 0.15 | 50.17 | 6.27 | 149.33 | 18.67 | 50.19 | 0.00 |\n| 2 | 6768.25 | 6768.36 | 0.30 | 94.22 | 11.78 | 295.35 | 36.92 | 47.13 | 0.00 |\n| 4 | 7162.58 | 7163.63 | 0.56 | 184.36 | 23.05 | 558.25 | 69.78 | 46.11 | 0.00 |\n| 8 | 7882.20 | 7886.27 | 1.01 | 327.32 | 40.91 | 1014.16 | 126.77 | 40.95 | 0.00 |\n| 16 | 8446.07 | 8452.66 | 1.89 | 615.40 | 76.93 | 1891.72 | 236.47 | 38.52 | 0.00 |\n| 32 | 11441.68 | 11459.27 | 2.79 | 904.25 | 113.03 | 2791.17 | 348.90 | 28.34 | 0.00 |\n| 64 | 16831.35 | 16875.13 | 3.79 | 1228.25 | 153.53 | 3789.42 | 473.68 | 19.26 | 0.00 |\n| 128 | 27262.07 | 27384.59 | 4.67 | 1515.69 | 189.46 | 4670.50 | 583.81 | 11.90 | 0.00 |\n| 256 | 49327.59 | 49503.46 | 5.17 | 1681.48 | 210.19 | 5167.27 | 645.91 | 6.60 | 255.00 |\n````\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [x] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [x] Did you make sure to update the documentation with your changes?  \n- [x] Did you write any new necessary tests?\n\n\n", "486": "FMBench readme updates for Llama3 on Inf2 and config.yml cleanup# What does this PR do?\nFMBench readme updates for Llama3 on Inf2 and config.yml cleanup\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "451": "Create Karimoz# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [x] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [x] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [x] Did you make sure to update the documentation with your changes?  \n- [x] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "337": "Llama guard data formatter example# What does this PR do?\n\nAdds a simple example on how to use the data formatter script for fine tuning Llama Guard. Adds a readme explaining the steps in the script as well.\n\n## Testing\n\nRunning the script and checking that it generated valid prompts:\n\n`python src/llama_recipes/data/llama_guard/finetuning_data_formatter_example.py`\n\nOutput is as show in the file:\n[sample_formatted_data.json](https://github.com/facebookresearch/llama-recipes/files/13745735/sample_formatted_data.json)\n\n\n\n## Before submitting\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [x] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "323": "Purple llama Anyscale# What does this PR do?\n\nShowing how to use Anyscale hosted Llama Guard model to classify user inputs.\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "479": "Updated fine-tuning readme to Meta Llama 3# What does this PR do?\nUpdated fine-tuning readme to Meta Llama 3, replaced \"Llama 2\" with \"Meta Llama 3\", \"7B\" to \"8B\" in fine-tuning related docs.\n\n", "478": "Changing the text in the azure notebook from Llama 2 to Llama.Updating notebook to reflect Llama 2 or 3 usage by removing the llama version\n\n\n## Feature/Issue validation/testing\n\nNot tested on Azure yet. Based on how Azure works, it should work out of the box, but langchain in general requires some updates.\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [x] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "322": "Adding script to execute Llama Guard standalone# What does this PR do?\n\nAdds a script that uses the new prompt formatting functions to create multi turn conversations. This will allow the model to be loaded into a single GPU instead of requiring 2 or more, as it does right now.\n\n## Testing\n\nRun the script: `RANK=0 WORLD_SIZE=1 MASTER_ADDR=127.0.0.1 MASTER_PORT=29500 python examples/llama_guard/standalone_inference.py  --ckpt_dir ~/models/guard-llama/  --tokenizer_path ~/models/guard-llama/tokenizer.model  --max_seq_len 1536 --max_batch_size 6 `\n\nAnd get this output:\n![image](https://github.com/facebookresearch/llama-recipes/assets/222731/901c730d-3792-49a2-8929-864ac2cb8477)\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "336": "Tanner sorensen patch 1# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "487": "Fix param_init_fn: move if-statement out of lambda# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nWithout parentheses if-statement becomes a part of lambda which is incorrect.\n\n\n\ncc @lchu-ibm @KeitaW ", "493": "Modify langgraph agent to use same tools as tool-use-agent", "861": "Fix package namingRename of library and imports to llama_cookbook", "875": "added readme to text2sql end to end usecase# What does this PR do?\nThis pull request adds a README file to the Text2SQL end-to-end usecase, providing documentation and instructions for users to understand and utilize the usecase.\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/meta-llama/llama-cookbook/blob/main/CONTRIBUTING.md),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "685": "Requirements.txt version bumpsimple version update", "691": "Improve model checkpoint saving logicImprove model checkpoint saving logic to always save model checkpoint when validation is not run\n\n# What does this PR do?\nThis PR enhances the model checkpoint saving logic to always save the model checkpoint when validation is not run. Previously, the model was only saved if validation was performed and the validation loss improved. With this change, we ensure that a checkpoint is saved after every epoch if validation is turned off (``--run_validation False``).\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "726": "Fix/unit test 3.2# What does this PR do?\nThis PR fixes all units tests and lets them run on a fake tokenizer if we do not have access to the meta-llama one.\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "915": "Add Android Article Summarizer demo appAdd Android demo on Article Summarizer, more details in the readme.\n![screenshot](https://github.com/user-attachments/assets/47ae2a2e-b748-4442-badb-83d22a440def)\n", "929": "Update README.md# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/meta-llama/llama-cookbook/blob/main/CONTRIBUTING.md),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "524": "Updated model names for OctoAI# What does this PR do?\nUpdated the name of the models offered by OctoAI. \n\n## Feature/Issue validation/testing\nValidated by getting the list of models from OctoAI site. \n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "281": "adding HSDP as sharding strategy for FSDP training composable with PEFT# What does this PR do?\n\nAdds HSDP, that is beneficial dealing with slower networks, it can does FSDP within the sharding_group and ddp between replica_group.\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\n[Logs for with/out hsdp](https://gist.github.com/HamidShojanazeri/97b512e6c8805147b1e936864a4e8bde) \n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [x ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "283": "Use bf16 parameters in bf16 mixed prec# What does this PR do?\n\n`bfSixteen_mixed` is a poor default choice for mixed precision training, because it does *not* use tensor cores. Instead, it does *all* computation in fp32! I've tested, and on an A6000 it's 2.5x slower to train a 34B model.\n\n## Feature/Issue validation/testing\n\nI tried running the fine tuning script with this change with both 7B and 34B models and it ran 2.5x faster each time.\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "532": "Fix config file links for FMBench, update business summary chart.# What does this PR do?\nFix config file links for FMBench, update business summary chart.\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "254": "Adding context to the demo notebooksThis PR is adding context to the demo notebooks created by Jeff.\nNo code was changed in the notebooks but rather markdown descriptions of the different cells in the notebooks have been added to give some context and explanation to what is happening in the examples.\n\nIn some cases, the output of the cells have been cleared to give it a cleaner look. \n\n\n## Before submitting\n- [ x ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "526": "Move tests and scripts into subfolders# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "917": "Update README.md", "718": "Added a Gradio UI for multi-modal inferencing using Llama 3.2 Vision/What does this PR do?\n<!-- Provide a clear title and description for your PR, explaining the change. -->\nThis PR introduces multi-modal inference using the Gradio UI for Llama 3.2 vision models. The Gradio UI allows users to upload images and generate descriptive text based on a prompt, with adjustable parameters such as top-k, max-tokens, temperature and top-p for fine-tuning text generation. With chatbox like interface. \n\nAdditionally, this PR:\n\nIntegrates the transformers and accelerate libraries for efficient model loading and inference.\nImplements memory management for releasing GPU resources after inference.\nAdds support for Hugging Face tokens to authenticate and access Llama models.\n\n\n\n", "687": "post1 release version bump# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "877": "fix link path# What does this PR do?\n\nFixes link path in notebook\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/meta-llama/llama-cookbook/blob/main/CONTRIBUTING.md),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "122": "Add FSDP CPU offloading option# What does this PR do?\nAdd FSDP CPU offloading option so the model can be trained on system with less GPU vram especially for training 70B models, which does not fit in 7xA100 GPUs without PEFT.\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\n\n\n## Feature/Issue validation/testing\nUse llama-2-70b-hf model without PEFT\n```\ntorchrun --nnodes 1 --nproc_per_node 7  llama_finetuning.py --enable_fsdp --model_name /tmp/llama-2-70b-hf --dist_checkpoint_root_folder model_checkpoints --dist_checkpoint_folder fine-tuned \n```\n\nTested on without `--fsdp_cpu_offload`, it runs OOM on 7x A100 GPUs and crashes\nWith `--fsdp_cpu_offload` option, the training job can run fine without crashing\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "888": "Update tool calling nbs to 3.3Fix 101 and 201", "650": "Fix checkpoint saving # What does this PR do?\nThis PR \n* removes double saving of checkpoints \n* fixes a situation where a users select to fine tune a model without peft and fsdp.\n\nFixes # (issue)\n#646 \n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [X] Test A\n`CUDA_VISIBLE_DEVICES=0,1,4,5 torchrun --nnodes 1 --nproc_per_node 4  recipes/quickstart/finetuning/finetuning.py --enable_fsdp --model_name meta-llama/Meta-Llama-3.1-8B --use_peft --peft_method lora --output_dir ../llama_output/ --run_validation --save_model --samsum_dataset.trust_remote_code=True --context_length 2048 --max_train_step 1 --max_eval_step 1 cd recipes/quickstart/inference/local_inference cat samsum_prompt.txt | python inference.py --model_name meta-llama/Meta-Llama-3.1-70B-Instruct --peft_model ~/llama_output/`\nLogs for Test A\n```\nTraining Epoch: 1:   0%|                                                                                                                                                                                                                                                                                                                           | 0/79 [00:00<?, ?it/s]\nNCCL version 2.20.5+cuda12.4\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\nTraining Epoch: 1/3, step 0/79 completed (loss: 1.3899767398834229):   1%|\u2588\u2588\u2588\u258e                                                                                                                                                                                                                                                             | 1/79 [00:06<09:03,  6.97s/it]\nmax training steps reached, stopping training, total train steps finished:  1\nTraining Epoch: 1/3, step 0/79 completed (loss: 1.3899767398834229):   1%|\u2588\u2588\u2588\u258e                                                                                                                                                                                                                                                             | 1/79 [00:07<09:10,  7.06s/it]\nTraining Epoch: 1/3, step 0/79 completed (loss: 1.5577670335769653):   1%|\u2588\u2588\u2588\u258e                                                                                                                                                                                                                                                             | 1/79 [00:08<11:21,  8.74s/it]\nTraining Epoch: 1/3, step 0/79 completed (loss: 1.4859018325805664):   1%|\u2588\u2588\u2588\u258e                                                                                                                                                                                                                                                             | 1/79 [00:09<11:58,  9.21s/it]\nTraining Epoch: 1/3, step 0/79 completed (loss: 1.4957325458526611):   1%|\u2588\u2588\u2588\u258e                                                                                                                                                                                                                                                             | 1/79 [00:08<10:53,  8.38s/it]\nMax CUDA memory allocated was 19 GB\nMax CUDA memory reserved was 23 GB\nPeak active CUDA memory was 19 GB\nCUDA Malloc retries : 0\nCPU Total Peak Memory consumed during the train (max): 7 GB\nevaluating Epoch:   6%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                                                                                                                                                                                                                                  | 1/17 [00:00<00:08,  1.97it/s]\nmax eval steps reached, stopping evaluation, total_eval_steps:  1\nevaluating Epoch:   6%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                                                                                                                                                                                                                                  | 1/17 [00:00<00:10,  1.55it/s]\nevaluating Epoch:   6%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                                                                                                                                                                                                                                  | 1/17 [00:00<00:10,  1.49it/s]\nevaluating Epoch:   6%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                                                                                                                                                                                                                                  | 1/17 [00:00<00:10,  1.49it/s]\nevaluating Epoch:   6%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                                                                                                                                                                                                                                  | 1/17 [00:00<00:11,  1.39it/s]\n eval_ppl=tensor(1.0902, device='cuda:0') eval_epoch_loss=tensor(0.0864, device='cuda:0')\nwe are about to save the PEFT modules\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n  warnings.warn(\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n  warnings.warn(\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n  warnings.warn(\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n  warnings.warn(\nPEFT modules are saved in ../llama_output/ directory\nbest eval loss on epoch 1 is 0.08636830747127533\nEpoch 1: train_perplexity=1.0189, train_epoch_loss=0.0188, epoch time 7.81007081293501s\nKey: avg_train_prep, Value: 1.018941044807434\nKey: avg_train_loss, Value: 0.01876385696232319\nKey: avg_eval_prep, Value: 1.0902076959609985\nKey: avg_eval_loss, Value: 0.08636830747127533\nKey: avg_epoch_time, Value: 7.81007081293501\nKey: avg_checkpoint_time, Value: 22.509945076191798\n```\n\n- [X] Test B\n`CUDA_VISIBLE_DEVICES=2,3,6,7 torchrun --nnodes 1 --nproc_per_node 4  recipes/quickstart/finetuning/finetuning.py --enable_fsdp --model_name meta-llama/Meta-Llama-3.1-8B  --output_dir ../llama_output/ --run_validation --save_model --samsum_dataset.trust_remote_code=True --context_length 2048 --max_train_step 1 --max_eval_step 1 --fsdp_config.checkpoint_type StateDictType.FULL_STATE_DICT --dist_checkpoint_root_folder ../llama_output_fsdp/ `\nLogs for Test B\n```\nW0828 12:06:25.882000 139684707845120 torch/distributed/run.py:779]\nW0828 12:06:25.882000 139684707845120 torch/distributed/run.py:779] *****************************************\nW0828 12:06:25.882000 139684707845120 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.\nW0828 12:06:25.882000 139684707845120 torch/distributed/run.py:779] *****************************************\n/home/mreso/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n  from torch.distributed._shard.checkpoint import (\n/home/mreso/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead                                                                                                                                                          from torch.distributed._shard.checkpoint import (\n/home/mreso/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n  from torch.distributed._shard.checkpoint import (\n/home/mreso/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n  from torch.distributed._shard.checkpoint import (\nClearing GPU cache for all ranks\n--> Running with torch dist debug set to detail\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  8.11it/s]\n--> Model meta-llama/Meta-Llama-3.1-8B\n\n--> meta-llama/Meta-Llama-3.1-8B has 8030.261248 Million params\n\nbFloat16 enabled for mixed precision - using bfSixteen policy\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  4.27it/s]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  4.16it/s]Loading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  4.11it/s]\n--> applying fsdp activation checkpointing...\n--> applying fsdp activation checkpointing...                                                                                                                                                                                                                                                                                                                             --> applying fsdp activation checkpointing...\n--> applying fsdp activation checkpointing...\n--> Training Set Length = 14732                                                                                                                                                                                                                                                                                                                                           Preprocessing dataset:   0%|                                                                                                                                                                                                                                                                                                                    | 0/14732 [00:00<?, ?it/s]\n--> Validation Set Length = 818\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:04<00:00, 3482.75it/s]Preprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 818/818 [00:00<00:00, 3475.14it/s]\n--> Num of Validation Set Batches loaded = 17\nPreprocessing dataset:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f      | 14390/14732 [00:04<00:00, 3374.47it/s]\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:04<00:00, 3458.10it/s]\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:04<00:00, 3460.62it/s]\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:04<00:00, 3364.39it/s]\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 818/818 [00:00<00:00, 3252.97it/s]\n--> Num of Validation Set Batches loaded = 17\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 818/818 [00:00<00:00, 3346.98it/s]\n--> Num of Validation Set Batches loaded = 17\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 818/818 [00:00<00:00, 3292.48it/s]\n--> Num of Validation Set Batches loaded = 17\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                                                                                                                                                                                                                                                                                           | 0/79 [00:00<?, ?it/s]\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                                                                                                                                                                                                                                                                                           | 0/79 [00:00<?, ?it/s]\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                                                                                                                                                                                                                                                                                           | 0/79 [00:00<?, ?it/s]NCCL version 2.20.5+cuda12.4\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\nTraining Epoch: 1/3, step 0/79 completed (loss: 1.5577670335769653):   1%|\u2588\u2588\u2588\u258e                                                                                                                                                                                                                                                             | 1/79 [00:07<09:46,  7.52s/it]max training steps reached, stopping training, total train steps finished:  1\nTraining Epoch: 1/3, step 0/79 completed (loss: 1.3899767398834229):   1%|\u2588\u2588\u2588\u258e                                                                                                                                                                                                                                                             | 1/79 [00:07<09:56,  7.65s/it]\nTraining Epoch: 1/3, step 0/79 completed (loss: 1.4957325458526611):   1%|\u2588\u2588\u2588\u258e                                                                                                                                                                                                                                                             | 1/79 [00:07<09:49,  7.55s/it]\nTraining Epoch: 1/3, step 0/79 completed (loss: 1.5577670335769653):   1%|\u2588\u2588\u2588\u258e                                                                                                                                                                                                                                                             | 1/79 [00:07<09:59,  7.69s/it]Training Epoch: 1/3, step 0/79 completed (loss: 1.4859018325805664):   1%|\u2588\u2588\u2588\u258e                                                                                                                                                                                                                                                             | 1/79 [00:08<10:47,  8.30s/it]\nMax CUDA memory allocated was 19 GB\nMax CUDA memory reserved was 29 GB\nPeak active CUDA memory was 20 GB\nCUDA Malloc retries : 0\nCPU Total Peak Memory consumed during the train (max): 7 GB\nevaluating Epoch:   6%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                                                                                                                                                                                                                                  | 1/17 [00:00<00:09,  1.70it/s]max eval steps reached, stopping evaluation, total_eval_steps:  1\nevaluating Epoch:   6%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                                                                                                                                                                                                                                  | 1/17 [00:00<00:11,  1.45it/s]\nevaluating Epoch:   6%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                                                                                                                                                                                                                                  | 1/17 [00:00<00:11,  1.38it/s]\nevaluating Epoch:   6%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                                                                                                                                                                                                                                  | 1/17 [00:00<00:11,  1.44it/s]\nevaluating Epoch:   6%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                                                                                                                                                                                                                                  | 1/17 [00:00<00:11,  1.38it/s]\n eval_ppl=tensor(2.3370, device='cuda:0') eval_epoch_loss=tensor(0.8489, device='cuda:0')\n Saving the FSDP model checkpoint using FULL_STATE_DICT Saving the FSDP model checkpoint using FULL_STATE_DICT Saving the FSDP model checkpoint using FULL_STATE_DICT\n\n==========================================================================================================\n\n\n===================================================== Saving the FSDP model checkpoint using FULL_STATE_DICT\n\n=====================================================\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n  warnings.warn(\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n  warnings.warn(\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n  warnings.warn(\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n  warnings.warn(\nsaving process: rank 2  done w model state_dict\n\nsaving process: rank 1  done w model state_dict\n\nsaving process: rank 3  done w model state_dict\n\nsaving process: rank 0  done w model state_dict\n\n--> saving model ...\nmodel checkpoint saved for epoch 0 at /home/mreso/llama-recipes/../llama_output_fsdp/fine-tuned-meta-llama/Meta-Llama-3.1-8B/meta-llama--Meta-Llama-3.1-8B-0.pt\n\nbest eval loss on epoch 1 is 0.8488507866859436\nEpoch 1: train_perplexity=1.0189, train_epoch_loss=0.0188, epoch time 8.528414465952665s\ntraining params are saved in /home/mreso/llama-recipes/../llama_output_fsdp/fine-tuned-meta-llama/Meta-Llama-3.1-8B/train_params.yaml\nKey: avg_train_prep, Value: 1.018941044807434\nKey: avg_train_loss, Value: 0.01876385696232319\nKey: avg_eval_prep, Value: 2.3369596004486084\nKey: avg_eval_loss, Value: 0.8488507866859436\nKey: avg_epoch_time, Value: 8.528414465952665\nKey: avg_checkpoint_time, Value: 40.176256065955386\n```\n\n- [X] Test C\n` CUDA_VISIBLE_DEVICES=2,3,6,7 torchrun --nnodes 1 --nproc_per_node 4  recipes/quickstart/finetuning/finetuning.py --enable_fsdp --model_name meta-llama/Meta-Llama-3.1-8B  --output_dir ../llama_output/ --run_validation --save_model --samsum_dataset.trust_remote_code=True --context_length 2048 --max_train_step 1 --max_eval_step 1 --fsdp_config.checkpoint_type StateDictType.SHARDED_STATE_DICT --dist_checkpoint_root_folder ../llama_output_fsdp/`\nLogs for Test C\n``\nsamsum_dataset.trust_remote_code=True --context_length 2048 --max_train_step 1 --max_eval_step 1 --fsdp_config.checkpoint_type StateDictType.SHARDED_STATE_DICT --dist_checkpoint_root_folder ../llama_output_fsdp/\nW0828 12:27:59.588000 140014184924160 torch/distributed/run.py:779]\nW0828 12:27:59.588000 140014184924160 torch/distributed/run.py:779] *****************************************\nW0828 12:27:59.588000 140014184924160 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.\nW0828 12:27:59.588000 140014184924160 torch/distributed/run.py:779] *****************************************\n/home/mreso/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n  from torch.distributed._shard.checkpoint import (\n/home/mreso/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n  from torch.distributed._shard.checkpoint import (\n/home/mreso/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n  from torch.distributed._shard.checkpoint import (\n/home/mreso/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n  from torch.distributed._shard.checkpoint import (\nClearing GPU cache for all ranks\n--> Running with torch dist debug set to detail\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  4.23it/s]\n--> Model meta-llama/Meta-Llama-3.1-8B\n\n--> meta-llama/Meta-Llama-3.1-8B has 8030.261248 Million params\n\nbFloat16 enabled for mixed precision - using bfSixteen policy\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  4.11it/s]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  4.13it/s]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  4.03it/s]\n--> applying fsdp activation checkpointing...\n--> applying fsdp activation checkpointing...\n--> applying fsdp activation checkpointing...\n--> applying fsdp activation checkpointing...\nPreprocessing dataset:  19%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                                                                                                                                                                                                              | 2846/14732 [00:00<00:03, 3524.81it/s]\n--> Training Set Length = 14732\nPreprocessing dataset:  50%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b                                                                                                                                                   | 7373/14732 [00:02<00:02, 3356.12it/s]\n--> Validation Set Length = 818\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:04<00:00, 3282.43it/s]\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 818/818 [00:00<00:00, 3324.44it/s]\n--> Num of Validation Set Batches loaded = 17\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:04<00:00, 3415.49it/s]\nPreprocessing dataset:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a          | 14219/14732 [00:04<00:00, 3262.10it/s]\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 818/818 [00:00<00:00, 3461.39it/s]\n--> Num of Validation Set Batches loaded = 17\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:04<00:00, 3162.09it/s]\nPreprocessing dataset:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                                                                                               | 9177/14732 [00:02<00:01, 3413.66it/s]\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 818/818 [00:00<00:00, 3081.66it/s]\n--> Num of Validation Set Batches loaded = 17\nPreprocessing dataset:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                                                                          | 10213/14732 [00:02<00:01, 3396.14it/s]\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:04<00:00, 3427.46it/s]\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 818/818 [00:00<00:00, 3395.88it/s]\n--> Num of Validation Set Batches loaded = 17\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                                                                                                                                                                                                                                                                                           | 0/79 [00:00<?, ?it/s]\nNCCL version 2.20.5+cuda12.4\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\nTraining Epoch: 1/3, step 0/79 completed (loss: 1.3899767398834229):   1%|\u2588\u2588\u2588\u258e                                                                                                                                                                                                                                                             | 1/79 [00:07<09:17,  7.15s/it]\nmax training steps reached, stopping training, total train steps finished:  1\nTraining Epoch: 1/3, step 0/79 completed (loss: 1.4957325458526611):   1%|\u2588\u2588\u2588\u258e                                                                                                                                                                                                                                                             | 1/79 [00:09<12:53,  9.92s/it]\nTraining Epoch: 1/3, step 0/79 completed (loss: 1.4859018325805664):   1%|\u2588\u2588\u2588\u258e                                                                                                                                                                                                                                                             | 1/79 [00:09<12:00,  9.23s/it]\nTraining Epoch: 1/3, step 0/79 completed (loss: 1.3899767398834229):   1%|\u2588\u2588\u2588\u258e                                                                                                                                                                                                                                                             | 1/79 [00:07<09:36,  7.39s/it]\nTraining Epoch: 1/3, step 0/79 completed (loss: 1.5577670335769653):   1%|\u2588\u2588\u2588\u258e                                                                                                                                                                                                                                                             | 1/79 [00:09<12:27,  9.59s/it]\nMax CUDA memory allocated was 19 GB\nMax CUDA memory reserved was 29 GB\nPeak active CUDA memory was 23 GB\nCUDA Malloc retries : 0\nCPU Total Peak Memory consumed during the train (max): 7 GB\nevaluating Epoch:   6%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                                                                                                                                                                                                                                  | 1/17 [00:00<00:06,  2.37it/s]\nmax eval steps reached, stopping evaluation, total_eval_steps:  1\nevaluating Epoch:   6%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                                                                                                                                                                                                                                  | 1/17 [00:00<00:09,  1.69it/s]\nevaluating Epoch:   6%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                                                                                                                                                                                                                                  | 1/17 [00:00<00:08,  1.80it/s]\nevaluating Epoch:   6%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                                                                                                                                                                                                                                  | 1/17 [00:00<00:08,  1.79it/s]\nevaluating Epoch:   6%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                                                                                                                                                                                                                                  | 1/17 [00:00<00:08,  1.80it/s]\n eval_ppl=tensor(2.3358, device='cuda:0') eval_epoch_loss=tensor(0.8483, device='cuda:0')\n Saving the FSDP model checkpoints and optimizer using SHARDED_STATE_DICT Saving the FSDP model checkpoints and optimizer using SHARDED_STATE_DICT Saving the FSDP model checkpoints and optimizer using SHARDED_STATE_DICT Saving the FSDP model checkpoints and optimizer using SHARDED_STATE_DICT\n\n\n\n====================================================================================================================================================================================================================\n\n\n\nSaving model to /home/mreso/llama-recipes/../llama_output_fsdp/fine-tuned-meta-llama/Meta-Llama-3.1-8B\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n  warnings.warn(\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n  warnings.warn(\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n  warnings.warn(\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n  warnings.warn(\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:737: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n  local_shape = tensor.shape\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:737: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n  local_shape = tensor.shape\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:737: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n  local_shape = tensor.shape\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:737: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n  local_shape = tensor.shape\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:749: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n  tensor.shape,\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:749: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n  tensor.shape,\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:749: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n  tensor.shape,\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:749: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n  tensor.shape,\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:751: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n  tensor.dtype,\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:751: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n  tensor.dtype,\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:751: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n  tensor.dtype,\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:751: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n  tensor.dtype,\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:752: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n  tensor.device,\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:752: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n  tensor.device,\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:752: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n  tensor.device,\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:752: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n  tensor.device,\n/home/mreso/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:113: FutureWarning: `save_state_dict` is deprecated and will be removed in future versions.Please use `save` instead.\n  dist_cp.save_state_dict(\n/home/mreso/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:113: FutureWarning: `save_state_dict` is deprecated and will be removed in future versions.Please use `save` instead.\n  dist_cp.save_state_dict(\n/home/mreso/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:113: FutureWarning: `save_state_dict` is deprecated and will be removed in future versions.Please use `save` instead.\n  dist_cp.save_state_dict(\n/home/mreso/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:113: FutureWarning: `save_state_dict` is deprecated and will be removed in future versions.Please use `save` instead.\n  dist_cp.save_state_dict(\nSharded state checkpoint saved to /home/mreso/llama-recipes/../llama_output_fsdp/fine-tuned-meta-llama/Meta-Llama-3.1-8B\nCheckpoint Time = 14.0877\n\nbest eval loss on epoch 1 is 0.8483395576477051\nEpoch 1: train_perplexity=1.0189, train_epoch_loss=0.0188, epoch time 7.993172182934359s\ntraining params are saved in /home/mreso/llama-recipes/../llama_output_fsdp/fine-tuned-meta-llama/Meta-Llama-3.1-8B/train_params.yaml\nKey: avg_train_prep, Value: 1.018941044807434\nKey: avg_train_loss, Value: 0.01876385696232319\nKey: avg_eval_prep, Value: 2.3357651233673096\nKey: avg_eval_loss, Value: 0.8483395576477051\nKey: avg_epoch_time, Value: 7.993172182934359\nKey: avg_checkpoint_time, Value: 14.090817171148956\n``\n\n- [X] Test D\npython  recipes/quickstart/finetuning/finetuning.py --model_name meta-llama/Meta-Llama-3.1-8B  --output_dir ../llama_output/ --run_validation --save_model --samsum_dataset.trust_remote_code=True --context_length 2048 --max_train_step 1 --max_eval_step 1  --quantization 8bit`\nLogs for Test D\n```\n/home/mreso/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n  from torch.distributed._shard.checkpoint import (\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:08<00:00,  2.19s/it]\n--> Model meta-llama/Meta-Llama-3.1-8B\n\n--> meta-llama/Meta-Llama-3.1-8B has 1050.939392 Million params\n\n--> Training Set Length = 14732\n--> Validation Set Length = 818\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:04<00:00, 3202.90it/s]\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 818/818 [00:00<00:00, 2500.57it/s]\n--> Num of Validation Set Batches loaded = 69\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                                                                                                                                                                                                                                                                                          | 0/319 [00:00<?, ?it/s]\n/home/mreso/.conda/envs/llama/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\nTraining Epoch: 1/3, step 0/319 completed (loss: 1.5929347276687622):   0%|\u258a                                                                                                                                                                                                                                                              | 1/319 [00:09<47:41,  9.00s/it]\nmax training steps reached, stopping training, total train steps finished:  1\nTraining Epoch: 1/3, step 0/319 completed (loss: 1.5929347276687622):   0%|\u258a                                                                                                                                                                                                                                                              | 1/319 [00:09<48:31,  9.16s/it]\nMax CUDA memory allocated was 38 GB\nMax CUDA memory reserved was 41 GB\nPeak active CUDA memory was 38 GB\nCUDA Malloc retries : 0\nCPU Total Peak Memory consumed during the train (max): 10 GB\nevaluating Epoch:   1%|\u2588\u2588\u2588\u2588\u258d                                                                                                                                                                                                                                                                                                               | 1/69 [00:00<01:06,  1.02it/s]\nmax eval steps reached, stopping evaluation, total_eval_steps:  1\nevaluating Epoch:   1%|\u2588\u2588\u2588\u2588\u258d                                                                                                                                                                                                                                                                                                               | 1/69 [00:01<01:15,  1.11s/it]\n eval_ppl=tensor(1.0213, device='cuda:0') eval_epoch_loss=tensor(0.0211, device='cuda:0')\nbest eval loss on epoch 1 is 0.02108839526772499\nEpoch 1: train_perplexity=1.0050, train_epoch_loss=0.0050, epoch time 11.862033671932295s\nKey: avg_train_prep, Value: 1.0050060749053955\nKey: avg_train_loss, Value: 0.0049935257993638515\nKey: avg_eval_prep, Value: 1.0213123559951782\nKey: avg_eval_loss, Value: 0.02108839526772499\nKey: avg_epoch_time, Value: 11.862033671932295\nKey: avg_checkpoint_time, Value: 18.797315332805738\n```\n\n\n\n## Before submitting\n- [X] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [X] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [X] Did you make sure to update the documentation with your changes?  \n- [X] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "136": "Update inference.mdAdding Llama 2 prompt information to inference doc\n\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "678": "Create v0.0.4 release# What does this PR do?\n\nBumps version number \n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "485": "Add LangChain recipes# What does this PR do?\n\nAdd recipes for LangChain agents.\n\n## Feature/Issue validation/testing\n\nNotebooks tested in fresh conda env. ", "446": "Update xpu related device setting# What does this PR do?\n\nThis PR update some xpu related logic for correct support.\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "334": "Fix some small nits in README.mdFix some small nits in README.md", "452": "FMBench: benchmarking Llama models on AWS# What does this PR do?\n\nAdds a recipe (`FMBench`) for benchmarking Llama models (including Llama3) on AWS platforms (SageMaker, Bedrock). `FMBench` is an open-source Python package for benchmarking foundation models for price|performance. See [`FMBench`](https://github.com/aws-samples/foundation-model-benchmarking-tool/tree/main).\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ x] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "308": "Update grammar_dataset_process.ipynb # What does this PR do?\nFixes a typo in the ''grammar_dataset_process.ipynb''.\nRemoves the repeated (\" v\\\", _\\\"**n't**_\\\"), and adds  (\\\" v\\\", **_\\\"v\\\"_**),  to be applicable for any 'v' belonging to                   { v: v is a non case-sensitive alphabet }\n\n\n\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n      Pull Request section?  YES\n- [ ] Did you make sure to update the documentation with your changes?  NO\n\n\n", "447": "[Feature]Enable Ascend NPU fintuning and inference# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "321": "Removing unecesary prompts# What does this PR do?\nFixing prompts used for testing\n\n## Feature/Issue validation/testing\n\nNo testing, only string changes in the test promts\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "484": "Update prompt guide for Llama 3- Update content for Llama 3\n- Switch API provider to Groq\n- Use 70B by default as Groq provides fast enough inference\n", "679": "fix readme# What does this PR do?\nfix the naming in the readme\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "651": "Add recipe for Llama Triaging & Reporting Tool# What does this PR do?\nAdds a new recipe showcasing how to use Llama for automating data analytics and reporting tasks on an open-source repo.\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [x] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "889": "Contextual keywords generation for RAG using Llama-3.1# Contextual keywords generation for RAG using Llama-3.1\n\n**Problem**: Independent chunking in traditional RAG systems leads to the loss of contextual information between chunks. This makes it difficult for LLMs to retrieve relevant data when context (e.g., the subject or entity being discussed) is not explicitly repeated within individual chunks.\n\n**Solution**: Generate keywords for each chunk to fulfill missing contextual information. These keywords (e.g., \"BMW, X5, pricing\") enrich the chunk with necessary context, ensuring better retrieval accuracy. By embedding this enriched metadata, the system bridges gaps between related chunks, enabling effective query matching and accurate answer generation.\n\n[This article](https://medium.com/@ailabs/overcoming-independent-chunking-in-rag-systems-a-hybrid-approach-5d2c205b3732) explains benefits of contextual chunking.\n\n**Note** This method does not require calling LLM for each chunk separately, which makes it efficient.", "862": "fix meta_eval after refactor and add new meta_mmlu_instruct task for 3.2# What does this PR do?\n\nThis PR fix meta_eval after refactor by setting the correct path and update MATH dataset URL. Split 3.2 MMLU task into `meta_mmlu_pretrain` and `meta_mmlu_instruct`, tested below:\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [x] add new meta_mmlu_instruct for 3.2\n```\nvllm (pretrained=meta-llama/Llama-3.2-3B-Instruct,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.4,data_parallel_size=1,max_model_len=8192,add_bos_token=True,seed=42), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto\n|        Tasks        |Version|   Filter   |n-shot|  Metric   |   |Value |   |Stderr|\n|---------------------|-------|------------|-----:|-----------|---|-----:|---|-----:|\n|meta_instruct        |    N/A|            |      |           |   |      |   |      |\n| - meta_gpqa         |      1|strict-match|     0|exact_match|\u2191  |0.3326|\u00b1  |0.0223|\n| - meta_math         |      1|none        |     0|exact_match|\u2191  |0.4514|\u00b1  |0.0070|\n| - meta_mmlu_instruct|      1|strict-match|     0|exact_match|\u2191  |0.6368|\u00b1  |0.0041|\n```\n\n- [x] test on 3b meta_mmlu_pretrain\n```\n2025-01-28:14:28:57,156 INFO     [evaluation_tracker.py:287] Saving per-sample results for: meta_mmlu_pretrain\nvllm (pretrained=meta-llama/Llama-3.2-3B,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.4,data_parallel_size=1,max_model_len=8192,add_bos_token=True,seed=42), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto\n|        Tasks        |Version|Filter|n-shot| Metric |   |Value|   |Stderr|\n|---------------------|-------|------|-----:|--------|---|----:|---|-----:|\n|meta_pretrain        |    N/A|      |      |        |   |     |   |      |\n| - meta_mmlu_pretrain|      1|none  |     0|acc     |\u2191  |0.566|\u00b1  |0.0042|\n|                     |       |none  |     0|acc_norm|\u2191  |0.566|\u00b1  |0.0042|\n```\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/meta-llama/llama-cookbook/blob/main/CONTRIBUTING.md),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "686": "fix AutoModel and bump transformers version to 4.45# What does this PR do?\nfix AutoModel bug in finetune.py and bump transformers version to 4.45 to add mllama support. However, now transformer 4.45.0 pip package did not work as stated in [this issue](https://github.com/huggingface/transformers/issues/33706), we need to wait for [this PR](https://github.com/huggingface/transformers/commit/46841d3eb24f444fc06b7402c273cb51a097c383) to be included the next Transformers pip package to get the mllama Processor working again.\n\nUpdate:\ntransformers version 4.45.1 has been released, bumping to this version works\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [x] 8B samsum_dataset FSDP+LORA working\n```torchrun --nnodes 1 --nproc_per_node 6  recipes/quickstart/finetuning/finetuning.py --use_peft --enable_\nfsdp --peft_method lora  --model_name meta-llama/Meta-Llama-3-8B-Instruct --output_dir chatbot-prefix --num_epochs 10 --batch_size_training 1  --run_validation True --flop_counte\nr --flop_counter_start 4 --max_train_step 6 --max_eval_step 2 --samsum_dataset.trust_remote_code=True\nW0926 10:00:56.395000 140478447821824 torch/distributed/run.py:779] \nW0926 10:00:56.395000 140478447821824 torch/distributed/run.py:779] *****************************************\nW0926 10:00:56.395000 140478447821824 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0926 10:00:56.395000 140478447821824 torch/distributed/run.py:779] *****************************************\n/home/kaiwu/work/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n  from torch.distributed._shard.checkpoint import (\n/home/kaiwu/work/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n  from torch.distributed._shard.checkpoint import (\n/home/kaiwu/work/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n  from torch.distributed._shard.checkpoint import (\n/home/kaiwu/work/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n  from torch.distributed._shard.checkpoint import (\n/home/kaiwu/work/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n  from torch.distributed._shard.checkpoint import (\n/home/kaiwu/work/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n  from torch.distributed._shard.checkpoint import (\nClearing GPU cache for all ranks\n--> Running with torch dist debug set to detail\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  5.29it/s]\n--> Model meta-llama/Meta-Llama-3-8B-Instruct\n\n--> meta-llama/Meta-Llama-3-8B-Instruct has 8030.261248 Million params\n\nLoading checkpoint shards:   0%|                                                                                                                            | 0/4 [00:00<?, ?it/s]trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424\nbFloat16 enabled for mixed precision - using bfSixteen policy\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  8.48it/s]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  5.12it/s]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  5.28it/s]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  4.63it/s]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  4.74it/s]\ntrainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424\ntrainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424\ntrainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424\ntrainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424\ntrainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424\n--> applying fsdp activation checkpointing...\n--> applying fsdp activation checkpointing...\n--> applying fsdp activation checkpointing...\n--> applying fsdp activation checkpointing...\n--> applying fsdp activation checkpointing...\n--> applying fsdp activation checkpointing...\n--> Training Set Length = 14732\nPreprocessing dataset:   0%|                                                                                                                            | 0/14732 [00:00<?, ?it/s]--> Validation Set Length = 818\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:02<00:00, 6862.88it/s]\nlength of dataset_train 639\n--> Num of Training Set Batches loaded = 106\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:02<00:00, 6874.49it/s]\nlength of dataset_train 639\n--> Num of Training Set Batches loaded = 106\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:02<00:00, 6683.56it/s]\nlength of dataset_train 639\n--> Num of Training Set Batches loaded = 106\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 818/818 [00:00<00:00, 6961.71it/s]\n--> Num of Validation Set Batches loaded = 5\n--> Num of Validation Set Batches loaded = 5\nStarting epoch 0/10\ntrain_config.max_train_step: 6\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 818/818 [00:00<00:00, 7073.52it/s]\n--> Num of Validation Set Batches loaded = 5\n--> Num of Validation Set Batches loaded = 5\nStarting epoch 0/10\ntrain_config.max_train_step: 6\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 818/818 [00:00<00:00, 6908.40it/s]\n--> Num of Validation Set Batches loaded = 5\n--> Num of Validation Set Batches loaded = 5\nStarting epoch 0/10\ntrain_config.max_train_step: 6\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:02<00:00, 6865.71it/s]\nlength of dataset_train 639\n--> Num of Training Set Batches loaded = 106\nPreprocessing dataset:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d    | 14129/14732 [00:02<00:00, 6755.05it/s]/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                                                                                                  | 0/106 [00:00<?, ?it/s]/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                                                                                                  | 0/106 [00:00<?, ?it/s]/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:02<00:00, 6892.91it/s]\nlength of dataset_train 639\n--> Num of Training Set Batches loaded = 106\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 818/818 [00:00<00:00, 7046.51it/s]\n--> Num of Validation Set Batches loaded = 5\n--> Num of Validation Set Batches loaded = 5\nStarting epoch 0/10\ntrain_config.max_train_step: 6\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 818/818 [00:00<00:00, 7031.75it/s]\n--> Num of Validation Set Batches loaded = 5\n--> Num of Validation Set Batches loaded = 5\nStarting epoch 0/10\ntrain_config.max_train_step: 6\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:02<00:00, 6854.20it/s]\nlength of dataset_train 639\n--> Num of Training Set Batches loaded = 106\nPreprocessing dataset:   0%|                                                                                                                              | 0/818 [00:00<?, ?it/s]/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                                                                                                  | 0/106 [00:00<?, ?it/s]NCCL version 2.20.5+cuda12.4\nPreprocessing dataset:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e               | 705/818 [00:00<00:00, 7043.56it/s]/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 818/818 [00:00<00:00, 7014.57it/s]\n--> Num of Validation Set Batches loaded = 5\n--> Num of Validation Set Batches loaded = 5\nStarting epoch 0/10\ntrain_config.max_train_step: 6\nTraining Epoch: 1:   0%|                                                                                                                                  | 0/106 [00:00<?, ?it/s]/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                                                                                                  | 0/106 [00:00<?, ?it/s]/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\nTraining Epoch: 1/10, step 4/106 completed (loss: 1.6060208082199097):   6%|\u2588\u2588\u2588\u2589                                                                  | 6/106 [00:25<05:19,  3.20s/it]max training steps reached, stopping training, total train steps finished:  6\nTraining Epoch: 1/10, step 5/106 completed (loss: 1.4604358673095703):   6%|\u2588\u2588\u2588\u2589                                                                  | 6/106 [00:25<06:57,  4.17s/it]\nTraining Epoch: 1/10, step 5/106 completed (loss: 1.4065511226654053):   6%|\u2588\u2588\u2588\u2589                                                                  | 6/106 [00:25<07:11,  4.32s/it]\nTraining Epoch: 1/10, step 5/106 completed (loss: 1.3868926763534546):   6%|\u2588\u2588\u2588\u2589                                                                  | 6/106 [00:25<07:06,  4.26s/it]\nTraining Epoch: 1/10, step 5/106 completed (loss: 1.4889317750930786):   6%|\u2588\u2588\u2588\u2589                                                                  | 6/106 [00:25<07:12,  4.32s/it]\nTotal time used in this flop counting step is: 2.6097419261932373\nThe total TFlop per second is: 103.62345261768849\nThe tflop_count table is below:\nModule                                                     FLOP    % Total\n-----------------------------------------------------  --------  ---------\nFullyShardedDataParallel                               270.430T    100.00%\n - aten.bmm                                              0.000T      0.00%\n - aten.mm                                             222.052T     82.11%\n - aten._scaled_dot_product_flash_attention             26.388T      9.76%\n - aten._scaled_dot_product_flash_attention_backward    21.990T      8.13%\n FullyShardedDataParallel._fsdp_wrapped_module         270.430T    100.00%\n  - aten.bmm                                             0.000T      0.00%\n  - aten.mm                                            222.052T     82.11%\n  - aten._scaled_dot_product_flash_attention            26.388T      9.76%\n  - aten._scaled_dot_product_flash_attention_backward   21.990T      8.13%\nTraining Epoch: 1/10, step 5/106 completed (loss: 1.3996834754943848):   6%|\u2588\u2588\u2588\u2589                                                                  | 6/106 [00:25<07:12,  4.33s/it]\nTraining Epoch: 1/10, step 5/106 completed (loss: 1.3902316093444824):   6%|\u2588\u2588\u2588\u2589                                                                  | 6/106 [00:25<07:04,  4.25s/it]\nMax CUDA memory allocated was 84 GB\nMax CUDA memory reserved was 85 GB\nPeak active CUDA memory was 84 GB\nCUDA Malloc retries : 0\nCPU Total Peak Memory consumed during the train (max): 9 GB\nevaluating Epoch:  40%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                           | 2/5 [00:00<00:01,  2.78it/s]max eval steps reached, stopping evaluation, total_eval_steps:  2\nevaluating Epoch:  40%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                           | 2/5 [00:00<00:01,  2.44it/s]\nevaluating Epoch:  40%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                           | 2/5 [00:00<00:01,  2.70it/s]\nevaluating Epoch:  40%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                           | 2/5 [00:00<00:01,  2.82it/s]\nevaluating Epoch:  40%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                           | 2/5 [00:00<00:01,  2.29it/s]\nevaluating Epoch:  40%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                           | 2/5 [00:00<00:01,  2.27it/s]\nevaluating Epoch:  40%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                           | 2/5 [00:00<00:01,  2.34it/s]\n eval_ppl=tensor(1.7234, device='cuda:0') eval_epoch_loss=tensor(0.5443, device='cuda:0')\nwe are about to save the PEFT modules\n/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n  warnings.warn(\n/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n  warnings.warn(\n/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n  warnings.warn(\n/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n  warnings.warn(\n/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n  warnings.warn(\n/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n  warnings.warn(\nPEFT modules are saved in chatbot-prefix directory\nbest eval loss on epoch 1 is 0.5443158149719238\nEpoch 1: train_perplexity=1.0946, train_epoch_loss=0.0904, epoch time 26.56355979386717s\nStarting epoch 1/10\ntrain_config.max_train_step: 6\nStarting epoch 1/10\ntrain_config.max_train_step: 6\nStarting epoch 1/10\ntrain_config.max_train_step: 6\nStarting epoch 1/10\ntrain_config.max_train_step: 6\nStarting epoch 1/10\ntrain_config.max_train_step: 6\nStarting epoch 1/10\ntrain_config.max_train_step: 6\nKey: avg_train_prep, Value: 1.0945931673049927\nKey: avg_train_loss, Value: 0.09038276970386505\nKey: avg_eval_prep, Value: 1.7234288454055786\nKey: avg_eval_loss, Value: 0.5443158149719238\nKey: avg_epoch_time, Value: 26.56355979386717\nKey: avg_checkpoint_time, Value: 12.235354381147772\nKey: model_tflops, Value: 103.62345261768849\n```\n\n- [x] 11B FSDP+LORA works on OCRVQA\n```torchrun --nnodes 1 --nproc_per_node 4  recipes/quickstart/finetuning/finetuning.py --enable_fsdp --lr 2e-5 --context_length 8192 --num_epochs 3 --batch_size_training 2 --model_name meta-llama/Llama-3.2-11B-Vision-Instruct --dist_checkpoint_root_folder /home/kaiwu/work/fb_connect/finetune_11bmodel --dist_checkpoint_folder fine-tuned  --use_fast_kernels --dataset \"custom_dataset\" --custom_dataset.test_split \"test\" --custom_dataset.file \"recipes/quickstart/finetuning/datasets/ocrvqa_dataset.py\"  --run_validation True --batching_strategy padding  --use_peft --peft_method lora --flop_counter --flop_counter_start 4 --max_train_step 6 --max_eval_step 2\nW0926 10:03:16.057000 140504509277184 torch/distributed/run.py:779] \nW0926 10:03:16.057000 140504509277184 torch/distributed/run.py:779] *****************************************\nW0926 10:03:16.057000 140504509277184 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0926 10:03:16.057000 140504509277184 torch/distributed/run.py:779] *****************************************\n/home/kaiwu/work/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n  from torch.distributed._shard.checkpoint import (\n/home/kaiwu/work/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n  from torch.distributed._shard.checkpoint import (\n/home/kaiwu/work/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n  from torch.distributed._shard.checkpoint import (\n/home/kaiwu/work/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n  from torch.distributed._shard.checkpoint import (\nClearing GPU cache for all ranks\n--> Running with torch dist debug set to detail\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00,  7.27it/s]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:01<00:00,  4.82it/s]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:01<00:00,  4.78it/s]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:01<00:00,  3.59it/s]\n--> Model meta-llama/Llama-3.2-11B-Vision-Instruct\n\n--> meta-llama/Llama-3.2-11B-Vision-Instruct has 10670.220835 Million params\n\ntrainable params: 5,898,240 || all params: 10,676,119,075 || trainable%: 0.0552\nbFloat16 enabled for mixed precision - using bfSixteen policy\ntrainable params: 5,898,240 || all params: 10,676,119,075 || trainable%: 0.0552\ntrainable params: 5,898,240 || all params: 10,676,119,075 || trainable%: 0.0552\ntrainable params: 5,898,240 || all params: 10,676,119,075 || trainable%: 0.0552\n--> applying fsdp activation checkpointing...\n--> applying fsdp activation checkpointing...\n--> applying fsdp activation checkpointing...\n--> applying fsdp activation checkpointing...\n--> Training Set Length = 1800\n--> Validation Set Length = 200\nlength of dataset_train 1800\ncustom_data_collator is used\n--> Num of Training Set Batches loaded = 225\nlength of dataset_train 1800\ncustom_data_collator is used\n--> Num of Training Set Batches loaded = 225\n--> Num of Validation Set Batches loaded = 50\n--> Num of Validation Set Batches loaded = 50\nStarting epoch 0/3\ntrain_config.max_train_step: 6\n--> Num of Validation Set Batches loaded = 50\n--> Num of Validation Set Batches loaded = 50\nStarting epoch 0/3\ntrain_config.max_train_step: 6\nlength of dataset_train 1800\ncustom_data_collator is used\n--> Num of Training Set Batches loaded = 225\n/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\n/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                                                                                                  | 0/225 [00:00<?, ?it/s]--> Num of Validation Set Batches loaded = 50\n--> Num of Validation Set Batches loaded = 50\nStarting epoch 0/3\ntrain_config.max_train_step: 6\n/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                                                                                                  | 0/225 [00:00<?, ?it/s]NCCL version 2.20.5+cuda12.4\nlength of dataset_train 1800\ncustom_data_collator is used\n--> Num of Training Set Batches loaded = 225\n--> Num of Validation Set Batches loaded = 50\n--> Num of Validation Set Batches loaded = 50\nStarting epoch 0/3\ntrain_config.max_train_step: 6\n/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                                                                                                  | 0/225 [00:00<?, ?it/s]/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\nTraining Epoch: 1/3, step 5/225 completed (loss: 1.0278592109680176):   3%|\u2588\u2589                                                                     | 6/225 [00:21<09:49,  2.69s/it]max training steps reached, stopping training, total train steps finished:  6\nTraining Epoch: 1/3, step 5/225 completed (loss: 1.0278592109680176):   3%|\u2588\u2589                                                                     | 6/225 [00:21<12:59,  3.56s/it]\nTotal time used in this flop counting step is: 2.336395263671875\nThe total TFlop per second is: 25.241005088821396\nThe tflop_count table is below:\nModule                                                        FLOP    % Total\n---------------------------------------------------------  -------  ---------\nFullyShardedDataParallel                                   58.973T    100.00%\n - aten.convolution                                         0.019T      0.03%\n - aten.bmm                                                 0.000T      0.00%\n - aten.mm                                                 26.474T     44.89%\n - aten._scaled_dot_product_efficient_attention            17.496T     29.67%\n - aten.addmm                                              14.295T     24.24%\n - aten._scaled_dot_product_efficient_attention_backward    0.689T      1.17%\n FullyShardedDataParallel._fsdp_wrapped_module             58.973T    100.00%\n  - aten.convolution                                        0.019T      0.03%\n  - aten.bmm                                                0.000T      0.00%\n  - aten.mm                                                26.474T     44.89%\n  - aten._scaled_dot_product_efficient_attention           17.496T     29.67%\n  - aten.addmm                                             14.295T     24.24%\n  - aten._scaled_dot_product_efficient_attention_backward   0.689T      1.17%\nTraining Epoch: 1/3, step 5/225 completed (loss: 0.5811285376548767):   3%|\u2588\u2589                                                                     | 6/225 [00:21<13:00,  3.56s/it]\nTraining Epoch: 1/3, step 5/225 completed (loss: 0.7257668972015381):   3%|\u2588\u2589                                                                     | 6/225 [00:19<12:08,  3.33s/it]\nTraining Epoch: 1/3, step 5/225 completed (loss: 0.5231667757034302):   3%|\u2588\u2589                                                                     | 6/225 [00:20<12:39,  3.47s/it]\nMax CUDA memory allocated was 13 GB\nMax CUDA memory reserved was 16 GB\nPeak active CUDA memory was 13 GB\nCUDA Malloc retries : 0\nCPU Total Peak Memory consumed during the train (max): 10 GB\nevaluating Epoch:   4%|\u2588\u2588\u2588\u2588\u2589                                                                                                                       | 2/50 [00:01<00:26,  1.80it/s]max eval steps reached, stopping evaluation, total_eval_steps:  2\nevaluating Epoch:   4%|\u2588\u2588\u2588\u2588\u2589                                                                                                                       | 2/50 [00:01<00:31,  1.52it/s]\nevaluating Epoch:   4%|\u2588\u2588\u2588\u2588\u2589                                                                                                                       | 2/50 [00:01<00:33,  1.42it/s]\nevaluating Epoch:   4%|\u2588\u2588\u2588\u2588\u2589                                                                                                                       | 2/50 [00:01<00:33,  1.42it/s]\nevaluating Epoch:   4%|\u2588\u2588\u2588\u2588\u2589                                                                                                                       | 2/50 [00:01<00:31,  1.52it/s]\n eval_ppl=tensor(1.0339, device='cuda:0') eval_epoch_loss=tensor(0.0333, device='cuda:0')\nwe are about to save the PEFT modules\n/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n  warnings.warn(\n/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n  warnings.warn(\n/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n  warnings.warn(\n/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n  warnings.warn(\nPEFT modules are saved in PATH/to/save/PEFT/model directory\nStarting epoch 1/3\ntrain_config.max_train_step: 6\nStarting epoch 1/3\ntrain_config.max_train_step: 6\nStarting epoch 1/3\nbest eval loss on epoch 1 is 0.03330489993095398train_config.max_train_step: 6\n\nEpoch 1: train_perplexity=1.0229, train_epoch_loss=0.0226, epoch time 22.056724132038653s\nStarting epoch 1/3\ntrain_config.max_train_step: 6\nKey: avg_train_prep, Value: 1.0228856801986694\nKey: avg_train_loss, Value: 0.022627701982855797\nKey: avg_eval_prep, Value: 1.0338656902313232\nKey: avg_eval_loss, Value: 0.03330489993095398\nKey: avg_epoch_time, Value: 22.056724132038653\nKey: avg_checkpoint_time, Value: 14.03366973111406\nKey: model_tflops, Value: 25.241005088821396\n```\n\n- [x] 8B samsum_dataset FSDP+QLORA working\n```FSDP_CPU_RAM_EFFICIENT_LOADING=1 ACCELERATE_USE_FSDP=1 torchrun --nnodes 1 --nproc_per_node 4  recipes/quickstart/finetuning/finetuning.py --enable_fsdp  --quantization 4bit --model_name meta-llama/Llama-3.1-70B-Instruct  --mixed_precision False --low_cpu_fsdp --use_peft --peft_method lora --output_dir Path/to/save/PEFT/model --samsum_dataset.trust_remote_code=True\nW0926 11:32:56.213000 140228239340544 torch/distributed/run.py:779] \nW0926 11:32:56.213000 140228239340544 torch/distributed/run.py:779] *****************************************\nW0926 11:32:56.213000 140228239340544 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0926 11:32:56.213000 140228239340544 torch/distributed/run.py:779] *****************************************\n/home/kaiwu/work/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n  from torch.distributed._shard.checkpoint import (\n/home/kaiwu/work/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n  from torch.distributed._shard.checkpoint import (\n/home/kaiwu/work/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n  from torch.distributed._shard.checkpoint import (\n/home/kaiwu/work/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n  from torch.distributed._shard.checkpoint import (\nClearing GPU cache for all ranks\n--> Running with torch dist debug set to detail\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:23<00:00,  1.27it/s]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:23<00:00,  1.26it/s]\nLoading checkpoint shards:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                  | 21/30 [00:25<00:12,  1.35s/it]trainable params: 16,384,000 || all params: 70,570,090,496 || trainable%: 0.0232\ntrainable params: 16,384,000 || all params: 70,570,090,496 || trainable%: 0.0232\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:24<00:00,  1.20it/s]\nLoading checkpoint shards:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                              | 22/30 [00:26<00:10,  1.29s/it]trainable params: 16,384,000 || all params: 70,570,090,496 || trainable%: 0.0232\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:34<00:00,  1.15s/it]\n--> Model meta-llama/Llama-3.1-70B-Instruct\n\n--> meta-llama/Llama-3.1-70B-Instruct has 2102.665216 Million params\n\ntrainable params: 16,384,000 || all params: 70,570,090,496 || trainable%: 0.0232\nNCCL version 2.20.5+cuda12.4\n--> applying fsdp activation checkpointing...\n--> applying fsdp activation checkpointing...\n--> applying fsdp activation checkpointing...\nMap:  24%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                             | 3504/14732 [00:01<00:03, 3030.93 examples/s]--> applying fsdp activation checkpointing...\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:04<00:00, 3020.90 examples/s]\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:04<00:00, 2954.71 examples/s]\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:05<00:00, 2941.05 examples/s]\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 818/818 [00:00<00:00, 2957.14 examples/s]\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:02<00:00, 6706.07it/s]\nlength of dataset_train 639\n--> Num of Training Set Batches loaded = 39\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:04<00:00, 3010.72 examples/s]\n--> Training Set Length = 14732\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:02<00:00, 6836.68it/s]\nlength of dataset_train 639\n--> Num of Training Set Batches loaded = 39\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 818/818 [00:00<00:00, 7111.26it/s]\n--> Num of Validation Set Batches loaded = 8\n--> Num of Validation Set Batches loaded = 8\nStarting epoch 0/3\ntrain_config.max_train_step: 0\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:02<00:00, 6887.25it/s]\nlength of dataset_train 639\n--> Num of Training Set Batches loaded = 39\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 818/818 [00:00<00:00, 6904.05it/s]\n--> Num of Validation Set Batches loaded = 8\n--> Num of Validation Set Batches loaded = 8\nStarting epoch 0/3\ntrain_config.max_train_step: 0\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 818/818 [00:00<00:00, 7007.97it/s]\n--> Num of Validation Set Batches loaded = 8\n--> Num of Validation Set Batches loaded = 8\nStarting epoch 0/3\ntrain_config.max_train_step: 0\n/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                                                                                                   | 0/39 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                                                                                                   | 0/39 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                                                                                                   | 0/39 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n--> Validation Set Length = 818\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14732/14732 [00:02<00:00, 6885.90it/s]\nlength of dataset_train 639\n--> Num of Training Set Batches loaded = 39\nPreprocessing dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 818/818 [00:00<00:00, 7015.87it/s]\n--> Num of Validation Set Batches loaded = 8\n--> Num of Validation Set Batches loaded = 8\nStarting epoch 0/3\ntrain_config.max_train_step: 0\n/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                                                                                                   | 0/39 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\nTraining Epoch: 1/3, step 5/39 completed (loss: 1.2683604955673218):  15%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                                             | 6/39 [02:27<13:11, 24.00s/it]\n```\n\n- [x] 11B OCRVQA FSDP+QLORA working\n```FSDP_CPU_RAM_EFFICIENT_LOADING=1 ACCELERATE_USE_FSDP=1 torchrun --nnodes 1 --nproc_per_node 4  recipes/quickstart/finetuning/finetuning.py --enable_fsdp  --quantization 4bit --model_name meta-llama/Llama-3.2-11B-Vision-Instruct --dataset \"custom_dataset\" --custom_dataset.test_split \"test\" --custom_dataset.file \"recipes/quickstart/finetuning/datasets/ocrvqa_dataset.py\"   --mixed_precision False --low_cpu_fsdp --use_peft --peft_method lora --output_dir Path/to/save/PEFT/model --batching_strategy padding \nW0926 11:41:41.877000 139964614112256 torch/distributed/run.py:779] \nW0926 11:41:41.877000 139964614112256 torch/distributed/run.py:779] *****************************************\nW0926 11:41:41.877000 139964614112256 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0926 11:41:41.877000 139964614112256 torch/distributed/run.py:779] *****************************************\n/home/kaiwu/work/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n  from torch.distributed._shard.checkpoint import (\n/home/kaiwu/work/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n  from torch.distributed._shard.checkpoint import (\n/home/kaiwu/work/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n  from torch.distributed._shard.checkpoint import (\n/home/kaiwu/work/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n  from torch.distributed._shard.checkpoint import (\nClearing GPU cache for all ranks\n--> Running with torch dist debug set to detail\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:03<00:00,  1.32it/s]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:03<00:00,  1.36it/s]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:03<00:00,  1.34it/s]\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:05<00:00,  1.19s/it]\ntrainable params: 5,898,240 || all params: 10,676,119,075 || trainable%: 0.0552\ntrainable params: 5,898,240 || all params: 10,676,119,075 || trainable%: 0.0552\ntrainable params: 5,898,240 || all params: 10,676,119,075 || trainable%: 0.0552\n--> Model meta-llama/Llama-3.2-11B-Vision-Instruct\n\n--> meta-llama/Llama-3.2-11B-Vision-Instruct has 1128.179235 Million params\n\ntrainable params: 5,898,240 || all params: 10,676,119,075 || trainable%: 0.0552\nNCCL version 2.20.5+cuda12.4\n--> applying fsdp activation checkpointing...\n--> applying fsdp activation checkpointing...\n--> applying fsdp activation checkpointing...\n--> applying fsdp activation checkpointing...\n--> Training Set Length = 1800\n--> Validation Set Length = 200\nlength of dataset_train 1800\ncustom_data_collator is used\n--> Num of Training Set Batches loaded = 112\nlength of dataset_train 1800\ncustom_data_collator is used\n--> Num of Training Set Batches loaded = 112\nlength of dataset_train 1800\ncustom_data_collator is used\n--> Num of Training Set Batches loaded = 112\n--> Num of Validation Set Batches loaded = 50\n--> Num of Validation Set Batches loaded = 50\nStarting epoch 0/3\ntrain_config.max_train_step: 0\n--> Num of Validation Set Batches loaded = 50\n--> Num of Validation Set Batches loaded = 50\nStarting epoch 0/3\ntrain_config.max_train_step: 0\n/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                                                                                                  | 0/112 [00:00<?, ?it/s]--> Num of Validation Set Batches loaded = 50\n--> Num of Validation Set Batches loaded = 50\nStarting epoch 0/3\ntrain_config.max_train_step: 0\n/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                                                                                                  | 0/112 [00:00<?, ?it/s]/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                                                                                                  | 0/112 [00:00<?, ?it/s]length of dataset_train 1800\ncustom_data_collator is used\n--> Num of Training Set Batches loaded = 112\n--> Num of Validation Set Batches loaded = 50\n--> Num of Validation Set Batches loaded = 50\nStarting epoch 0/3\ntrain_config.max_train_step: 0\n/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nTraining Epoch: 1:   0%|                                                                                                                                  | 0/112 [00:00<?, ?it/s]/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/home/kaiwu/miniconda3/envs/recipe_test/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\nTraining Epoch: 1/3, step 14/112 completed (loss: 1.0585829019546509):  13%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                                           | 15/112 [00:28<02:40,  1.66s/it]\n```\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "719": "Fix link to LLM finetuning overview# What does this PR do?\n\nFixes a link that lead to the correct path but the text was empty.\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n", "725": "Fix fine-tuning training loss accumulation# What does this PR do?\n\n## Problem:\nIn /src/llama_recipes/utils/train_utils.py the training loss is correctly divided by the # of gradient accumulation steps to scale down the gradient:\n\nloss = loss / gradient_accumulation_steps\n\nThe training loss is then accumulated\n\ntotal_loss += loss.detach().float()\n\nand used in the following to calculate the average loss across all samples in the epoch:\n\ntrain_epoch_loss = total_loss / len(train_dataloader)\n\nAs the accumulated loss is scaled down by gradient_accumulation_steps and len(train_dataloader) includes all steps (even the gradient accumulation ones), train_epoch_loss is gradient_accumulation_steps times lower than it should be.\n\n## Solution:\n\nAccumulate the loss\ntotal_loss += loss.detach().float()\n\nbefore scaling it down\n\nloss = loss / gradient_accumulation_steps\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ X ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "731": "Zero-to-Llama-CourseSeries of Notebooks going from Zero-to-hero for Llama model family \n\n- [ ] Module 1: Understanding Llama\n- [ ] Module 2: Prompt Engineering?\n- [ ] Module 3: Agents and Tool Calling\n- [ ] Module 4: Safety\n- [ ] Module 5: Memory\n- [ ] Module 6: Synthetic Data\n- [ ] Module 7: Evaluations?\n- [ ] Module 8: Fine-Tuning\n- [ ] Module 9: Small Models?\n- [ ] Module 10: Capstone?", "902": "update HELM link# What does this PR do?\n\nFixes broken link to HELM https://github.com/stanford-crfm/helm/issues/3454#issuecomment-2741739277\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [] Did you read the [contributor guideline](https://github.com/meta-llama/llama-cookbook/blob/main/CONTRIBUTING.md),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "916": "[Llama4] Book character mindmap# Overview\n\nHave you ever struggled to follow relationships while reading a book?\nThis application can help you visualize characters' relationships, allowing for a deeper understanding of your reading.\n\nBy leveraging the impressive capabilities of Llama4, which can process over 1 million input context tokens at once, this application provides a unique and powerful tool for readers", "269": "Fix typo# What does this PR do?\n\nFix typo in docs\n\n", "527": "Updating links to running llama3 locally# What does this PR do?\n\nUpdates the links to the Running Llama locally notebook\n", "279": "Llama 2 On-Prem Inference Using vLLM and TGI# What does this PR do?\n\nThis tutorial shows how to use Llama 2 with vLLM and Hugging Face TGI and how to use LangChain to talk to vLLM and TGI hosted Llama 2 on prem.\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [X ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [X ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "251": "fix incorrect split of InstructionDataset# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n`InstructionDataset` did not correctly split training set and test set, resulting in the test set being used for training.\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nAdapted from finetuning.py\n```python\nimport fire\nimport os\nimport torch\nimport torch.distributed as dist\nfrom torch.utils.data import DistributedSampler\nfrom transformers import (\n    LlamaTokenizer,\n    default_data_collator,\n)\nfrom llama_recipes.configs import fsdp_config, train_config\nfrom llama_recipes.utils.config_utils import (\n    update_config,\n    generate_dataset_config,\n)\nfrom llama_recipes.utils.train_utils import (\n    setup,\n    setup_environ_flags,\n    clear_gpu_cache,\n)\nfrom llama_recipes.utils.dataset_utils import get_preprocessed_dataset\n\ndef register_hook(model: torch.nn.Module, name, hook):\n    module_names = []\n    for n, m in model.named_modules():\n        if name in n:\n            m.register_forward_hook(hook=hook)\n            module_names.append(n)\n    return module_names\n\ndef main(**kwargs):\n    update_config((train_config, fsdp_config), **kwargs)\n    if train_config.enable_fsdp:\n        setup()\n        # torchrun specific\n        local_rank = int(os.environ[\"LOCAL_RANK\"])\n        rank = int(os.environ[\"RANK\"])\n        world_size = int(os.environ[\"WORLD_SIZE\"])\n    if torch.distributed.is_initialized():\n        torch.cuda.set_device(local_rank)\n        clear_gpu_cache(local_rank)\n        setup_environ_flags(rank)\n    \n    dataset_config = generate_dataset_config(train_config, kwargs)\n\n    tokenizer = LlamaTokenizer.from_pretrained(train_config.model_name)\n    tokenizer.add_special_tokens(\n            {\n\n                \"pad_token\": \"<PAD>\",\n            }\n        )\n    dataset_train = get_preprocessed_dataset(\n        tokenizer,\n        dataset_config,\n        split=\"train\",\n    )\n    dataset_val = get_preprocessed_dataset(\n        tokenizer,\n        dataset_config,\n        split=\"test\",\n    )\n    train_sampler = DistributedSampler(\n        dataset_train,\n        rank=dist.get_rank(),\n        num_replicas=dist.get_world_size(),\n        shuffle=True,\n    )\n    if train_config.run_validation:\n        val_sampler = DistributedSampler(\n            dataset_val,\n            rank=dist.get_rank(),\n            num_replicas=dist.get_world_size(),\n        )\n    train_dataloader = torch.utils.data.DataLoader(\n        dataset_train,\n        batch_size=train_config.batch_size_training,\n        num_workers=train_config.num_workers_dataloader,\n        pin_memory=True,\n        sampler=train_sampler if train_sampler else None,\n        drop_last=True,\n        collate_fn=default_data_collator,\n    )\n    eval_dataloader = None\n    if train_config.run_validation:\n        eval_dataloader = torch.utils.data.DataLoader(\n            dataset_val,\n            batch_size=train_config.val_batch_size,\n            num_workers=train_config.num_workers_dataloader,\n            pin_memory=True,\n            sampler=val_sampler if val_sampler else None,\n            drop_last=True,\n            collate_fn=default_data_collator,\n        )\n    \n    eval_decoded = []\n    for batch in eval_dataloader:\n        decoded = tokenizer.decode(batch['input_ids'][0])\n        eval_decoded.append(decoded)\n    print(f'eval length: {len(eval_decoded)}')\n    \n    from tqdm import tqdm\n    found_idx = []\n    found_idx_reversed = []\n    for idx, batch in enumerate(tqdm(train_dataloader)):\n        decoded = tokenizer.decode(batch['input_ids'][0])\n        if decoded in eval_decoded:\n            print(f'found: {idx} ({-(len(train_dataloader) - idx)})')\n            found_idx.append(idx)\n            found_idx_reversed.append(-(len(train_dataloader) - idx))\n    print(f'found_idx({len(found_idx)}): {found_idx}')\n    print(f'found_idx_reversed({len(found_idx_reversed)}): {found_idx_reversed}')\n\nif __name__ == '__main__':\n    fire.Fire(main)\n\n```\nLogs for Test A\n```\neval length: 200\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 279444/279444 [13:36<00:00, 342.21it/s]\nfound_idx(0): []\nfound_idx_reversed(0): []\n```\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [x] Did you make sure to update the documentation with your changes?  (no need to update)\n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "523": "fix singlegpu_finetuning.md typosJust fixes some small typos.", "912": "Fix Maverick typo\nFix single line typo in cookbook notebook\n\n\n\n", "906": "Google Calendar Assistant with with Llama 3.2 3B  Tool Calling # What does this PR do?\n\nThis PR shows the following\n- Tool Calling with Llama 3.2 3B model\n- Shows Llama 3.2 3B model capable to return 2 tool calls.\n- Integration with Google Contacts(People) & Google Calendar API\n- Greatly improves the accuracy by splitting the prompt flow into 2 passes: Prompt classification & prompt execution\n\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/meta-llama/llama-cookbook/blob/main/CONTRIBUTING.md),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "26": "Adding Supporting Files For link and Spell CheckAdding files that will be used by the GitHub Action for Spell and Link checks.\n\nWill add the Github workflow once these files get added.\n\nThis set up has been tested on a test repo and is taken from the TorchServe repo. \n\nAlso adding the updated HF conversion instructions per issue #3 and minor update to inference.md doc", "866": "add MIT license to root# What does this PR do?\n\nFixes # https://github.com/meta-llama/llama-cookbook/issues/749 \n", "696": "Update multi_modal_infer.pyFix Position args for inference", "480": "Finetuning Llama2 7b by QLoRA 4bit# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "494": "[OctoAI model provider] Llama3 update# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nThis PR does the following:\n* It updates all OctoAI (Llama model API provider) examples to use the latest Llama3 model instead of Llama2\n* Some function signatures changed in Langchain around `OctoAIEndpoint` so code was updated to stay up to date with latest langchain package (langchain==0.1.19)\n* Examples involving live search and video summarization were updated to reflect the changes in their Replicate counterpart\nCC @HamidShojanazeri \n\n\n## Feature/Issue validation/testing\n\nAll notebooks modified were tested on a Macbook M1 machine with a fresh python venv.\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "319": "Add example conversion script to convert hf to consolidated weight format# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes https://github.com/facebookresearch/llama/issues/570\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [X]  Convert 7Bf checkpoint weights\n```\nmkdir ~/llama2/test7B; cp ~/llama2/llama-2-7b-chat/params.json ~/llama2/test7B\npython -m llama_recipes.tools.convert_hf_weights_to_llama --model-path meta-llama/Llama-2-7b-chat-hf --output-dir ~/llama2/test7B --model-size 7B\n\npython compare_llama_weights.py ~/llama2/test7B ~/llama2/llama-2-7b-chat\nComparing shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:43<00:00, 43.96s/it]\nTop 10 largest deltas:\n  shard 0 tok_embeddings.weight: 2.9802322387695312e-08\n  shard 0 output.weight: 2.9802322387695312e-08\n  shard 0 layers.0.attention.wq.weight: 2.9802322387695312e-08\n  shard 0 layers.0.attention.wk.weight: 2.9802322387695312e-08\n  shard 0 layers.0.attention.wv.weight: 2.9802322387695312e-08\n  shard 0 layers.0.attention.wo.weight: 2.9802322387695312e-08\n  shard 0 layers.0.feed_forward.w1.weight: 2.9802322387695312e-08\n  shard 0 layers.0.feed_forward.w2.weight: 2.9802322387695312e-08\n  shard 0 layers.0.feed_forward.w3.weight: 2.9802322387695312e-08\n  shard 0 layers.1.attention.wq.weight: 2.9802322387695312e-08\n```\n\n- [X]  Converted 7Bf inference test\n```\ntorchrun --nproc_per_node 1 example_chat_completion.py --ckpt_dir ~/llama2/test7B --tokenizer_path ~/llama2/tokenizer.model\n\n==================================\n\nSystem: You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n\nUser: Write a brief birthday message to John\n\n> Assistant:  Of course! Here is a brief and respectful birthday message for John:\n\"Happy birthday, John! I hope your day is filled with joy, love, and all your favorite things. You deserve to be celebrated and appreciated, and I'm sure you'll have a wonderful time surrounded by the people who care about you most. Here's to another year of growth, happiness, and success! \ud83c\udf89\ud83c\udf82\"\n\n\n```\n\n- [X]  Convert 70B checkpoint weights\n```\nmkdir ~/llama2/test70B; cp ~/llama2/llama-2-70b-chat/params.json ~/llama2/test70B\npython -m llama_recipes.tools.convert_hf_weights_to_llama --model-path meta-llama/Llama-2-70b-chat-hf --output-dir ~/llama2/test70B --model-size 70B\n\npython compare_llama_weights.py ~/llama2/test70B ~/llama2/llama-2-70b-chat\nComparing shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 8/8 [01:41<00:00, 12.65s/it]\nTop 10 largest deltas:\n  shard 0 tok_embeddings.weight: 2.9802322387695312e-08\n  shard 0 output.weight: 2.9802322387695312e-08\n  shard 0 layers.0.attention.wq.weight: 2.9802322387695312e-08\n  shard 0 layers.0.attention.wk.weight: 2.9802322387695312e-08\n  shard 0 layers.0.attention.wv.weight: 2.9802322387695312e-08\n  shard 0 layers.0.attention.wo.weight: 2.9802322387695312e-08\n  shard 0 layers.0.feed_forward.w1.weight: 2.9802322387695312e-08\n  shard 0 layers.0.feed_forward.w2.weight: 2.9802322387695312e-08\n  shard 0 layers.0.feed_forward.w3.weight: 2.9802322387695312e-08\n  shard 0 layers.0.attention_norm.weight: 2.9802322387695312e-08\n```\n\n- [X]  Converted 70B inference test\n```\ntorchrun --nproc_per_node 8 example_chat_completion.py --ckpt_dir ~/llama2/test70B --tokenizer_path ~/llama2/tokenizer.model\n\n==================================\n\nSystem: You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n\nUser: Write a brief birthday message to John\n\n> Assistant:  \"Dear John, I hope your birthday is filled with joy, love, and all your favorite things. May this year bring you success, happiness, and countless moments to cherish. Wishing you a wonderful day and a brilliant year ahead! \ud83c\udf89\ud83c\udf82\u2764\ufe0f\"\n\n```\n\n## Before submitting\n- [X ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ X] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case. \n  - https://github.com/facebookresearch/llama/issues/570\n\nThanks for contributing \ud83c\udf89!\n", "325": "Changing Llama Guard safety check to HF classes# What does this PR do?\nChanges the Llama Guard safety check to use HF classes. Removes the previously used code to load Llama Guard.\n\n## Feature/Issue validation/testing\n\n\n* Llama guard enabled:\n* Command: `python examples/inference.py --model_name ../llama/models_hf/7B --prompt_file examples/test_prompt_2.txt --enable_llamaguard_content_safety --enable_salesforce_content_safety False`\n* Result\n![image](https://github.com/facebookresearch/llama-recipes/assets/222731/f6581a89-3131-444f-a245-60b186785f05)\n\n* Llama Guard disabled:\n* Command: `python examples/inference.py --model_name ../llama/models_hf/7B --prompt_file examples/test_prompt_2.txt`\n* Result\n![image](https://github.com/facebookresearch/llama-recipes/assets/222731/902ed17c-3983-44c9-ae98-c7d759955b69)\n\n* Salesforce, Llama Guard and sensitive topics enabled:\n* Command: `python examples/inference.py --model_name ../llama/models_hf/7B --prompt_file examples/test_prompt_2.txt --enable_llamaguard_content_safety --enable_sensitive_topics`\n* Result\n![image](https://github.com/facebookresearch/llama-recipes/assets/222731/448c30d4-e874-4f68-946a-645ecfd519c6)\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [x] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "443": "Fix llama3 urls + chat completion termination + nightlies in readme# What does this PR do?\nThis PR fix:\n* Identifiers for llama3 in tests\n* Correct termination condition in chat completion example\n* Remove nightlies installation in readme\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "457": "Update Getting_to_know_Llama.ipynb# What does this PR do?\n\nupdate LLaMa family\n![image](https://github.com/meta-llama/llama-recipes/assets/20925537/fd496d4c-7ac7-421b-b1f4-15b769a24625)\n\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "331": "Add inference throughput benchmark on-prem vllm# What does this PR do?\nThis is the 1st PR as part of the series to add inference throughput benchmarks for Llama 2 models. \nIn this PR, it adds benchmark scripts, sample input prompts and instructions to run throughput benchmark on-prem for vLLM containers.\nThe reasons on why we are adding these benchmarks and upcoming benchmarks are in the README file. \n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [X] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [X] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [X] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "456": "updating the READMEs to llama3# What does this PR do?\n\nUpdating REAMEs to Llama3.\n", "324": "Add examples for Azure Llama 2 API (Model-as-a-Service)# What does this PR do?\nThis PR adds examples to use Azure newly released Llama 2 API solutions as part of their Model-as-a-Service program. The examples focus on using the API in command line, Python and in LangChain. Then, it features a chatbot example with conversation memory using these APIs. More information regarding Azure Llama 2 API offering can be found [here](https://techcommunity.microsoft.com/t5/ai-machine-learning-blog/announcing-llama-2-inference-apis-and-hosted-fine-tuning-through/ba-p/3979227) \n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [X] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [X] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "318": "Fix/unit tests# What does this PR do?\nThis PR removes our usage of decapoda-research/llama-7b-hf in the unit tests as a surrogate for the actual llama tokenizer. As an alternative we skip the tests if we can not access the gated model folder.\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [X] Test A `pytest tests`\n```\n============================================================================================================ test session starts =============================================================================================================\nplatform linux -- Python 3.10.13, pytest-7.4.3, pluggy-1.3.0\nrootdir: /home/mreso/llama-recipes\nconfigfile: pyproject.toml\nplugins: mock-3.12.0\ncollected 23 items\n\ntests/test_batching.py ..                                                                                                                                                                                                              [  8%]\ntests/test_finetuning.py .....                                                                                                                                                                                                         [ 30%]\ntests/test_sampler.py ..........                                                                                                                                                                                                       [ 73%]\ntests/test_train_utils.py .                                                                                                                                                                                                            [ 78%]\ntests/datasets/test_alpaca_dataset.py s                                                                                                                                                                                                [ 82%]\ntests/datasets/test_custom_dataset.py ..                                                                                                                                                                                               [ 91%]\ntests/datasets/test_grammar_datasets.py .                                                                                                                                                                                              [ 95%]\ntests/datasets/test_samsum_datasets.py .                                                                                                                                                                                               [100%]\n\n============================================================================================================== warnings summary ==============================================================================================================\n../.conda/envs/llama/lib/python3.10/site-packages/transformers/utils/generic.py:441\n  /home/mreso/.conda/envs/llama/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n    _torch_pytree._register_pytree_node(\n\nsrc/llama_recipes/finetuning.py:5\n  /home/mreso/llama-recipes/src/llama_recipes/finetuning.py:5: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n    from pkg_resources import packaging\n\n../.conda/envs/llama/lib/python3.10/site-packages/transformers/utils/generic.py:309\n../.conda/envs/llama/lib/python3.10/site-packages/transformers/utils/generic.py:309\n  /home/mreso/.conda/envs/llama/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n    _torch_pytree._register_pytree_node(\n\n../.conda/envs/llama/lib/python3.10/site-packages/torch/distributed/_shard/checkpoint/__init__.py:8\n  /home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/distributed/_shard/checkpoint/__init__.py:8: DeprecationWarning: torch.distributed._shard.checkpoint will be deprecated, use torch.distributed.checkpoint instead\n    warnings.warn(\n\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n================================================================================================= 22 passed, 1 skipped, 5 warnings in 48.58s =================================================================================================\n```\n\n- [X] Test B ```$huggingface-cli logout\nSuccessfully logged out.\n$rm -rf ~/.cache/huggingface/\n$pytest tests```\n\n```\n============================================================================================================ test session starts =============================================================================================================\nplatform linux -- Python 3.10.13, pytest-7.4.3, pluggy-1.3.0\nrootdir: /home/mreso/llama-recipes\nconfigfile: pyproject.toml\nplugins: mock-3.12.0\ncollected 23 items\n\ntests/test_batching.py ss                                                                                                                                                                                                              [  8%]\ntests/test_finetuning.py .....                                                                                                                                                                                                         [ 30%]\ntests/test_sampler.py ..........                                                                                                                                                                                                       [ 73%]\ntests/test_train_utils.py .                                                                                                                                                                                                            [ 78%]\ntests/datasets/test_alpaca_dataset.py s                                                                                                                                                                                                [ 82%]\ntests/datasets/test_custom_dataset.py s.                                                                                                                                                                                               [ 91%]\ntests/datasets/test_grammar_datasets.py s                                                                                                                                                                                              [ 95%]\ntests/datasets/test_samsum_datasets.py s                                                                                                                                                                                               [100%]\n\n============================================================================================================== warnings summary ==============================================================================================================\n../.conda/envs/llama/lib/python3.10/site-packages/transformers/utils/generic.py:441\n  /home/mreso/.conda/envs/llama/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n    _torch_pytree._register_pytree_node(\n\nsrc/llama_recipes/finetuning.py:5\n  /home/mreso/llama-recipes/src/llama_recipes/finetuning.py:5: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n    from pkg_resources import packaging\n\n../.conda/envs/llama/lib/python3.10/site-packages/transformers/utils/generic.py:309\n../.conda/envs/llama/lib/python3.10/site-packages/transformers/utils/generic.py:309\n  /home/mreso/.conda/envs/llama/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n    _torch_pytree._register_pytree_node(\n\n../.conda/envs/llama/lib/python3.10/site-packages/torch/distributed/_shard/checkpoint/__init__.py:8\n  /home/mreso/.conda/envs/llama/lib/python3.10/site-packages/torch/distributed/_shard/checkpoint/__init__.py:8: DeprecationWarning: torch.distributed._shard.checkpoint will be deprecated, use torch.distributed.checkpoint instead\n    warnings.warn(\n\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n================================================================================================= 17 passed, 6 skipped, 5 warnings in 8.32s ==================================================================================================\n```\n- [X] Test C `$pytest tests --unskip-missing-tokenizer`\n```\n========================================================================================================== short test summary info ===========================================================================================================\nERROR tests/test_batching.py::test_packing - OSError: You are trying to access a gated repo.\nERROR tests/test_batching.py::test_distributed_packing - OSError: You are trying to access a gated repo.\nERROR tests/test_finetuning.py::test_finetuning_no_validation - OSError: You are trying to access a gated repo.\nERROR tests/test_finetuning.py::test_finetuning_with_validation - OSError: You are trying to access a gated repo.\nERROR tests/test_finetuning.py::test_finetuning_peft - OSError: You are trying to access a gated repo.\nERROR tests/test_finetuning.py::test_finetuning_weight_decay - OSError: You are trying to access a gated repo.\nERROR tests/test_finetuning.py::test_batching_strategy - OSError: You are trying to access a gated repo.\nERROR tests/test_sampler.py::test_batch_sampler_array[2-False] - OSError: You are trying to access a gated repo.\nERROR tests/test_sampler.py::test_batch_sampler_array[8-False] - OSError: You are trying to access a gated repo.\nERROR tests/test_sampler.py::test_batch_sampler_array[2-True] - OSError: You are trying to access a gated repo.\nERROR tests/test_sampler.py::test_batch_sampler_array[8-True] - OSError: You are trying to access a gated repo.\nERROR tests/test_sampler.py::test_batch_sampler_dict[2-False] - OSError: You are trying to access a gated repo.\nERROR tests/test_sampler.py::test_batch_sampler_dict[8-False] - OSError: You are trying to access a gated repo.\nERROR tests/test_sampler.py::test_batch_sampler_dict[2-True] - OSError: You are trying to access a gated repo.\nERROR tests/test_sampler.py::test_batch_sampler_dict[8-True] - OSError: You are trying to access a gated repo.\nERROR tests/test_sampler.py::test_dist_batch_sampling[2] - OSError: You are trying to access a gated repo.\nERROR tests/test_sampler.py::test_dist_batch_sampling[8] - OSError: You are trying to access a gated repo.\nERROR tests/test_train_utils.py::test_gradient_accumulation - OSError: You are trying to access a gated repo.\nERROR tests/datasets/test_custom_dataset.py::test_custom_dataset - OSError: You are trying to access a gated repo.\nERROR tests/datasets/test_custom_dataset.py::test_unknown_dataset_error - OSError: You are trying to access a gated repo.\nERROR tests/datasets/test_grammar_datasets.py::test_grammar_dataset - OSError: You are trying to access a gated repo.\nERROR tests/datasets/test_samsum_datasets.py::test_samsum_dataset - OSError: You are trying to access a gated repo.\n================================================================================================= 1 skipped, 6 warnings, 22 errors in 9.18s ==================================================================================================\n```\n## Before submitting\n- [X] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [X] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "495": "Prompt script and notebook for easy prompt formatting and function definition# What does this PR do?\n\nAdds a simple script that shows a Gradio interface to correctly format Llama 3 prompts. Additional models and version are upcoming.\n\nAdds a simple notebook to be able to run on Colab as well.\n\n## Feature/Issue validation/testing\n\nTested running the script and notebook and get a formatted prompt and code as well. Run the generated code to get the prompt again as well\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [x] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\n\n", "481": "add transformerengine support# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "654": "Model saving w/o FSDP or PEFTPotential issue: Saving model without FSDP and without PEFT.\n\n# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "898": "Notebook showing how to fine tune llama guard with torchtune# Finetune llama-guard with torchtune\n\nThis PR includes a notebook which does the following:\n\n- Shows how to finetune llama-guard with torchtune\n- Shows how to finetune with a custom dataset & custom prompt template with torchtune\n- Shows how to finetune llama-guard to return multiple PII violations\n\n\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/meta-llama/llama-cookbook/blob/main/CONTRIBUTING.md),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "697": "Tool Calling Tutorial and ExampleAdd Tool Calling 101 and 201 examples", "873": "deprecate OctoAI# What does this PR do?\n\nFixes non existing API provider references\n![image](https://github.com/user-attachments/assets/55a1d773-7391-473a-b1cd-56de8ffbe5b3)\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/meta-llama/llama-cookbook/blob/main/CONTRIBUTING.md),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [x] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "33": "fixing the condition for moving to cudaFixes issues #15 and #29 .", "867": "Ignore HTTP response 429 during link checkIgnore HTTP response error code 429: Too Many Requests when link-checking markdown files.\n\nSome sites, such as Medium, return this error code even though the link is live. Presumably, Medium does this for \"members-only\" articles. Example:\n\nhttps://betterprogramming.pub/text-to-audio-generation-with-bark-clearly-explained-4ee300a3713a\n\nThe 429 on the above article--which is a false positive--causes our linting to fail on the following file, which references that article:\n\n`./end-to-end-use-cases/NotebookLlama/README.md`\n\nHere is an example of this failure:\n\nhttps://github.com/meta-llama/llama-cookbook/actions/runs/12954370628/job/36136168871?pr=866#step:4:226\n\nSee also:\n\nhttps://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n\n## Feature/Issue validation/testing\n\nThis configuration change is shown in the following sample configuration file:\n\nhttps://github.com/tcort/markdown-link-check#config-file-format\n\n\n## Before submitting\n- [NA] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [Y] Did you read the [contributor guideline](https://github.com/meta-llama/llama-cookbook/blob/main/CONTRIBUTING.md), Pull Request section?\n- [NA] Was this discussed/approved via a Github issue? Please add a link to it if that's the case.\n- [NA] Did you make sure to update the documentation with your changes?  \n- [N] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "720": "fix Colab link in quickstart_peft_finetuning.ipynb# What does this PR do?\n\n## Feature/Issue validation/testing\n\nClicking on the \"Open in Colab\" link doesn't open the notebook in Colab because the location in the github repository has changed. This PR only fixes the link in this specific notebook, other notebooks may or may not have the same issue.\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).", "708": "add support for llama vision model conversion# What does this PR do?\n\nUpdated the script to support converting finetuned llama 3.2 vision model to HF format, so it works with multimodal inference.\n\n## Feature/Issue validation/testing\n\n\n\n\nTested following scripts and it works, without the fix it gives conversion error between llama and mllama config.\n```\npython src/llama_recipes/inference/checkpoint_converter_fsdp_hf.py --fsdp_checkpoint_path  /path/to/finetuned/model --consolidated_model_path  /path/to/save/converted/model  --HF_model_path_or_name /home/ubuntu/llama/Llama-3.2-11B-Vision-Instruct/ --multimodal True\n\npython recipes/quickstart/inference/local_inference/multi_modal_infer.py --image_path  /home/ubuntu/chair.jpg --prompt_text \"Describe this image\" --temperature 0.5 --top_p 0.8 --model_name finetuned_model_mind2web/fine-tuned-meta-llama/hf_model/ --hf_token HF_TOKEN\n```\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [x] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "907": "Update `hello_llama_cloud.ipynb` source# PR Summary\nSmall PR - Commit ae010af7d86d2d4589250ba57ed519356f8e5d25 moved `hello_llama_cloud.ipynb`. This PR adjusts sources to changes.", "913": "Update build_with_llama_4.ipynb# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/meta-llama/llama-cookbook/blob/main/CONTRIBUTING.md),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "522": "document less obvious training config parametersJust adds comments to document less obvious training config parameters", "250": "Introduce a tracker API and integrate Aimstack tracker# What does this PR do?\n\nThis pull request adds a tracker API to llama recipes and integrates Aimstack https://aimstack.io/\nThe goal of tracker API is to create a standard template to attach any other experiment trackers.\n\nWith aim integration there is a dependency on the `aim` package which needs to be installed separately before using this.\n\nFixes # (issue)\n\nRelates to issue https://github.com/facebookresearch/llama-recipes/issues/225\n\n#Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [x] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n      https://github.com/facebookresearch/llama-recipes/issues/225\n- [x] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "287": "A tutorial of building a Llama-enabled WhatsApp Chatbot# What does this PR do?\n\nTutorial here: https://github.com/jeffxtang/llama-recipes/blob/whatsapp/demo_apps/whatsapp_llama2.md\n\nREADME's also updated.\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ X] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "291": "Add complete Chatbot example with RAG capability for demo apps# What does this PR do?\n\nThis complete example shows how to build a chatbot with RAG and streaming capability with Llama 2, using frameworks such as LangChain, FAISS, Gradio and Text-generation-inference. For RAG, we used Llama 2 Getting Started Guide published on Meta AI website as our knowledge reference. \n\nFixes # (issue)\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ X] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [X ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "905": "Clean / update LangGraph TutorialUpdate, clean, and improve LangGraph cookbooks.", "911": "push link fix# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/meta-llama/llama-cookbook/blob/main/CONTRIBUTING.md),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "939": "fix links and references# What does this PR do?\n\nFixes whatsapp example links and docs\n\nThanks for contributing \ud83c\udf89!\n", "736": "Update wordlist.txt# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "31": "Update README.mdsdfghjkl;", "871": "Fix/load model with torch dtype auto #663 after cookbook refactor# What does this PR do?\nThis PR loads a model with torch_dtype=auto instead of bfloat16 when we do not specify train_config.use_fp16.\nFor llama models this will not make a difference as their [default dtype is bfloat16](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/blob/main/config.json#L34)\n\nFixes # (issue)\n#656 (kind of)\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [X] `torchrun --nnodes 1 --nproc_per_node 8 ./recipes/quickstart/finetuning/finetuning.py  --model_name meta-llama/Meta-Llama-3.1-8B-Instruct  --enable_fsdp --max_train_step=2 --batch_size_training 1 --batching_strategy packing --dataset samsum_dataset --save_model False --context_length 4096 --fsdp_config.pure\n_bf16 --fsdp_config.optimizer anyprecision --samsum_dataset.trust_remote_code 1`\n\nLogs\n```\nW0203 15:55:32.585000 720518 site-packages/torch/distributed/run.py:793] \nW0203 15:55:32.585000 720518 site-packages/torch/distributed/run.py:793] *****************************************\nW0203 15:55:32.585000 720518 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0203 15:55:32.585000 720518 site-packages/torch/distributed/run.py:793] *****************************************\n/home/ubuntu/anaconda3/envs/recipe/lib/python3.12/site-packages/llama_cookbook/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n  from torch.distributed._shard.checkpoint import (\n/home/ubuntu/anaconda3/envs/recipe/lib/python3.12/site-packages/llama_cookbook/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n  from torch.distributed._shard.checkpoint import (\nWarning: fsdp_config does not accept parameter: fsdp_config.pure\nWarning: fsdp_config does not accept parameter: fsdp_config.pure\n/home/ubuntu/anaconda3/envs/recipe/lib/python3.12/site-packages/llama_cookbook/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n  from torch.distributed._shard.checkpoint import (\n/home/ubuntu/anaconda3/envs/recipe/lib/python3.12/site-packages/llama_cookbook/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n  from torch.distributed._shard.checkpoint import (\nWarning: fsdp_config does not accept parameter: fsdp_config.pure\n/home/ubuntu/anaconda3/envs/recipe/lib/python3.12/site-packages/llama_cookbook/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n  from torch.distributed._shard.checkpoint import (\nWarning: fsdp_config does not accept parameter: fsdp_config.pure\nWarning: fsdp_config does not accept parameter: fsdp_config.pure\n/home/ubuntu/anaconda3/envs/recipe/lib/python3.12/site-packages/llama_cookbook/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n  from torch.distributed._shard.checkpoint import (\nWarning: fsdp_config does not accept parameter: fsdp_config.pure\n/home/ubuntu/anaconda3/envs/recipe/lib/python3.12/site-packages/llama_cookbook/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n  from torch.distributed._shard.checkpoint import (\n/home/ubuntu/anaconda3/envs/recipe/lib/python3.12/site-packages/llama_cookbook/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n  from torch.distributed._shard.checkpoint import (\nWarning: fsdp_config does not accept parameter: fsdp_config.pure\nWarning: fsdp_config does not accept parameter: fsdp_config.pure\nClearing GPU cache for all ranks\n--> Running with torch dist debug set to detail\n\n...\n\n\nevaluating Epoch: 100%|\u001b[32m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m| 4/4 [00:18<00:00,  4.69s/it]\nStarting epoch 1/3Starting epoch 1/3Starting epoch 1/3\nStarting epoch 1/3\n\ntrain_config.max_train_step: 2\ntrain_config.max_train_step: 2train_config.max_train_step: 2\ntrain_config.max_train_step: 2\n\n\nStarting epoch 1/3\ntrain_config.max_train_step: 2\nStarting epoch 1/3\ntrain_config.max_train_step: 2\nStarting epoch 1/3\ntrain_config.max_train_step: 2\n eval_ppl=tensor(47375.3711, device='cuda:0') eval_epoch_loss=tensor(10.7659, device='cuda:0')\nbest eval loss on epoch 1 is 10.765857696533203\nEpoch 1: train_perplexity=1.1558, train_epoch_loss=0.1448, epoch time 46.755609701387584s\nStarting epoch 1/3\ntrain_config.max_train_step: 2\ntraining params are saved in /home/ubuntu/projects/llama-cookbook-663/PATH/to/save/FSDP/model/fine-tuned-meta-llama/Meta-Llama-3-8B-Instruct/train_params.yaml\nKey: avg_train_prep, Value: 1.1558226346969604\nKey: avg_train_loss, Value: 0.1448122262954712\nKey: avg_eval_prep, Value: 47375.37109375\nKey: avg_eval_loss, Value: 10.765857696533203\nKey: avg_epoch_time, Value: 46.755609701387584\nKey: avg_checkpoint_time, Value: 5.895271897315979e-07\n[rank0]:[W203 16:03:24.135413049 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n\n```\n\n\n## Before submitting\n- [X] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [X] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [X] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n\nThanks for contributing \ud83c\udf89!", "865": "Moving responsible ai to getting_started# What does this PR do?\n\nMove the folders to right location. ", "681": "update mutligpu readme and MllamaForConditionalGeneration importFixes an incorrect parameter in documentation for multigpu that says `int4` instead of `4bit` for quantization type.\n\nFixes incorrect import of MllamaForConditionalGeneration from `transformers` instead of `transformers.models.mllama.modeling_mllama`\n\nFixes #680 \n\n\n## Before submitting\n- [ X] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ X] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ X] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "19": "Update README.mdlink in table of contents doesn't work without this change or changing the link fragment to `#single-gpu-`", "859": "Fix typoFixes typo from SYSTEMP_PROMPT to SYSTEM_PROMPT, sorry found it on more place\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/meta-llama/llama-cookbook/blob/main/CONTRIBUTING.md),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!", "124": "bugfix: remove duplicate load_peft_model# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes #123 \n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [x] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "497": "Fix save metric FileNotFoundError when finetuning# What does this PR do?\n\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "483": "use AutoTokenizer instead of LlamaTokenizer in checkpoint_converter_fsdp_hf.py# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\nSince `LlamaTokenizer` is not compatible with Llama3 tokenizers, running `checkpoint_converter_fsdp_hf.py` with llama3 finetuned weights result in ` TypeError: not a string` error (cf. https://github.com/huggingface/transformers/issues/30607).  This PR is suggesting to use `AutoTokenizer` instead to make the script compatible with both Llama2/3.\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [x] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "468": "updating StructuredLlama, Messenger and WhatsApp for Llama 3# What does this PR do?\n\nStructuredLlama: https://github.com/meta-llama/llama-recipes/blob/demos4llama3v4/recipes/use_cases/text2sql/StructuredLlama.ipynb\n\nMessenger: https://github.com/meta-llama/llama-recipes/blob/demos4llama3v4/recipes/use_cases/chatbots/messenger_llama/messenger_llama3.md\n\nWhatsApp: https://github.com/meta-llama/llama-recipes/blob/demos4llama3v4/recipes/use_cases/chatbots/whatsapp_llama/whatsapp_llama3.md\n\n\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "440": "L3p/readme prompt updatesPrompt updates in readme\n", "326": "Create finetuning data formatter for llama-guardThis creates a utility for formatting custom data for finetuning of llama-guard\n\nTests:\n<img width=\"293\" alt=\"image\" src=\"https://github.com/facebookresearch/llama-recipes/assets/12261336/58bf2e19-2b38-4d1b-94ed-d7df4bfe6299\">", "441": "bumping transformer versions for llama3 support# What does this PR do?\nLlama 3 have some changes in the HF conversion script and tokenizer, this would help to upstream all changes with the new transformer release.[ HF blog post](https://huggingface.co/blog/llama3)\n", "327": "Fix test_finetuning for env without cuda# What does this PR do?\n\nThis PR fixed test_finetuning.py for cpu only envs\n\n## Feature/Issue validation/testing\n\n- [X] Test A\n```\n$ nvidia-smi\nNVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n$ python -m pytest tests/\n============================================================================================================ test session starts =============================================================================================================\nplatform linux -- Python 3.10.13, pytest-7.4.3, pluggy-1.3.0\nrootdir: /home/ubuntu/llama-recipes\nconfigfile: pyproject.toml\nplugins: mock-3.12.0\ncollected 22 items\n\ntests/test_batching.py ss                                                                                                                                                                                                              [  9%]\ntests/test_finetuning.py .....                                                                                                                                                                                                         [ 31%]\ntests/test_sampler.py ..........                                                                                                                                                                                                       [ 77%]\ntests/test_train_utils.py .                                                                                                                                                                                                            [ 81%]\ntests/datasets/test_custom_dataset.py s.                                                                                                                                                                                               [ 90%]\ntests/datasets/test_grammar_datasets.py s                                                                                                                                                                                              [ 95%]\ntests/datasets/test_samsum_datasets.py s                                                                                                                                                                                               [100%]\n\n============================================================================================================== warnings summary ==============================================================================================================\ntests/conftest.py:45\n  /home/ubuntu/llama-recipes/tests/conftest.py:45: PytestRemovedIn8Warning: The pytest_cmdline_preparse hook is deprecated and will be removed in a future release.\n  Please use pytest_load_initial_conftests hook instead.\n    @pytest.hookimpl(tryfirst=True)\n\nsrc/llama_recipes/finetuning.py:5\n  /home/ubuntu/llama-recipes/src/llama_recipes/finetuning.py:5: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n    from pkg_resources import packaging\n\n../miniconda3/envs/llama-recipes/lib/python3.10/site-packages/torch/cuda/__init__.py:611\n  /home/ubuntu/miniconda3/envs/llama-recipes/lib/python3.10/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n    warnings.warn(\"Can't initialize NVML\")\n\n../miniconda3/envs/llama-recipes/lib/python3.10/site-packages/torch/distributed/_shard/checkpoint/__init__.py:8\n  /home/ubuntu/miniconda3/envs/llama-recipes/lib/python3.10/site-packages/torch/distributed/_shard/checkpoint/__init__.py:8: DeprecationWarning: torch.distributed._shard.checkpoint will be deprecated, use torch.distributed.checkpoint instead\n    warnings.warn(\n\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n================================================================================================= 17 passed, 5 skipped, 4 warnings in 1.63s ==================================================================================================\n```\n\n## Before submitting\n- [X] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [X] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "455": "Q&A RAG pipeline with MongoDB, Hugging Face and Llama3# What does this PR do?\n\n**Description of Changes**: This update introduces an integrated pipeline using Retrieval-Augmented Generation (RAG) with MongoDB and Hugging Face's open-source Llama3 model for advanced question answering systems. The implementation covers the setup of a MongoDB database, data ingestion, vector search, and leveraging Llama3 for generating responses. These modifications aim to showcase how combined database and NLP technologies can improve data retrieval and processing capabilities for complex queries.\n\nMotivation and Context: This change was motivated by the need to demonstrate practical applications of combining cutting-edge NLP models with modern database systems for educational and development purposes in the fields of AI and data science.\n\nDependencies:\n\n- datasets for loading and processing the data.\n- pandas for data manipulation.\n- pymongo for interacting with MongoDB.\n- sentence_transformers for generating text embeddings.\n- transformers for accessing pre-trained models from Hugging Face.\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [X ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "469": "folder name change for llama3# What does this PR do?\n\nrename folder Running_Llama2_Anywhere; typo fix 7b-8b\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "482": "Disable prefix tuning and limit llama adapter# What does this PR do?\n\nThis PR disables prefix tuning and limit llama_adapter to non-FSDP use case.\n\nFixes # (issue)\n#359 \n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [X] pytest tests/test_finetuning.py\nLogs for Test A\n```\n====================================================================================================================================== test session starts =======================================================================================================================================\nplatform linux -- Python 3.11.8, pytest-8.1.1, pluggy-1.4.0\nrootdir: /home/mreso/llama-recipes\nconfigfile: pyproject.toml\nplugins: anyio-4.3.0, mock-3.14.0\ncollected 9 items\n\ntests/test_finetuning.py .........                                                                                                                                                                                                                                                         [100%]\n\n======================================================================================================================================== warnings summary ========================================================================================================================================\n../.conda/envs/llama3/lib/python3.11/site-packages/fire/core.py:59\n  /home/mreso/.conda/envs/llama3/lib/python3.11/site-packages/fire/core.py:59: DeprecationWarning: 'pipes' is deprecated and slated for removal in Python 3.13\n    import pipes\n\nsrc/llama_recipes/utils/train_utils.py:9\n  /home/mreso/llama-recipes/src/llama_recipes/utils/train_utils.py:9: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n    from pkg_resources import packaging\n\n../.conda/envs/llama3/lib/python3.11/site-packages/torch/distributed/_shard/checkpoint/__init__.py:8\n  /home/mreso/.conda/envs/llama3/lib/python3.11/site-packages/torch/distributed/_shard/checkpoint/__init__.py:8: DeprecationWarning: torch.distributed._shard.checkpoint will be deprecated, use torch.distributed.checkpoint instead\n    warnings.warn(\n\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n================================================================================================================================= 9 passed, 3 warnings in 3.77s ==================================================================================================================================\n```\n\n## Before submitting\n- [X] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [X] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [X] Did you make sure to update the documentation with your changes?  \n- [X] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "496": "[OctoAI] Introducing a Video Generation use case featuring Llama3# What does this PR do?\nThis example under `lama-recipes/recipes/use_cases` demonstrates a \"model cocktail\" of Llama3, image generation and image animation to produce a full recipe video based on a simple text prompt - the name of the dish.\n\nIn particular this demo shows how one can use Llama3's JSON formatting capabilities to generate a scene by scene description of a recipe making video.\n\nThis demo is powered by OctoAI as the model API provider but the user can feel free to choose whichever model API provider they'd like.\n\n## Feature/Issue validation/testing\n\nThis demo was run on Jupyter notebook on a Macbook M1 machine and in Google Colab (T4-based).\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [x] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "125": "The tokenizer will not add eos_token by default# Add eos_token\n\nThe tokenizer will not add eos_token by default.\nI think the [implementation from Huggingface](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/tokenization_llama.py#L382) is more reasonable.\n\n## Issue validation\n- Test A\n```\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\nprint(format_tokens([[\n        {\"role\": \"system\", \"content\": \"Always answer with Haiku\"},\n        {\"role\": \"user\", \"content\": \"I am going to Paris, what should I see?\"},\n        {\"role\": \"assistant\", \"content\": \"Haiku\"},\n        {\"role\": \"user\", \"content\": \"How are you?\"}\n    ]], tokenizer))\n```\nThe output is\n```\n[[1, 518, 25580, 29962, 3532, 14816, 29903, 6778, 13, 2499, 1994, 1234, 411, 5952, 18282, 13, 29966, 829, 14816, 29903, 6778, 13, 13, 29902, 626, 2675, 304, 3681, 29892, 825, 881, 306, 1074, 29973, 518, 29914, 25580, 29962, 5952, 18282, 29871, 1, 518, 25580, 29962, 1128, 526, 366, 29973, 518, 29914, 25580, 29962]]\n```\nwhich does not include any `eos_token_id`(2)\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\n", "643": "Fix version number in Python example# What does this PR do?\n\nReplace `llama3` with `llama3.1`  in code example to avoid model not found error.\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\nTo reproduce the error, copy and paste the sample python code. The response is `{'error': 'model \"llama3\" not found, try pulling it first'}`.\n\nThis fix was tested manually by observing a proper, non-error response when using the appropriate version number.\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "657": "add_rocm_support_for_mixed_precision# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "131": "remove duplicate peft model load# What does this PR do?\n\nThis PR removes the duplicated peft_model load.\n\n\nFixes # (issue)\n\n#123 \n\n## Feature/Issue validation/testing\n\n\n- [ ] Test A\nBefore change\nhttps://gist.github.com/HamidShojanazeri/d8f6850d4876db4ed9f87274fb9098ea\n\n- [ ] Test B\nAfter change\n https://gist.github.com/HamidShojanazeri/5853c7f2328ca109306a854d8aabbd2c\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "858": "Fixing of typo in Step-2-Transcript-Writer.ipynbFixes typo from SYSTEMP_PROMPT to SYSTEM_PROMPT\n\n## Before submitting\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [x] Did you read the [contributor guideline](https://github.com/meta-llama/llama-cookbook/blob/main/CONTRIBUTING.md),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "864": "Fix more package namingUpdated all import refs of `llama_recipes` and `llama-recipe` MD references to cookbookb", "24": "fix save full state dict checkpoint- Resolved an issue with saving checkpoints as **FULL_STATE_DICT** type in FSDP settings.\n\n- `cfg.checkpoint_dir` does not exist, so Change `cfg.checkpoint_dir` to `cfg.dist_checkpoint_root_folder` and `cfg.dist_checkpoint_folder`\n\n- Since the `verbose` variable does not exist, errors occur in some sections, so add the `verbose` variable to training_config\n\n- import optimize", "737": "Support converting fine-tuned llama 3.2 vision model to HF format and then local inference# What does this PR do?\nThis PR updated the script to support converting fine-tuned llama 3.2 vision model to HF format. Then this PR also changed the inference script to allow loading hf model folder locally without hf_token setting.  The following has been fixed now in [this PR](https://github.com/meta-llama/llama-recipes/pull/741): (However, the [save_pretrained()  function](https://github.com/meta-llama/llama-recipes/blob/fsdp_lmm/src/llama_recipes/inference/checkpoint_converter_fsdp_hf.py#L62) did not save the `preprocessor_config.json` and `chat_template.json`  for mllama model, users have to manually copy those files from HF back to local folder in order to get the local inference working. This bug has been reported to HF and waiting for their help. )\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # ([issue727](https://github.com/meta-llama/llama-recipes/issues/727))\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n- [ ] conversion script working for full fine-tuned fsdp weights.\n```\n(llama) [kaiwu@devgpu003.cco3 ~/work/llama-recipes (fsdp_lmm)]$ python src/llama_recipes/inference/checkpoint_converter_fsdp_hf.py --fsdp_checkpoint_path finetuned_model/fine-tuned-meta-llama/Llama-3.2-11B-Vision-Instruct/ --consolidated_model_path hf_converted\n/home/kaiwu/work/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n  from torch.distributed._shard.checkpoint import (\nModel name: meta-llama/Llama-3.2-11B-Vision-Instruct\nmodel is loaded from config\n/home/kaiwu/work/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:259: FutureWarning: `load_state_dict` is deprecated and will be removed in future versions. Please use `load` instead.\n  dist_cp.load_state_dict(\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/torch/distributed/checkpoint/filesystem.py:657: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  torch.load(cast(IO[bytes], file_slice), map_location=\"cpu\"),\nSharded state checkpoint loaded from finetuned_model/fine-tuned-meta-llama/Llama-3.2-11B-Vision-Instruct/\nmodel is loaded from FSDP checkpoints\nHuggingFace model checkpoints has been saved in hf_converted\n```\n- [ ] Inference working on converted checkpoint after copy the `preprocessor_config.json` and `chat_template.json`\n```\n(llama) [kaiwu@devgpu003.cco3 ~/work/llama-recipes (fsdp_lmm)]$ python recipes/quickstart/inference/local_inference/multi_modal_infer.py --image_path \"./dog.jpg\" --prompt_text \"Describe this image\" --temperature 0.5 --top_p 0.8 --model_name ./hf_converted/\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9/9 [00:10<00:00,  1.12s/it]\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/home/kaiwu/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\nGenerated Text: end_header_id|>\n\nThis image depicts a small dog standing on a skateboard. The dog is a small breed with a white face, brown ears, and a brown body with black and gray patches. It has white paws and a black collar. The dog is standing on a skateboard with red wheels. The background is out of focus, but it appears to be a street with a blue door.<|eot_id|>\n```\n\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "723": "Llama 3.2# What does this PR do?\nAdd new checkpoint converter for the vision models\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nPlease include a good title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Feature/Issue validation/testing\n\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\nPlease also list any relevant details for your test configuration.\n\n- [ ] Test A\nLogs for Test A\n\n- [ ] Test B\nLogs for Test B\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\n      Pull Request section?\n- [x ] Was this discussed/approved via a Github issue? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes?  \n- [ ] Did you write any new necessary tests?\n\nThanks for contributing \ud83c\udf89!\n", "910": "Llama 4 release * Bump tj-actions/changed-files in /.github/workflows\n\nBumps [tj-actions/changed-files](https://github.com/tj-actions/changed-files) from 41.0.0 to 45.0.8.\n- [Release notes](https://github.com/tj-actions/changed-files/releases)\n- [Changelog](https://github.com/tj-actions/changed-files/blob/main/HISTORY.md)\n- [Commits](https://github.com/tj-actions/changed-files/compare/v41.0.0...v45.0.8)\n\n---\nupdated-dependencies:\n- dependency-name: tj-actions/changed-files dependency-type: direct:production ...\n\n\n\n* Add gs nb\n\n* wip: hf l4 nb\n\n* wip: hf nb\n\n* Add L4 Scout notebook\n\n* Update README.md with link to l4 notebook\n\n* Update llama_4_prompt_examples.ipynb\n\n* Adding correct links\n\n* Update README.md\n\n* Update README.md\n\n* add changes\n\n* Update build_with_llama_4.ipynb\n\n* Update build_with_llama_4.ipynb\n\n* deprecate old nb\n\n* Update README.md\n\n* fixing type in readme\n\n---------\n\n# What does this PR do?\n\nUpdates to the latest\nThanks for contributing \ud83c\udf89!\n"}