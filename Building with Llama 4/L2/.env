# Ollama Configuration (Local)
LLAMA_BASE_URL=http://localhost:11434/v1
LLAMA_API_KEY=ollama

# Ollama model to use  
OLLAMA_MODEL=llama4:scout

# If you want to use cloud APIs instead (optional)
# LLAMA_API_KEY=your_actual_llama_api_key_here
# LLAMA_BASE_URL=https://api.llama-api.com

# Other API keys (add as needed)
# TOGETHER_API_KEY=your_together_api_key_here
# OPENAI_API_KEY=your_openai_api_key_here
