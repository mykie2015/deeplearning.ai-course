{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac978610",
   "metadata": {},
   "source": [
    "# Module 2 Assignment: Adding functionalities to your Automatic Code Review Crew\n",
    "\n",
    "Welcome to Module 2's assignment! In this lab, you will build upon the Automatic Code Review Crew you created in the assignment in Module 1. This time, you will add the new functionalities you learned in this lesson to take your crew's performance to the next level.\n",
    "\n",
    "## Background\n",
    "Your first try at the automation tool for code reviewing was a success. You want to go one step further by adding memory, guardrails and execution hooks to ensure you get the best possible results out of your tool.\n",
    "\n",
    "Here is a visual summary of the structure of your crew, as well as the new elements you will be adding: \n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"./images/agents-tasks-diagram.png\" width=600>\n",
    "</div>\n",
    "\n",
    "## General instructions for grading\n",
    "- Replace all `None` instances with your own solution.\n",
    "- You can add new cells to experiment, but these will be omitted by the grader. Only use the provided cells for your solution code.\n",
    "- Before submitting, make sure all the cells in your lab work correctly.\n",
    "- **Do not change variable names**: if you modify variable names, the grader won't be able to find your solutions\n",
    "- **Use the provided configuration**: for grading, please use all provided configurations. Don't change the configuration files or settings. You can experiment after submitting your lab.\n",
    "- To submit your notebook, save it and then click on the red **Submit Assignment** button at the top right of the page.\n",
    "\n",
    "**<font color='#5DADEC'>Please make sure to save your work periodically, so you don't lose any progress.</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b79c49",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. [Set up your notebook](#1)\n",
    "2. [Agents](#2)\n",
    "3. [Guardrails](#3)\n",
    "   - [Exercise 1: Write the `security_review_output_guardrail` guardrail](#ex1)\n",
    "   - [Exercise 2: Write the `review_decision_guardrail` guardrail](#ex2)\n",
    "4. [Tasks](#4)\n",
    "   - [Exercise 3: Create the Analyze Code Quality task](#ex3)\n",
    "   - [Exercise 4: Create the Review Security task](#ex4)\n",
    "   - [Exercise 5: Create the Review Decision task](#ex5)\n",
    "5. [Execution Hooks](#5)\n",
    "   - [Exercise 6: Create a hook to read the PR file](#ex6)\n",
    "6. [Creating the Crew](#6)\n",
    "   - [Exercise 7: Define the Crew](#ex7)\n",
    "   - [Exercise 8: Kickoff the Crew](#ex8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b18134",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "\n",
    "## 1 - Set up your notebook\n",
    "\n",
    "Begin by importing all necessary modules and configure your environment variables to connect to the LLM APIs.\n",
    "\n",
    "The libraries are already installed in the classroom. If you're running this notebook on your own machine, you can install the following:\n",
    "\n",
    "`!pip install crewai[tools]=1.3.0`\n",
    "\n",
    "<a id=\"1-1\"></a>\n",
    "\n",
    "### 1.1 - Import modules\n",
    "Run the following cell to import all the modules you will need for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff89a88",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "from crewai import Agent, Task, Crew\n",
    "from crewai_tools import SerperDevTool, ScrapeWebsiteTool\n",
    "from pydantic import BaseModel\n",
    "from utils import get_openai_api_key, get_serper_api_key, clean_markdown\n",
    "from IPython.display import Markdown, display\n",
    "import yaml\n",
    "import os\n",
    "os.environ[\"CREWAI_TESTING\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bca9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittests\n",
    "import dill"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34455f57",
   "metadata": {},
   "source": [
    "<a id=\"1-2\"></a>\n",
    "\n",
    "### 1.2 - Setup the environment variables\n",
    "\n",
    "Next, set up the environment variables to connect to the APIs, and create the LLM instance you will use for your Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e94324",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# set up the OpenAI model\n",
    "os.environ[\"MODEL\"] = \"gpt-4o-mini\"\n",
    "\n",
    "# set up the OpenAI API key \n",
    "os.environ[\"OPENAI_API_KEY\"] = get_openai_api_key()\n",
    "\n",
    "# set the Serper API key for the WebsiteSearchTool\n",
    "os.environ[\"SERPER_API_KEY\"] = get_serper_api_key()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e38c694",
   "metadata": {},
   "source": [
    "<a id=\"1-3\"></a>\n",
    "\n",
    "### 1.3 - Import configuration files\n",
    "\n",
    "Since you already defined the agents and tasks in Module 1's assignment, this time you will just load the parameters from a `YAML` file. These files contain `role`, `goal` and `backstory` for Agents, and `description` and `expected_output` for Tasks. This way, you only need to set up the new parameters and functionalities.\n",
    "\n",
    "Run the next cell to define the configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d2339a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# Define file paths for YAML configurations\n",
    "files = {\n",
    "    'agents': 'config/agents.yaml',\n",
    "    'tasks': 'config/tasks.yaml'\n",
    "}\n",
    "\n",
    "# Load configurations from YAML files\n",
    "configs = {}\n",
    "for config_type, file_path in files.items():\n",
    "    with open(file_path, 'r') as file:\n",
    "        configs[config_type] = yaml.safe_load(file)\n",
    "\n",
    "# Assign loaded configurations to specific variables\n",
    "agents_config = configs['agents']\n",
    "tasks_config = configs['tasks']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f737a3",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "\n",
    "## 2 - Agents\n",
    "\n",
    "You will begin by creating your agents. This time you don't have to write any code, because you will be using exactly the same agents as in the previous assignment. The only difference is that this time the `role`, `goal` and `backstory` for each agent are given to you in the YAML configuration file. \n",
    "\n",
    "Run the cell below to create all the agent instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daade9de",
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# Create the tool instances for the security engineer agent\n",
    "serper_search_tool = SerperDevTool(search_url=\"https://owasp.org\", \n",
    "                                   base_url=os.getenv(\"DLAI_SERPER_BASE_URL\")) \n",
    "scrape_website_tool = ScrapeWebsiteTool()\n",
    "\n",
    "\n",
    "# create the Senior Developer agent\n",
    "senior_developer = Agent( \n",
    "    # load role, goal, and backstory from the YAML configuration\n",
    "    config=agents_config['senior_developer'],\n",
    "    # set verbose\n",
    "    verbose=True \n",
    ")\n",
    "\n",
    "# create the Security Engineer agent\n",
    "security_engineer = Agent( \n",
    "    # load role, goal, and backstory from the YAML configuration\n",
    "    config=agents_config['security_engineer'],\n",
    "    # add the website search tools (you need to unpack the list)\n",
    "    tools=[serper_search_tool, scrape_website_tool],\n",
    "    # set verbose \n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# create the Tech Lead agent\n",
    "tech_lead = Agent(\n",
    "    # load role, goal, and backstory from the YAML configuration\n",
    "    config=agents_config['tech_lead'],\n",
    "    # set verbose\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfa893c",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "\n",
    "## 3 - Guardrails\n",
    "You need to guarantee security reviews have proper structure and standardized risk levels to prevent vulnerabilities from being misclassified or overlooked. Guardrails provide this essential validation!\n",
    "\n",
    "You will define two guardrails:\n",
    "\n",
    "\n",
    "| Name             | Functionality      | Task          |\n",
    "|------------------|--------------------|---------------|\n",
    "| `security_review_output_guardrail`| Ensures the security risks are within the specified categories| `review_security`|\n",
    "| `review_decision_guardrail` | \tEnsures output includes an actionable decision (approve, etc.) | `make_review_decision`|\n",
    "\n",
    "Each guardrail must return a tuple with a `bool` of whether the checks passed or failed, and a message (optional). If you want to know more details about guardrails in CrewAI, please check the [docs](https://docs.crewai.com/en/concepts/tasks#task-guardrails)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2858c8",
   "metadata": {},
   "source": [
    "<a id=\"ex1\"></a>\n",
    "\n",
    "### Exercise 1: Write the `security_review_output_guardrail` guardrail\n",
    "\n",
    "Complete the cell below to define the function to create the guardrail for the security review output. This guardrail needs to validate: \n",
    "- The `risk_level` for each vulnerability is one of the three accepted categories: `low`, `medium` or `high`.\n",
    "- The `highest_risk` actually matches the highest value in `risk_level`.\n",
    "\n",
    "Some of the structure is already given to you; you only need to fill in the `None` placeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4c3400",
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED CELL: Exercise 1\n",
    "\n",
    "def security_review_output_guardrail(output):\n",
    "    \n",
    "    # get the (JSON) output from the TaskOutput object\n",
    "    try: \n",
    "        json_output = output if type(output)==dict else output.json_dict \n",
    "    except Exception as e:\n",
    "        return (False, (\"Error retrieving the `json_dict` argument: \"\n",
    "                        f\"\\n{str(e)}\\n\"\n",
    "                        \"Make sure you set the output_json parameter in the Task.\"\n",
    "                        )\n",
    "                )\n",
    "\n",
    "    # define risk levels\n",
    "    valid_risk_levels = ['low', 'medium', 'high']\n",
    "    \n",
    "    # Check if security_vulnerabilities key exists\n",
    "    if 'security_vulnerabilities' not in json_output:\n",
    "        return (False, f\"Missing 'security_vulnerabilities' key in output. Got keys: {list(json_output.keys())}\")\n",
    "    \n",
    "    # validate that each of the risk levels has a valid value\n",
    "    for vuln in json_output['security_vulnerabilities']:\n",
    "        # validate the risk level\n",
    "        if vuln['risk_level'].lower() not in valid_risk_levels: \n",
    "            error_message = f\"Invalid risk level: {vuln['risk_level']}\"\n",
    "            return (False, error_message) \n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # validate that the highest risk level matches the highest risk level in the vulnerabilities\n",
    "    \n",
    "    # if the highest risk level is not valid risk level, return an error message\n",
    "    if None[\"None\"].lower() not in None:\n",
    "        error_message = None\n",
    "        return (None, None)\n",
    "    \n",
    "    # if it is one of the valid risk levels, then check if it matches the highest \n",
    "    # risk level in the vulnerabilities\n",
    "    else:\n",
    "        # get all risk_level values\n",
    "        risk_levels = [vuln['risk_level'].lower() for vuln in json_output['security_vulnerabilities']] \n",
    "        \n",
    "        # if \"high\" in risk levels, then highest risk level should be high\n",
    "        if \"high\" in risk_levels: \n",
    "            if json_output[\"highest_risk\"].lower() != None:\n",
    "                error_message = \"Highest risk level does not match the highest risk level in the vulnerabilities.\" \n",
    "                return (None, None)\n",
    "            \n",
    "        # if high is not present and medium is in risk levels, then highest risk level should be medium\n",
    "        elif \"medium\" in risk_levels: \n",
    "            if json_output[None].lower() != None:\n",
    "                error_message = \"Highest risk level does not match the highest risk level in the vulnerabilities.\" \n",
    "                return (None, None)\n",
    "            \n",
    "        # if high and medium are not present, then lowest risk level should be low\n",
    "        elif \"low\" in risk_levels: \n",
    "            if json_output[None].lower() != None:\n",
    "                error_message = \"Highest risk level does not match the highest risk level in the vulnerabilities.\" \n",
    "                return (None, None)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return (True, output.json_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7603d333",
   "metadata": {},
   "source": [
    "Try the guardrail with an invalid JSON dictionary, where the highest risk is not correctly identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5992a72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the output json for testing\n",
    "invalid_json = {\"highest_risk\": \"medium\", \n",
    "                        \"security_vulnerabilities\": [{\"risk_level\": \"high\"}, \n",
    "                                                     {\"risk_level\": \"medium\"}]}\n",
    "# test the guardrail\n",
    "security_review_output_guardrail(invalid_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e3c4b3",
   "metadata": {},
   "source": [
    "##### **Expected output**:\n",
    "```\n",
    "(False,\n",
    " 'Highest risk level does not match the highest risk level in the vulnerabilities.')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800fa309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the guardrail\n",
    "unittests.test_security_review_output_guardrail(security_review_output_guardrail)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64eadeaa",
   "metadata": {},
   "source": [
    "<a id=\"ex2\"></a>\n",
    "\n",
    "### Exercise 2: Write the `review_decision_guardrail` guardrail\n",
    "\n",
    "Complete the cell below to define the function to create the guardrail for the review decision output. This guardrail needs to make sure the output includes one of the required decision values: \"approve\", \"request changes\" or \"escalate\".\n",
    "\n",
    "Some of the structure is already given to you; you only need to fill in the `None` placeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbaf9ff",
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED CELL: Exercise 2\n",
    "\n",
    "def review_decision_guardrail(output):\n",
    "    # get the raw output from the TaskOutput object\n",
    "    try:\n",
    "        output = output if type(output)==str else output.raw\n",
    "    except Exception as e:\n",
    "        return (False, (\"Error retrieving the `raw` argument: \"\n",
    "                        f\"\\n{str(e)}\\n\"\n",
    "                        \"Make sure you set the raw parameter in the Task.\"\n",
    "                        )\n",
    "                )\n",
    "\n",
    "    # define the keywords to check for in the output\n",
    "    keywords = [\"approve\", \"request changes\", \"escalate\"]\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # check if any of the keywords are present in the output\n",
    "    if not any(None in None.lower() for keyword in keywords):\n",
    "        # write a suitable message to the console\n",
    "        error_message = None\n",
    "        return (None, None)\n",
    "\n",
    "    # if all checks pass, return True, and the output\n",
    "    return (None, None)\n",
    "\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfee8331",
   "metadata": {},
   "source": [
    "Try the guardrail with an output missing the possible decision values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed02faa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the raw output for testing\n",
    "input = 'Final decision: Elevate to human'\n",
    "\n",
    "# test the guardrail\n",
    "review_decision_guardrail(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7468ff25",
   "metadata": {},
   "source": [
    "##### **Expected output**:\n",
    "In this case, the message will vary depending on what you chose\n",
    "```\n",
    "(False, 'Output does not include one of the valid actionable decisions.')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2142222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the guardrail\n",
    "unittests.test_review_decision_guardrail(review_decision_guardrail)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc543628",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "\n",
    "## 4 - Tasks\n",
    "Now that you have defined the guardrails and execution hooks, you are ready to define the tasks. You will create the three original tasks. For each one, you will load the `description`, `expected_output` and `name` from the yaml file, but you will need to add the guardrails and output types. The difference file (PR) contents will be set in the `file_content` key of the inputs to the crew. \n",
    "\n",
    "<a id=\"ex3\"></a>\n",
    "\n",
    "### Exercise 3: Create the Analyze Code Quality task\n",
    "Start by defining the Analyze Code Quality task. The structure of the JSON output is defined for you. It includes the following keys:\n",
    "* `critical_issues`: a list of issues that need fixing\n",
    "* `minor_issues`: a list of suggested improvements\n",
    "* `reasoning`: text with the explanation of the reasoning\n",
    "\n",
    "You should:\n",
    "1. Define the Task:\n",
    "    * Use the `config` parameter to load the task configuration from the YAML file\n",
    "    * Set the `output_json` parameter with the corresponding structure\n",
    "    * Assign to the corresponding `agent`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caff9e6",
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED CELL: Exercise 3\n",
    "\n",
    "# Define the pydantic model for the code quality analysis output\n",
    "class CodeQualityJSON(BaseModel):\n",
    "    critical_issues: list[str]\n",
    "    minor_issues: list[str]\n",
    "    reasoning: str\n",
    "\n",
    "### START CODE HERE ###\n",
    "\n",
    "# Create the quality analysis task\n",
    "analyze_code_quality = None(\n",
    "    # Load the expected output, and name from the YAML configuration\n",
    "    config=tasks_config['analyze_code_quality'], \n",
    "    # Define the output type as a pydantic model\n",
    "    output_json=None,\n",
    "    # Define the agent that will perform this task\n",
    "    agent=senior_developer, \n",
    ")\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f50c0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the analyze_code_quality task\n",
    "unittests.test_analyze_code_quality(analyze_code_quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fc340e",
   "metadata": {},
   "source": [
    "<a id=\"ex4\"></a>\n",
    "\n",
    "### Exercise 4: Create the Review Security task\n",
    "\n",
    "1. Define the JSON structure of the Security vulnerabilities. You need the following keys:\n",
    "    * `description`: string with the description\n",
    "    * `risk_level`: a string indicating the level\n",
    "    * `evidence`: a string showing the evidence for the risk\n",
    "\n",
    "2. Define the JSON structure for the output. You need the following keys:\n",
    "    * `security_vulnerabilities`: a list of Security vulnerabilities (structure defined before)\n",
    "    * `blocking`: a boolean indicating if security issues should block approval\n",
    "    * `highest_risk`: a string with the most severe risk level found\n",
    "    * `security_recommendations`: a list of strings with specific fixes for identified vulnerabilities\n",
    "\n",
    "2. Define the Task:\n",
    "    * Use the `config` parameter to load the task configuration from the YAML file\n",
    "    * Set the `output_json` parameter\n",
    "    * Add the guardrail\n",
    "    * Assign to the corresponding agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3ac89e",
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED CELL: Exercise 4\n",
    "\n",
    "### START CODE HERE ###\n",
    "\n",
    "# Define the pydantic model for the security vulnerabilities\n",
    "class SecurityVulnerability(BaseModel):\n",
    "    None\n",
    "\n",
    "# Define the pydantic model for the security review output\n",
    "class ReviewSecurityJSON(BaseModel):\n",
    "    security_vulnerabilities: list[SecurityVulnerability] \n",
    "    None\n",
    "\n",
    "# Create the security review task\n",
    "review_security = None(\n",
    "    # Load the expected output, agent, and name from the YAML configuration\n",
    "    config=tasks_config['review_security'], \n",
    "    # Define the output type as a pydantic model\n",
    "    output_json=None,\n",
    "    # Add the security_review_output_guardrail guardrail\n",
    "    guardrails=[None],\n",
    "    # Define the agent that will perform this task\n",
    "    agent=security_engineer \n",
    ")\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab85d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the review_security task\n",
    "unittests.test_review_security(review_security, SecurityVulnerability, ReviewSecurityJSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fa1ed0",
   "metadata": {},
   "source": [
    "<a id=\"ex5\"></a>\n",
    "\n",
    "### Exercise 5: Create the Review Decision task\n",
    "\n",
    "Next, define the Review Decision task. For this task, you will need to set the corresponding `guardrail`. \n",
    "\n",
    "1. Define the Task:\n",
    "    * Use the `config` parameter to load the task configuration from the YAML file\n",
    "    * Set the `Markdown` parameter, to get the final report in Markdown format\n",
    "    * Add the guardrail\n",
    "    * Assign to the corresponding agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d3cb40",
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED CELL: Exercise 5\n",
    "\n",
    "### START CODE HERE ###\n",
    "\n",
    "# Create the review decision task\n",
    "make_review_decision = None(\n",
    "    # Load the expected output, agent, and name from the YAML configuration\n",
    "    config=tasks_config['make_review_decision'], \n",
    "    # Set the Markdown parameter to get the final report in Markdown format\n",
    "    markdown=None,\n",
    "    # Add the review_decision_guardrail guardrail\n",
    "    guardrails=[None],\n",
    "    # Set the context with the previous task objects\n",
    "    context=[analyze_code_quality, review_security], \n",
    "    # Define the agent that will perform this task\n",
    "    agent=tech_lead \n",
    ")\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099b4c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the make_review_decision task\n",
    "unittests.test_make_review_decision(make_review_decision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2413a44c",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "\n",
    "## 5 - Execution Hooks\n",
    "\n",
    "All your tasks require access to the code difference (PR file), but reading the file isn't really an \"agentic\" task that requires intelligent decision-making. Instead of having your agents read the file directly, you can create a **before-kickoff hook** that handles this automatically. This hook will read the file and add its content to the crew's inputs before the agents begin their work.\n",
    "\n",
    "<a id=\"ex6\"></a>\n",
    "\n",
    "### Exercise 6: Create a hook to read the PR file\n",
    "Write a function to be used as a hook. This function should:\n",
    "1. Receive the file path from the `file_path` parameter in the crew's inputs\n",
    "2. Read the contents of the PR file\n",
    "3. Add a new key to the `input` dictionary called `file_content` containing the loaded file contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dc5b29",
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED CELL: Exercise 6\n",
    "\n",
    "# Define the execution hook to read the PR file\n",
    "def read_file_hook(inputs):\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # get the file_path from the inputs\n",
    "    filename = None.get(None)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # if the filename is not provided, raise an error\n",
    "    if not filename:\n",
    "        raise ValueError(\"Missing 'file_path' in inputs\")\n",
    "\n",
    "    # try reading the file\n",
    "    try:\n",
    "        with open(filename, \"r\") as f:\n",
    "            file_contents = f.read()\n",
    "    # if there are any issues, raise an error\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to read file {filename}: {e}\")\n",
    "\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # add the file contents to the inputs\n",
    "    inputs[\"file_content\"] = None\n",
    "    \n",
    "    # return the modified inputs\n",
    "    return None\n",
    "\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437b42c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the read_file_hook\n",
    "unittests.test_read_file_hook(read_file_hook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a155cf4d",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "\n",
    "## 6 - Creating the Crew\n",
    "\n",
    "Now that all the elements are in place, you can define the Crew and kick it off to get the assessment for your PR.\n",
    "\n",
    "<a id=\"ex7\"></a>\n",
    "\n",
    "### Exercise 7: Define the Crew\n",
    "In this step, you will define the crew. You need to set up agents and tasks, just like in the previous module, but this time you will also need to add memory. Adding memory enables agents to remember previously identified security vulnerabilities and coding patterns across multiple pull requests, improving consistency and allowing them to recognize recurring issues without starting from scratch each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cd9842",
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED CELL: Exercise 7\n",
    "\n",
    "### START CODE HERE ###\n",
    "\n",
    "# Create the code review crew\n",
    "crew = None(\n",
    "    # add the list of agents\n",
    "    agents=[None],\n",
    "    # add the list of tasks (in order of execution) \n",
    "    tasks=[None],\n",
    "    # add memory to the crew\n",
    "    memory=None,\n",
    "    # add the before-kickoff hook to read the PR file\n",
    "    # you need to pass a list with the hook function from Ex 8\n",
    "    before_kickoff_callbacks= [None]\n",
    ")\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5d8097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the crew\n",
    "unittests.test_crew(crew)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e4ca3f",
   "metadata": {},
   "source": [
    "<a id=\"ex8\"></a>\n",
    "\n",
    "### Exercise 8: Kickoff the crew\n",
    "\n",
    "If all tests from the previous exercises are passed, you are ready to kickoff the crew and save the results in a `dill` file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3761109",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# GRADED CELL: Exercise 8\n",
    "\n",
    "# define the file path for the PR file\n",
    "file_path = \"files/code_changes.txt\" \n",
    "\n",
    "### START CODE HERE ###\n",
    "\n",
    "# kickoff the crew\n",
    "result = None.None(\n",
    "    # add the inputs dictionary with the file path\n",
    "    inputs={None: None}\n",
    "    )\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Save the result to a dill file\n",
    "with open('result.dill', 'wb') as f:\n",
    "    dill.dump(result, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8946610a",
   "metadata": {},
   "source": [
    "Let's check out the final report!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bd44ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the markdown output, in case the report is wrapped with code fences\n",
    "clean_report = clean_markdown(result.raw)\n",
    "\n",
    "# display the final report as markdown\n",
    "display(Markdown(clean_report))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2303b4",
   "metadata": {},
   "source": [
    "You reached the end of the assignment. At this point you are ready to submit for grading.\n",
    "\n",
    "After submitting and being satisfied with your grade you can take some time to experiment changing the guardrails, or adding new URLs to the Website Reading Tool, or even upload a different pull request file with code differences. Don't be afraid to shake things up!"
   ]
  }
 ],
 "metadata": {
  "grader_version": "1",
  "kernelspec": {
   "display_name": "agents-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
