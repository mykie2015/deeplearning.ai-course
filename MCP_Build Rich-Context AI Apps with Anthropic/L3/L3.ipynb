{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c10b768-f5b4-43bc-a785-99077422ce78",
   "metadata": {},
   "source": [
    "# Lesson 3: Chatbot Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d4fedc-4b90-4754-9f2d-fd3cfa321a14",
   "metadata": {},
   "source": [
    "In this lesson, you will familiarize yourself with the chatbot example you will work on during this course. The example includes the tool definitions and execution, as well as the chatbot code. Make sure to interact with the chatbot at the end of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ed96ba-5ade-4af4-9096-406ce48d5cf2",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd6bd1d4-f652-45d1-9efa-155a2cc01713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import json\n",
    "import os\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20f163a-87af-4e0c-87ed-1624c150c572",
   "metadata": {},
   "source": [
    "## Tool Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "549a7f46-74b3-4a1d-b084-055c99e3c318",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAPER_DIR = \"papers\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e43905e-56f3-404c-a322-f038055e9b1c",
   "metadata": {},
   "source": [
    "The first tool searches for relevant arXiv papers based on a topic and stores the papers' info in a JSON file (title, authors, summary, paper url and the publication date). The JSON files are organized by topics in the `papers` directory. The tool does not download the papers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "886633b8-ce67-4343-822d-cc3f98f953fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_papers(topic: str, max_results: int = 5) -> List[str]:\n",
    "    \"\"\"\n",
    "    Search for papers on arXiv based on a topic and store their information.\n",
    "    \n",
    "    Args:\n",
    "        topic: The topic to search for\n",
    "        max_results: Maximum number of results to retrieve (default: 5)\n",
    "        \n",
    "    Returns:\n",
    "        List of paper IDs found in the search\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use arxiv to find the papers \n",
    "    client = arxiv.Client()\n",
    "\n",
    "    # Search for the most relevant articles matching the queried topic\n",
    "    search = arxiv.Search(\n",
    "        query = topic,\n",
    "        max_results = max_results,\n",
    "        sort_by = arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "\n",
    "    papers = client.results(search)\n",
    "    \n",
    "    # Create directory for this topic\n",
    "    path = os.path.join(PAPER_DIR, topic.lower().replace(\" \", \"_\"))\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    file_path = os.path.join(path, \"papers_info.json\")\n",
    "\n",
    "    # Try to load existing papers info\n",
    "    try:\n",
    "        with open(file_path, \"r\") as json_file:\n",
    "            papers_info = json.load(json_file)\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        papers_info = {}\n",
    "\n",
    "    # Process each paper and add to papers_info  \n",
    "    paper_ids = []\n",
    "    for paper in papers:\n",
    "        paper_ids.append(paper.get_short_id())\n",
    "        paper_info = {\n",
    "            'title': paper.title,\n",
    "            'authors': [author.name for author in paper.authors],\n",
    "            'summary': paper.summary,\n",
    "            'pdf_url': paper.pdf_url,\n",
    "            'published': str(paper.published.date())\n",
    "        }\n",
    "        papers_info[paper.get_short_id()] = paper_info\n",
    "    \n",
    "    # Save updated papers_info to json file\n",
    "    with open(file_path, \"w\") as json_file:\n",
    "        json.dump(papers_info, json_file, indent=2)\n",
    "    \n",
    "    print(f\"Results are saved in: {file_path}\")\n",
    "    \n",
    "    return paper_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d20ee17a-afe6-438a-95b1-6e87742c7fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results are saved in: papers/model_context_protocol_in_llm_dev/papers_info.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['2501.12521v1',\n",
       " '2402.10671v3',\n",
       " '2310.17342v1',\n",
       " '2505.01834v1',\n",
       " '2311.09216v1']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_papers(\"model context protocol in llm dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb83565-69af-47f3-9ba3-a96965cff7df",
   "metadata": {},
   "source": [
    "The second tool looks for information about a specific paper across all topic directories inside the `papers` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df9b1997-81cd-447d-9665-1cb72d93bb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(paper_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Search for information about a specific paper across all topic directories.\n",
    "    \n",
    "    Args:\n",
    "        paper_id: The ID of the paper to look for\n",
    "        \n",
    "    Returns:\n",
    "        JSON string with paper information if found, error message if not found\n",
    "    \"\"\"\n",
    " \n",
    "    for item in os.listdir(PAPER_DIR):\n",
    "        item_path = os.path.join(PAPER_DIR, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            file_path = os.path.join(item_path, \"papers_info.json\")\n",
    "            if os.path.isfile(file_path):\n",
    "                try:\n",
    "                    with open(file_path, \"r\") as json_file:\n",
    "                        papers_info = json.load(json_file)\n",
    "                        if paper_id in papers_info:\n",
    "                            return json.dumps(papers_info[paper_id], indent=2)\n",
    "                except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "                    print(f\"Error reading {file_path}: {str(e)}\")\n",
    "                    continue\n",
    "    \n",
    "    return f\"There's no saved information related to paper {paper_id}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ebe0de7-8f07-4e08-a670-7b371fc3d2d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"title\": \"An Empirically-grounded tool for Automatic Prompt Linting and Repair: A Case Study on Bias, Vulnerability, and Optimization in Developer Prompts\",\\n  \"authors\": [\\n    \"Dhia Elhaq Rzig\",\\n    \"Dhruba Jyoti Paul\",\\n    \"Kaiser Pister\",\\n    \"Jordan Henkel\",\\n    \"Foyzul Hassan\"\\n  ],\\n  \"summary\": \"The tidal wave of advancements in Large Language Models (LLMs) has led to\\\\ntheir swift integration into application-level logic. Many software systems now\\\\nuse prompts to interact with these black-box models, combining natural language\\\\nwith dynamic values interpolated at runtime, to perform tasks ranging from\\\\nsentiment analysis to question answering. Due to the programmatic and\\\\nstructured natural language aspects of these prompts, we refer to them as\\\\nDeveloper Prompts. Unlike traditional software artifacts, Dev Prompts blend\\\\nnatural language instructions with artificial languages such as programming and\\\\nmarkup languages, thus requiring specialized tools for analysis, distinct from\\\\nclassical software evaluation methods.\\\\n  In response to this need, we introduce PromptDoctor, a tool explicitly\\\\ndesigned to detect and correct issues of Dev Prompts. PromptDoctor identifies\\\\nand addresses problems related to bias, vulnerability, and sub-optimal\\\\nperformance in Dev Prompts, helping mitigate their possible harms. In our\\\\nanalysis of 2,173 Dev Prompts, selected as a representative sample of 40,573\\\\nDev Prompts, we found that 3.46% contained one or more forms of bias, 10.75%\\\\nwere vulnerable to prompt injection attacks. Additionally, 3,310 were amenable\\\\nto automated prompt optimization. To address these issues, we applied\\\\nPromptDoctor to the flawed Dev Prompts we discovered. PromptDoctor de-biased\\\\n68.29% of the biased Dev Prompts, hardened 41.81% of the vulnerable Dev\\\\nPrompts, and improved the performance of 37.1% sub-optimal Dev Prompts.\\\\nFinally, we developed a PromptDoctor VSCode extension, enabling developers to\\\\neasily enhance Dev Prompts in their existing development workflows. The data\\\\nand source code for this work are available at\",\\n  \"pdf_url\": \"http://arxiv.org/pdf/2501.12521v1\",\\n  \"published\": \"2025-01-21\"\\n}'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_info('2501.12521v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ea3013-e690-4bc8-8622-27b4d42d61e4",
   "metadata": {},
   "source": [
    "## Tool Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7d2260-452d-472a-b56e-326479cb18c9",
   "metadata": {},
   "source": [
    "Here are the schema of each tool which you will provide to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5bdea5f-e93a-4018-8c13-00d5ee10c0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"name\": \"search_papers\",\n",
    "        \"description\": \"Search for papers on arXiv based on a topic and store their information.\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"topic\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The topic to search for\"\n",
    "                }, \n",
    "                \"max_results\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"Maximum number of results to retrieve\",\n",
    "                    \"default\": 5\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"topic\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"extract_info\",\n",
    "        \"description\": \"Search for information about a specific paper across all topic directories.\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"paper_id\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The ID of the paper to look for\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"paper_id\"]\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec668d24-1559-41b7-bc8a-e2dca77dfaf2",
   "metadata": {},
   "source": [
    "## Tool Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c728c1ec-36b1-48b4-9f85-622464ac79f4",
   "metadata": {},
   "source": [
    "This code handles tool mapping and execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c90790c0-efc4-4068-9c00-d2592d80bc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_tool_function = {\n",
    "    \"search_papers\": search_papers,\n",
    "    \"extract_info\": extract_info\n",
    "}\n",
    "\n",
    "def execute_tool(tool_name, tool_args):\n",
    "    \n",
    "    result = mapping_tool_function[tool_name](**tool_args)\n",
    "\n",
    "    if result is None:\n",
    "        result = \"The operation completed but didn't return any results.\"\n",
    "        \n",
    "    elif isinstance(result, list):\n",
    "        result = ', '.join(result)\n",
    "        \n",
    "    elif isinstance(result, dict):\n",
    "        # Convert dictionaries to formatted JSON strings\n",
    "        result = json.dumps(result, indent=2)\n",
    "    \n",
    "    else:\n",
    "        # For any other type, convert using str()\n",
    "        result = str(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8fc4d3-58ac-482c-8bbd-bccd6ef9fc31",
   "metadata": {},
   "source": [
    "## Chatbot Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ba0fad-b0e4-4415-a431-341e9ca85087",
   "metadata": {},
   "source": [
    "The chatbot handles the user's queries one by one, but it does not persist memory across the queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe662400-8506-464e-a3da-75a3d8848bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv() \n",
    "client = openai.OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "    base_url=os.environ.get(\"OPENAI_API_BASE\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175586b4-acdf-4103-8039-134478a4f797",
   "metadata": {},
   "source": [
    "### Query Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12a896e0-3f56-417e-aa51-c61756048593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(query):\n",
    "    \n",
    "    messages = [{'role': 'user', 'content': query}]\n",
    "    \n",
    "    # Convert tools to OpenAI format\n",
    "    openai_tools = []\n",
    "    for tool in tools:\n",
    "        openai_tools.append({\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": tool[\"name\"],\n",
    "                \"description\": tool[\"description\"],\n",
    "                \"parameters\": tool[\"input_schema\"]\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        max_completion_tokens=2024,\n",
    "        model=os.environ.get(\"DEFAULT_LLM_MODEL\", \"o4-mini\"),\n",
    "        tools=openai_tools,\n",
    "        messages=messages\n",
    "    )\n",
    "    \n",
    "    process_query = True\n",
    "    while process_query:\n",
    "        message = response.choices[0].message\n",
    "        \n",
    "        if message.content:\n",
    "            print(message.content)\n",
    "            \n",
    "        if message.tool_calls:\n",
    "            # Add assistant message with tool calls\n",
    "            messages.append({\n",
    "                \"role\": \"assistant\", \n",
    "                \"content\": message.content,\n",
    "                \"tool_calls\": message.tool_calls\n",
    "            })\n",
    "            \n",
    "            # Process each tool call\n",
    "            for tool_call in message.tool_calls:\n",
    "                tool_name = tool_call.function.name\n",
    "                tool_args = json.loads(tool_call.function.arguments)\n",
    "                print(f\"Calling tool {tool_name} with args {tool_args}\")\n",
    "                \n",
    "                result = execute_tool(tool_name, tool_args)\n",
    "                \n",
    "                # Add tool result message\n",
    "                messages.append({\n",
    "                    \"role\": \"tool\",\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"content\": result\n",
    "                })\n",
    "            \n",
    "            # Get next response\n",
    "            response = client.chat.completions.create(\n",
    "                max_completion_tokens=2024,\n",
    "                model=os.environ.get(\"DEFAULT_LLM_MODEL\", \"o4-mini\"),\n",
    "                tools=openai_tools,\n",
    "                messages=messages\n",
    "            )\n",
    "        else:\n",
    "            process_query = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2921ee7f-d2be-464b-ab7b-8db2a3c13ba9",
   "metadata": {},
   "source": [
    "### Chat Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16979cdc-81e9-432b-ba7f-e810b52961e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_loop():\n",
    "    print(\"Type your queries or 'quit' to exit.\")\n",
    "    while True:\n",
    "        try:\n",
    "            query = input(\"\\nQuery: \").strip()\n",
    "            if query.lower() == 'quit':\n",
    "                break\n",
    "    \n",
    "            process_query(query)\n",
    "            print(\"\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfaf254-f22a-4951-885e-1d21fbc41ff3",
   "metadata": {},
   "source": [
    "Feel free to interact with the chatbot. Here's an example query: \n",
    "\n",
    "- Search for 2 papers on \"LLM interpretability\"\n",
    "\n",
    "To access the `papers` folder: 1) click on the `File` option on the top menu of the notebook and 2) click on `Open` and then 3) click on `L3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39676f70-1c72-4da3-8363-da281bd5a83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type your queries or 'quit' to exit.\n",
      "Could you clarify what you mean by “MCP servers”? For example, are you referring to:\n",
      "\n",
      "  • Minecraft Coder Pack (MCP)–based game servers  \n",
      "  • A “multi-cloud” or “managed cloud platform” (MCP) server rollout  \n",
      "  • A microservice control-plane (MCP) in a service-mesh or custom RPC framework  \n",
      "  • Something else entirely  \n",
      "\n",
      "Once I know which “MCP” you’re working with, I can give you targeted best practices for development, deployment, scaling, monitoring, and security.\n",
      "\n",
      "\n",
      "Could you clarify what you mean by “model context protocol”? For example:\n",
      "\n",
      "  • Are you asking about how to structure prompt context and system/user messages for a language model?  \n",
      "  • Are you referring to an API or communication protocol for passing context to a deployed ML model?  \n",
      "  • Or something else (e.g. context windows, state management, data schemas)?\n",
      "\n",
      "A bit more detail will help me give you the right explanation.\n",
      "\n",
      "\n",
      "Here’s a consolidated set of best practices for designing, developing, and maintaining “model-context protocol” services—that is, systems whose primary job is to assemble, manage, and serve context to large-language or other AI models over time.\n",
      "\n",
      "1. Define Clear Abstractions  \n",
      "   • Context Unit  \n",
      "     – Decide what a “chunk” of context is: a document, a Q&A pair, a turn in a conversation, a structured feature.  \n",
      "   • Context Protocol/API  \n",
      "     – Define simple, versioned, language-agnostic RPC/REST/gRPC methods such as  \n",
      "        • initializeSession(sessionId, initialContext)  \n",
      "        • appendContext(sessionId, contextEntry)  \n",
      "        • queryContext(sessionId, query)  \n",
      "        • updateContext(sessionId, selector, patch)  \n",
      "        • closeSession(sessionId)  \n",
      "   • Separation of Concerns  \n",
      "     – Keep your context store, retrieval logic, and model-invocation code in separate modules/services.\n",
      "\n",
      "2. Design for Modularity & Versioning  \n",
      "   • Version Your Protocol  \n",
      "     – Use semantic versioning (v1.0.0, v1.1.0, v2.0.0) for context-API changes.  \n",
      "   • Pluginable Retrieval Strategies  \n",
      "     – Support multiple retrieval or featurization plugins (dense vector, sparse index, hybrid).  \n",
      "   • Adapter Pattern  \n",
      "     – If you swap out LLM providers, write thin adapters so your higher-level protocol doesn’t change.\n",
      "\n",
      "3. Context Management and Scaling  \n",
      "   • Batching & Rate-Limiting  \n",
      "     – Batch multiple “appends” or “reads” when under heavy load.  \n",
      "   • Sliding Window + Summarization  \n",
      "     – For long sessions, periodically summarise older context and evict raw tokens to stay within token limits.  \n",
      "   • Cache Hot Context  \n",
      "     – Keep recently-used contexts in a fast in-memory cache (Redis, Memcached).  \n",
      "   • Sharding  \n",
      "     – Partition sessions by user-ID hash or tenant to scale horizontally.\n",
      "\n",
      "4. Storage and Retrieval  \n",
      "   • Hybrid Indexing  \n",
      "     – Combine a vector-DB for semantic search with a keyword index or RDBMS for metadata searches.  \n",
      "   • Efficient Updates  \n",
      "     – Use append-only logs (Kafka, event sourcing) or CRDTs if multiple writers collaborate on a session.  \n",
      "   • Schema Evolution  \n",
      "     – If you store context entries in a document store, use schema-version metadata to support rolling upgrades.\n",
      "\n",
      "5. Robustness & Observability  \n",
      "   • Health Checks & Circuit Breakers  \n",
      "     – Expose /health, /ready endpoints; auto-tripping circuit breakers when downstream model-calls or DB calls fail.  \n",
      "   • Metrics & Logging  \n",
      "     – Track per-session token usage, latencies (context retrieval, model response), cache hit/miss rates, error rates.  \n",
      "   • Auditing & Traceability  \n",
      "     – Log each context change with a user-ID, timestamp and change reason; keep an append-only audit trail.\n",
      "\n",
      "6. Testing & Quality  \n",
      "   • Unit & Integration Tests  \n",
      "     – Mock your storage layer and your model-inference layer; test edge-cases (empty context, very large queries, concurrent writes).  \n",
      "   • Fuzz / Chaos Testing  \n",
      "     – Inject failures in your DB, your vector-store, your model-API to ensure graceful degradation.  \n",
      "   • Performance Benchmarks  \n",
      "     – Continuously benchmark & profile tail latencies and throughput for context-fetch + model-call chains.\n",
      "\n",
      "7. Security & Governance  \n",
      "   • Authentication & Authorization  \n",
      "     – Enforce per-tenant or per-user ACLs on who can read/write each session’s context.  \n",
      "   • Data Encryption  \n",
      "     – Encrypt PII both at rest and in transit, especially if your context store holds sensitive user data.  \n",
      "   • Retention & Deletion  \n",
      "     – Implement a data-retention policy; provide GDPR-style “right to be forgotten” deletion endpoints.\n",
      "\n",
      "8. Lifecycle & Maintenance  \n",
      "   • Backfills & Migrations  \n",
      "     – Define robust migration scripts when you upgrade schema or switch storage backends.  \n",
      "   • Monitoring & Alerting  \n",
      "     – Set SLOs for  p99 latency, error budgets; alert on quota surges, out-of-memory, unexpected growth in context size.  \n",
      "   • Documentation & On-Call Playbooks  \n",
      "     – Maintain up-to-date API docs, sequence diagrams of context flows, and runbooks for common incidents.\n",
      "\n",
      "9. UX & Feedback Loops  \n",
      "   • Explainability Hooks  \n",
      "     – Allow clients to request provenance for each bit of context the model used (e.g. “Why did you use this passage?”).  \n",
      "   • Adaptive Policies  \n",
      "     – Use ML or heuristic triggers (low confidence, topic drift) to ask users to clarify or to re-summarize context.\n",
      "\n",
      "10. Continuous Improvement  \n",
      "   • Usage Analytics  \n",
      "     – Track which context-assembling strategies yield best model performance (accuracy, hallucination rate).  \n",
      "   • A/B Experimentation  \n",
      "     – Try different summarization intervals, chunk sizes, retrieval thresholds, and measure downstream metrics.  \n",
      "\n",
      "By treating your context-management layer as a first-class, versioned, observable micro-service—with clear abstractions, plugin points, rigorous testing, and strong operational guardrails—you’ll be able to iterate quickly, scale safely, and maintain high-quality interactions with your underlying LLM or other AI models.\n",
      "\n",
      "\n",
      "Could you clarify what you’d like me to find? For example, are you looking for papers on a particular topic (and how many results), or details about a specific paper by its ID?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34df7890-4b4c-4ec9-b06f-abc8c4a290e8",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\"> 🚨\n",
    "&nbsp; <b>Different Run Results:</b> The output generated by AI chat models can vary with each execution due to their dynamic, probabilistic nature. Don't be surprised if your results differ from those shown in the video.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb34ee2d",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "<p> 💻 &nbsp; <b> To Access the <code>requirements.txt</code> file or the <code>papers</code> folder: </b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Open\"</em> and finally 3) click on <em>\"L3\"</em>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508916f3-8fa1-4e21-bfa7-081a810bc36c",
   "metadata": {},
   "source": [
    "In the next lessons, you will take out the tool definitions to wrap them in an MCP server. Then you will create an MCP client inside the chatbot to make the chatbot MCP compatible.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02d207b-e07d-49ff-bb03-7954aa86c167",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "[Guide on how to implement tool use](https://docs.anthropic.com/en/docs/build-with-claude/tool-use/overview#how-to-implement-tool-use)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e5135e-01c3-4632-9f83-a1e6dd811049",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "\n",
    "\n",
    "<p> ⬇ &nbsp; <b>Download Notebooks:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Download as\"</em> and select <em>\"Notebook (.ipynb)\"</em>.</p>\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
