{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c10b768-f5b4-43bc-a785-99077422ce78",
   "metadata": {},
   "source": [
    "# Lesson 3: Chatbot Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d4fedc-4b90-4754-9f2d-fd3cfa321a14",
   "metadata": {},
   "source": [
    "In this lesson, you will familiarize yourself with the chatbot example you will work on during this course. The example includes the tool definitions and execution, as well as the chatbot code. Make sure to interact with the chatbot at the end of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ed96ba-5ade-4af4-9096-406ce48d5cf2",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd6bd1d4-f652-45d1-9efa-155a2cc01713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import json\n",
    "import os\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20f163a-87af-4e0c-87ed-1624c150c572",
   "metadata": {},
   "source": [
    "## Tool Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "549a7f46-74b3-4a1d-b084-055c99e3c318",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAPER_DIR = \"papers\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e43905e-56f3-404c-a322-f038055e9b1c",
   "metadata": {},
   "source": [
    "The first tool searches for relevant arXiv papers based on a topic and stores the papers' info in a JSON file (title, authors, summary, paper url and the publication date). The JSON files are organized by topics in the `papers` directory. The tool does not download the papers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "886633b8-ce67-4343-822d-cc3f98f953fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_papers(topic: str, max_results: int = 5) -> List[str]:\n",
    "    \"\"\"\n",
    "    Search for papers on arXiv based on a topic and store their information.\n",
    "    \n",
    "    Args:\n",
    "        topic: The topic to search for\n",
    "        max_results: Maximum number of results to retrieve (default: 5)\n",
    "        \n",
    "    Returns:\n",
    "        List of paper IDs found in the search\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use arxiv to find the papers \n",
    "    client = arxiv.Client()\n",
    "\n",
    "    # Search for the most relevant articles matching the queried topic\n",
    "    search = arxiv.Search(\n",
    "        query = topic,\n",
    "        max_results = max_results,\n",
    "        sort_by = arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "\n",
    "    papers = client.results(search)\n",
    "    \n",
    "    # Create directory for this topic\n",
    "    path = os.path.join(PAPER_DIR, topic.lower().replace(\" \", \"_\"))\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    file_path = os.path.join(path, \"papers_info.json\")\n",
    "\n",
    "    # Try to load existing papers info\n",
    "    try:\n",
    "        with open(file_path, \"r\") as json_file:\n",
    "            papers_info = json.load(json_file)\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        papers_info = {}\n",
    "\n",
    "    # Process each paper and add to papers_info  \n",
    "    paper_ids = []\n",
    "    for paper in papers:\n",
    "        paper_ids.append(paper.get_short_id())\n",
    "        paper_info = {\n",
    "            'title': paper.title,\n",
    "            'authors': [author.name for author in paper.authors],\n",
    "            'summary': paper.summary,\n",
    "            'pdf_url': paper.pdf_url,\n",
    "            'published': str(paper.published.date())\n",
    "        }\n",
    "        papers_info[paper.get_short_id()] = paper_info\n",
    "    \n",
    "    # Save updated papers_info to json file\n",
    "    with open(file_path, \"w\") as json_file:\n",
    "        json.dump(papers_info, json_file, indent=2)\n",
    "    \n",
    "    print(f\"Results are saved in: {file_path}\")\n",
    "    \n",
    "    return paper_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d20ee17a-afe6-438a-95b1-6e87742c7fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results are saved in: papers/model_context_protocol_in_llm_dev/papers_info.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['2501.12521v1',\n",
       " '2402.10671v3',\n",
       " '2310.17342v1',\n",
       " '2505.01834v1',\n",
       " '2212.10378v2']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_papers(\"model context protocol in llm dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb83565-69af-47f3-9ba3-a96965cff7df",
   "metadata": {},
   "source": [
    "The second tool looks for information about a specific paper across all topic directories inside the `papers` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df9b1997-81cd-447d-9665-1cb72d93bb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(paper_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Search for information about a specific paper across all topic directories.\n",
    "    \n",
    "    Args:\n",
    "        paper_id: The ID of the paper to look for\n",
    "        \n",
    "    Returns:\n",
    "        JSON string with paper information if found, error message if not found\n",
    "    \"\"\"\n",
    " \n",
    "    for item in os.listdir(PAPER_DIR):\n",
    "        item_path = os.path.join(PAPER_DIR, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            file_path = os.path.join(item_path, \"papers_info.json\")\n",
    "            if os.path.isfile(file_path):\n",
    "                try:\n",
    "                    with open(file_path, \"r\") as json_file:\n",
    "                        papers_info = json.load(json_file)\n",
    "                        if paper_id in papers_info:\n",
    "                            return json.dumps(papers_info[paper_id], indent=2)\n",
    "                except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "                    print(f\"Error reading {file_path}: {str(e)}\")\n",
    "                    continue\n",
    "    \n",
    "    return f\"There's no saved information related to paper {paper_id}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ebe0de7-8f07-4e08-a670-7b371fc3d2d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"title\": \"An Empirically-grounded tool for Automatic Prompt Linting and Repair: A Case Study on Bias, Vulnerability, and Optimization in Developer Prompts\",\\n  \"authors\": [\\n    \"Dhia Elhaq Rzig\",\\n    \"Dhruba Jyoti Paul\",\\n    \"Kaiser Pister\",\\n    \"Jordan Henkel\",\\n    \"Foyzul Hassan\"\\n  ],\\n  \"summary\": \"The tidal wave of advancements in Large Language Models (LLMs) has led to\\\\ntheir swift integration into application-level logic. Many software systems now\\\\nuse prompts to interact with these black-box models, combining natural language\\\\nwith dynamic values interpolated at runtime, to perform tasks ranging from\\\\nsentiment analysis to question answering. Due to the programmatic and\\\\nstructured natural language aspects of these prompts, we refer to them as\\\\nDeveloper Prompts. Unlike traditional software artifacts, Dev Prompts blend\\\\nnatural language instructions with artificial languages such as programming and\\\\nmarkup languages, thus requiring specialized tools for analysis, distinct from\\\\nclassical software evaluation methods.\\\\n  In response to this need, we introduce PromptDoctor, a tool explicitly\\\\ndesigned to detect and correct issues of Dev Prompts. PromptDoctor identifies\\\\nand addresses problems related to bias, vulnerability, and sub-optimal\\\\nperformance in Dev Prompts, helping mitigate their possible harms. In our\\\\nanalysis of 2,173 Dev Prompts, selected as a representative sample of 40,573\\\\nDev Prompts, we found that 3.46% contained one or more forms of bias, 10.75%\\\\nwere vulnerable to prompt injection attacks. Additionally, 3,310 were amenable\\\\nto automated prompt optimization. To address these issues, we applied\\\\nPromptDoctor to the flawed Dev Prompts we discovered. PromptDoctor de-biased\\\\n68.29% of the biased Dev Prompts, hardened 41.81% of the vulnerable Dev\\\\nPrompts, and improved the performance of 37.1% sub-optimal Dev Prompts.\\\\nFinally, we developed a PromptDoctor VSCode extension, enabling developers to\\\\neasily enhance Dev Prompts in their existing development workflows. The data\\\\nand source code for this work are available at\",\\n  \"pdf_url\": \"http://arxiv.org/pdf/2501.12521v1\",\\n  \"published\": \"2025-01-21\"\\n}'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_info('2501.12521v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ea3013-e690-4bc8-8622-27b4d42d61e4",
   "metadata": {},
   "source": [
    "## Tool Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7d2260-452d-472a-b56e-326479cb18c9",
   "metadata": {},
   "source": [
    "Here are the schema of each tool which you will provide to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5bdea5f-e93a-4018-8c13-00d5ee10c0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"name\": \"search_papers\",\n",
    "        \"description\": \"Search for papers on arXiv based on a topic and store their information.\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"topic\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The topic to search for\"\n",
    "                }, \n",
    "                \"max_results\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"Maximum number of results to retrieve\",\n",
    "                    \"default\": 5\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"topic\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"extract_info\",\n",
    "        \"description\": \"Search for information about a specific paper across all topic directories.\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"paper_id\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The ID of the paper to look for\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"paper_id\"]\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec668d24-1559-41b7-bc8a-e2dca77dfaf2",
   "metadata": {},
   "source": [
    "## Tool Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c728c1ec-36b1-48b4-9f85-622464ac79f4",
   "metadata": {},
   "source": [
    "This code handles tool mapping and execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c90790c0-efc4-4068-9c00-d2592d80bc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_tool_function = {\n",
    "    \"search_papers\": search_papers,\n",
    "    \"extract_info\": extract_info\n",
    "}\n",
    "\n",
    "def execute_tool(tool_name, tool_args):\n",
    "    \n",
    "    result = mapping_tool_function[tool_name](**tool_args)\n",
    "\n",
    "    if result is None:\n",
    "        result = \"The operation completed but didn't return any results.\"\n",
    "        \n",
    "    elif isinstance(result, list):\n",
    "        result = ', '.join(result)\n",
    "        \n",
    "    elif isinstance(result, dict):\n",
    "        # Convert dictionaries to formatted JSON strings\n",
    "        result = json.dumps(result, indent=2)\n",
    "    \n",
    "    else:\n",
    "        # For any other type, convert using str()\n",
    "        result = str(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8fc4d3-58ac-482c-8bbd-bccd6ef9fc31",
   "metadata": {},
   "source": [
    "## Chatbot Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ba0fad-b0e4-4415-a431-341e9ca85087",
   "metadata": {},
   "source": [
    "The chatbot handles the user's queries one by one, but it does not persist memory across the queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe662400-8506-464e-a3da-75a3d8848bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv() \n",
    "client = openai.OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "    base_url=os.environ.get(\"OPENAI_API_BASE\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175586b4-acdf-4103-8039-134478a4f797",
   "metadata": {},
   "source": [
    "### Query Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12a896e0-3f56-417e-aa51-c61756048593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(query):\n",
    "    \n",
    "    messages = [{'role': 'user', 'content': query}]\n",
    "    \n",
    "    # Convert tools to OpenAI format\n",
    "    openai_tools = []\n",
    "    for tool in tools:\n",
    "        openai_tools.append({\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": tool[\"name\"],\n",
    "                \"description\": tool[\"description\"],\n",
    "                \"parameters\": tool[\"input_schema\"]\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        max_completion_tokens=2024,\n",
    "        model=os.environ.get(\"DEFAULT_LLM_MODEL\", \"o4-mini\"),\n",
    "        tools=openai_tools,\n",
    "        messages=messages\n",
    "    )\n",
    "    \n",
    "    process_query = True\n",
    "    while process_query:\n",
    "        message = response.choices[0].message\n",
    "        \n",
    "        if message.content:\n",
    "            print(message.content)\n",
    "            \n",
    "        if message.tool_calls:\n",
    "            # Add assistant message with tool calls\n",
    "            messages.append({\n",
    "                \"role\": \"assistant\", \n",
    "                \"content\": message.content,\n",
    "                \"tool_calls\": message.tool_calls\n",
    "            })\n",
    "            \n",
    "            # Process each tool call\n",
    "            for tool_call in message.tool_calls:\n",
    "                tool_name = tool_call.function.name\n",
    "                tool_args = json.loads(tool_call.function.arguments)\n",
    "                print(f\"Calling tool {tool_name} with args {tool_args}\")\n",
    "                \n",
    "                result = execute_tool(tool_name, tool_args)\n",
    "                \n",
    "                # Add tool result message\n",
    "                messages.append({\n",
    "                    \"role\": \"tool\",\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"content\": result\n",
    "                })\n",
    "            \n",
    "            # Get next response\n",
    "            response = client.chat.completions.create(\n",
    "                max_completion_tokens=2024,\n",
    "                model=os.environ.get(\"DEFAULT_LLM_MODEL\", \"o4-mini\"),\n",
    "                tools=openai_tools,\n",
    "                messages=messages\n",
    "            )\n",
    "        else:\n",
    "            process_query = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2921ee7f-d2be-464b-ab7b-8db2a3c13ba9",
   "metadata": {},
   "source": [
    "### Chat Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16979cdc-81e9-432b-ba7f-e810b52961e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_loop():\n",
    "    print(\"Type your queries or 'quit' to exit.\")\n",
    "    while True:\n",
    "        try:\n",
    "            query = input(\"\\nQuery: \").strip()\n",
    "            if query.lower() == 'quit':\n",
    "                break\n",
    "    \n",
    "            process_query(query)\n",
    "            print(\"\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfaf254-f22a-4951-885e-1d21fbc41ff3",
   "metadata": {},
   "source": [
    "Feel free to interact with the chatbot. Here's an example query: \n",
    "\n",
    "- Search for 2 papers on \"LLM interpretability\"\n",
    "\n",
    "To access the `papers` folder: 1) click on the `File` option on the top menu of the notebook and 2) click on `Open` and then 3) click on `L3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39676f70-1c72-4da3-8363-da281bd5a83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type your queries or 'quit' to exit.\n",
      "Calling tool search_papers with args {'topic': 'mcp llm coding', 'max_results': 5}\n",
      "Results are saved in: papers/mcp_llm_coding/papers_info.json\n",
      "Calling tool extract_info with args {'paper_id': '2506.13666v1'}\n",
      "Calling tool extract_info with args {'paper_id': '2504.03767v2'}\n",
      "Calling tool extract_info with args {'paper_id': '2504.08999v1'}\n",
      "Calling tool extract_info with args {'paper_id': '2505.14590v4'}\n",
      "Calling tool extract_info with args {'paper_id': '2505.16957v1'}\n",
      "Below is a minimal end-to-end example showing how you might wire up a “Model Context Protocol” (MCP) service (a little tool-server) and an LLM client that uses it. We’ll use Python + FastAPI for the MCP server and Python + the OpenAI API as our LLM.\n",
      "\n",
      "–––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\n",
      "1. MCP-style Tool Server (FastAPI)\n",
      "–––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\n",
      "\n",
      "Save as server.py:\n",
      "\n",
      "```python\n",
      "from fastapi import FastAPI, HTTPException\n",
      "from pydantic  import BaseModel\n",
      "import math, requests\n",
      "\n",
      "app = FastAPI(title=\"Example MCP Tool Server\")\n",
      "\n",
      "class MCPRequest(BaseModel):\n",
      "    tool: str\n",
      "    input: str\n",
      "\n",
      "class MCPResponse(BaseModel):\n",
      "    success: bool\n",
      "    output: str\n",
      "\n",
      "@app.post(\"/mcp\", response_model=MCPResponse)\n",
      "async def call_tool(req: MCPRequest):\n",
      "    \"\"\"\n",
      "    A toy MCP server offering two tools:\n",
      "      1) calculator\n",
      "      2) web_search (via some public search)\n",
      "    \"\"\"\n",
      "    tool = req.tool.lower()\n",
      "    payload = req.input\n",
      "\n",
      "    if tool == \"calculator\":\n",
      "        try:\n",
      "            # VERY simple eval – in prod, you'd sandbox!\n",
      "            result = str(eval(payload, {\"__builtins__\": {}}, math.__dict__))\n",
      "            return MCPResponse(success=True, output=result)\n",
      "        except Exception as e:\n",
      "            return MCPResponse(success=False, output=f\"calc error: {e}\")\n",
      "\n",
      "    elif tool == \"web_search\":\n",
      "        # pretend we have a real search API\n",
      "        # here just return a canned snippet\n",
      "        snippet = f\"Search results for '{payload}': … [snipped]\"\n",
      "        return MCPResponse(success=True, output=snippet)\n",
      "\n",
      "    else:\n",
      "        raise HTTPException(status_code=404, detail=f\"Unknown tool '{tool}'\")\n",
      "```\n",
      "\n",
      "Run with:\n",
      "```\n",
      "uvicorn server:app --port 8001\n",
      "```\n",
      "\n",
      "–––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\n",
      "2. LLM Client That Drives the MCP Workflow\n",
      "–––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\n",
      "\n",
      "Save as client.py. This script prompts the OpenAI chat model to decide (in the prompt) when and how to call your MCP server, parse the answer, issue the HTTP call to `/mcp`, then feed the tool’s output back into the chat.\n",
      "\n",
      "```python\n",
      "import os, json, requests\n",
      "import openai\n",
      "\n",
      "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
      "\n",
      "MCP_SERVER = \"http://localhost:8001/mcp\"\n",
      "\n",
      "def call_mcp_tool(tool_name, tool_input):\n",
      "    payload = {\"tool\": tool_name, \"input\": tool_input}\n",
      "    resp = requests.post(MCP_SERVER, json=payload)\n",
      "    resp.raise_for_status()\n",
      "    return resp.json()[\"output\"]  # simplified\n",
      "\n",
      "def ask_with_tools(user_query):\n",
      "    # 1) Initial prompt: ask LLM how to proceed\n",
      "    system_msg = {\n",
      "        \"role\": \"system\", \n",
      "        \"content\": (\n",
      "            \"You are an agent that can call two tools via the Model Context Protocol (MCP) server:\\n\"\n",
      "            \"  • calculator: evaluate arithmetic expressions\\n\"\n",
      "            \"  • web_search: search for a web snippet\\n\\n\"\n",
      "            \"When you need to use a tool, respond in JSON EXACTLY of the form:\\n\"\n",
      "            \"{\\\"tool\\\": TOOL_NAME, \\\"input\\\": TOOL_INPUT}\\n\"\n",
      "            \"Otherwise answer the user question.\"\n",
      "        )\n",
      "    }\n",
      "    user_msg = {\"role\": \"user\", \"content\": user_query}\n",
      "\n",
      "    chat = openai.ChatCompletion.create(\n",
      "        model=\"gpt-4o-mini\",  # or another\n",
      "        messages=[system_msg, user_msg],\n",
      "        temperature=0\n",
      "    )\n",
      "\n",
      "    assistant_content = chat.choices[0].message[\"content\"].strip()\n",
      "    # Try parsing JSON\n",
      "    try:\n",
      "        j = json.loads(assistant_content)\n",
      "        tool = j[\"tool\"]\n",
      "        inp  = j[\"input\"]\n",
      "        # 2) Actually call the tool\n",
      "        tool_output = call_mcp_tool(tool, inp)\n",
      "        # 3) Feed the tool output back in a follow-up\n",
      "        followup = {\n",
      "            \"role\":\"assistant\",\n",
      "            \"content\": f\"[Tool {tool} returned: {tool_output}]\"\n",
      "        }\n",
      "        final = openai.ChatCompletion.create(\n",
      "            model=\"gpt-4o-mini\",\n",
      "            messages=[system_msg, user_msg, followup],\n",
      "            temperature=0\n",
      "        )\n",
      "        return final.choices[0].message[\"content\"].strip()\n",
      "\n",
      "    except json.JSONDecodeError:\n",
      "        # LLM answered directly\n",
      "        return assistant_content\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    q = \"What is 13 * (7 + 2)? Then give me a quick web search result on 'quantum computing basics'.\"\n",
      "    answer = ask_with_tools(q)\n",
      "    print(\"=== AGENT ANSWER ===\")\n",
      "    print(answer)\n",
      "```\n",
      "\n",
      "–––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\n",
      "3. How it works\n",
      "–––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\n",
      "\n",
      "1. The client sends the user’s question + instructions to the LLM.\n",
      "2. The LLM decides (in its output) that it needs a tool, and emits a JSON blob  \n",
      "   `{ \"tool\": \"...\", \"input\": \"...\" }`.\n",
      "3. Our Python code parses that JSON, calls our MCP server’s `/mcp` endpoint,  \n",
      "   and collects the tool result.\n",
      "4. We append a new message `[Tool X returned: …]` back into the conversation and  \n",
      "   let the LLM finish its answer, combining tool outputs as needed.\n",
      "\n",
      "–––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\n",
      "4. Next Steps & Best Practices\n",
      "–––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\n",
      "\n",
      "•  Sandbox your tools! Don’t `eval` user-supplied code in production.  \n",
      "•  Add authentication & rate limits on `/mcp`.  \n",
      "•  Design a richer tool-schema: for each tool define input/output JSON schema,  \n",
      "   usage docs, safety constraints.  \n",
      "•  Log all MCP calls for audit/red-teaming.  \n",
      "•  If you have many tools, consider a discovery endpoint (`/tools` → schema list).  \n",
      "•  In larger systems you may wrap this into a more formal agent architecture  \n",
      "   (e.g. as in LangChain or AutoGPT), but the pattern above is the core.  \n",
      "\n",
      "This skeleton should get you started coding with “MCP” and your favorite LLM!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34df7890-4b4c-4ec9-b06f-abc8c4a290e8",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\"> 🚨\n",
    "&nbsp; <b>Different Run Results:</b> The output generated by AI chat models can vary with each execution due to their dynamic, probabilistic nature. Don't be surprised if your results differ from those shown in the video.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb34ee2d",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "<p> 💻 &nbsp; <b> To Access the <code>requirements.txt</code> file or the <code>papers</code> folder: </b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Open\"</em> and finally 3) click on <em>\"L3\"</em>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508916f3-8fa1-4e21-bfa7-081a810bc36c",
   "metadata": {},
   "source": [
    "In the next lessons, you will take out the tool definitions to wrap them in an MCP server. Then you will create an MCP client inside the chatbot to make the chatbot MCP compatible.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02d207b-e07d-49ff-bb03-7954aa86c167",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "[Guide on how to implement tool use](https://docs.anthropic.com/en/docs/build-with-claude/tool-use/overview#how-to-implement-tool-use)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e5135e-01c3-4632-9f83-a1e6dd811049",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "\n",
    "\n",
    "<p> ⬇ &nbsp; <b>Download Notebooks:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Download as\"</em> and select <em>\"Notebook (.ipynb)\"</em>.</p>\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
